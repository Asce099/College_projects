{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate n-grams from sentences.\n",
    "def extract_ngrams(data, num):\n",
    "    n_grams = ngrams(word_tokenize(data), num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'Mary had a little Lamb.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram:  ['Mary', 'had', 'a', 'little', 'Lamb', '.']\n",
      "2-gram:  ['Mary had', 'had a', 'a little', 'little Lamb', 'Lamb .']\n",
      "3-gram:  ['Mary had a', 'had a little', 'a little Lamb', 'little Lamb .']\n",
      "4-gram:  ['Mary had a little', 'had a little Lamb', 'a little Lamb .']\n"
     ]
    }
   ],
   "source": [
    "print(\"1-gram: \", extract_ngrams(data, 1))\n",
    "print(\"2-gram: \", extract_ngrams(data, 2))\n",
    "print(\"3-gram: \", extract_ngrams(data, 3))\n",
    "print(\"4-gram: \", extract_ngrams(data, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module nltk.stem.util in nltk.stem:\n",
      "\n",
      "NAME\n",
      "    nltk.stem.util\n",
      "\n",
      "DESCRIPTION\n",
      "    # Natural Language Toolkit: Stemmer Utilities\n",
      "    #\n",
      "    # Copyright (C) 2001-2022 NLTK Project\n",
      "    # Author: Helder <he7d3r@gmail.com>\n",
      "    # URL: <https://www.nltk.org/>\n",
      "    # For license information, see LICENSE.TXT\n",
      "\n",
      "FUNCTIONS\n",
      "    prefix_replace(original, old, new)\n",
      "        Replaces the old prefix of the original string by a new suffix\n",
      "        \n",
      "        :param original: string\n",
      "        :param old: string\n",
      "        :param new: string\n",
      "        :return: string\n",
      "    \n",
      "    suffix_replace(original, old, new)\n",
      "        Replaces the old suffix of the original string by a new suffix\n",
      "\n",
      "FILE\n",
      "    c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages\\nltk\\stem\\util.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ngrams in module nltk.util:\n",
      "\n",
      "ngrams(sequence, n, **kwargs)\n",
      "    Return the ngrams generated from a sequence of items, as an iterator.\n",
      "    For example:\n",
      "    \n",
      "        >>> from nltk.util import ngrams\n",
      "        >>> list(ngrams([1,2,3,4,5], 3))\n",
      "        [(1, 2, 3), (2, 3, 4), (3, 4, 5)]\n",
      "    \n",
      "    Wrap with list for a list version of this function.  Set pad_left\n",
      "    or pad_right to true in order to get additional ngrams:\n",
      "    \n",
      "        >>> list(ngrams([1,2,3,4,5], 2, pad_right=True))\n",
      "        [(1, 2), (2, 3), (3, 4), (4, 5), (5, None)]\n",
      "        >>> list(ngrams([1,2,3,4,5], 2, pad_right=True, right_pad_symbol='</s>'))\n",
      "        [(1, 2), (2, 3), (3, 4), (4, 5), (5, '</s>')]\n",
      "        >>> list(ngrams([1,2,3,4,5], 2, pad_left=True, left_pad_symbol='<s>'))\n",
      "        [('<s>', 1), (1, 2), (2, 3), (3, 4), (4, 5)]\n",
      "        >>> list(ngrams([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
      "        [('<s>', 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, '</s>')]\n",
      "    \n",
      "    \n",
      "    :param sequence: the source data to be converted into ngrams\n",
      "    :type sequence: sequence or iter\n",
      "    :param n: the degree of the ngrams\n",
      "    :type n: int\n",
      "    :param pad_left: whether the ngrams should be left-padded\n",
      "    :type pad_left: bool\n",
      "    :param pad_right: whether the ngrams should be right-padded\n",
      "    :type pad_right: bool\n",
      "    :param left_pad_symbol: the symbol to use for left padding (default is None)\n",
      "    :type left_pad_symbol: any\n",
      "    :param right_pad_symbol: the symbol to use for right padding (default is None)\n",
      "    :type right_pad_symbol: any\n",
      "    :rtype: sequence or iter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function bigrams in module nltk.util:\n",
      "\n",
      "bigrams(sequence, **kwargs)\n",
      "    Return the bigrams generated from a sequence of items, as an iterator.\n",
      "    For example:\n",
      "    \n",
      "        >>> from nltk.util import bigrams\n",
      "        >>> list(bigrams([1,2,3,4,5]))\n",
      "        [(1, 2), (2, 3), (3, 4), (4, 5)]\n",
      "    \n",
      "    Use bigrams for a list version of this function.\n",
      "    \n",
      "    :param sequence: the source data to be converted into bigrams\n",
      "    :type sequence: sequence or iter\n",
      "    :rtype: iter(tuple)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams\n",
    "from nltk.util import everygrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary', 'had'),\n",
       " ('had', 'a'),\n",
       " ('a', 'little'),\n",
       " ('little', 'Lamb'),\n",
       " ('Lamb', '.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(word_tokenize(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary',),\n",
       " ('Mary', 'had'),\n",
       " ('Mary', 'had', 'a'),\n",
       " ('Mary', 'had', 'a', 'little'),\n",
       " ('Mary', 'had', 'a', 'little', 'Lamb'),\n",
       " ('Mary', 'had', 'a', 'little', 'Lamb', '.'),\n",
       " ('had',),\n",
       " ('had', 'a'),\n",
       " ('had', 'a', 'little'),\n",
       " ('had', 'a', 'little', 'Lamb'),\n",
       " ('had', 'a', 'little', 'Lamb', '.'),\n",
       " ('a',),\n",
       " ('a', 'little'),\n",
       " ('a', 'little', 'Lamb'),\n",
       " ('a', 'little', 'Lamb', '.'),\n",
       " ('little',),\n",
       " ('little', 'Lamb'),\n",
       " ('little', 'Lamb', '.'),\n",
       " ('Lamb',),\n",
       " ('Lamb', '.'),\n",
       " ('.',)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(everygrams(word_tokenize(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate n-grams from sentences.\n",
    "def tb_extract_ngrams(data, num):\n",
    "    n_grams = TextBlob(data).ngrams(num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram:  ['Mary', 'had', 'a', 'little', 'Lamb']\n",
      "2-gram:  ['Mary had', 'had a', 'a little', 'little Lamb']\n",
      "3-gram:  ['Mary had a', 'had a little', 'a little Lamb']\n",
      "4-gram:  ['Mary had a little', 'had a little Lamb']\n"
     ]
    }
   ],
   "source": [
    "print(\"1-gram: \", tb_extract_ngrams(data, 1))\n",
    "print(\"2-gram: \", tb_extract_ngrams(data, 2))\n",
    "print(\"3-gram: \", tb_extract_ngrams(data, 3))\n",
    "print(\"4-gram: \", tb_extract_ngrams(data, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mary had a little Lamb."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(data)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textacy\n",
      "  Downloading textacy-0.12.0-py3-none-any.whl (208 kB)\n",
      "     -------------------------------------- 208.4/208.4 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from textacy) (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.13.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from textacy) (1.1.0)\n",
      "Collecting cytoolz>=0.10.1\n",
      "  Downloading cytoolz-0.11.2.tar.gz (481 kB)\n",
      "     -------------------------------------- 481.0/481.0 kB 3.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jellyfish>=0.8.0\n",
      "  Downloading jellyfish-0.9.0-cp39-cp39-win_amd64.whl (26 kB)\n",
      "Requirement already satisfied: spacy>=3.0.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from textacy) (3.3.1)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from textacy) (2.7.1)\n",
      "Requirement already satisfied: catalogue~=2.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from textacy) (2.0.7)\n",
      "Collecting pyphen>=0.10.0\n",
      "  Downloading pyphen-0.12.0-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.19.6 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from textacy) (4.64.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from textacy) (1.7.3)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from textacy) (2.28.0)\n",
      "Collecting cachetools>=4.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from textacy) (1.22.4)\n",
      "Collecting toolz>=0.8.0\n",
      "  Downloading toolz-0.11.2-py3-none-any.whl (55 kB)\n",
      "     ---------------------------------------- 55.8/55.8 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from requests>=2.10.0->textacy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from requests>=2.10.0->textacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from requests>=2.10.0->textacy) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from requests>=2.10.0->textacy) (2021.10.8)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from scikit-learn>=0.19.0->textacy) (2.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from spacy>=3.0.0->textacy) (3.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from spacy>=3.0.0->textacy) (3.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from spacy>=3.0.0->textacy) (1.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from spacy>=3.0.0->textacy) (1.0.7)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from spacy>=3.0.0->textacy) (8.0.17)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from spacy>=3.0.0->textacy) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from spacy>=3.0.0->textacy) (2.4.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from spacy>=3.0.0->textacy) (0.7.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from spacy>=3.0.0->textacy) (3.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from spacy>=3.0.0->textacy) (0.9.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from spacy>=3.0.0->textacy) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from spacy>=3.0.0->textacy) (21.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from spacy>=3.0.0->textacy) (0.4.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from spacy>=3.0.0->textacy) (0.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from spacy>=3.0.0->textacy) (1.8.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from spacy>=3.0.0->textacy) (61.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from tqdm>=4.19.6->textacy) (0.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from packaging>=20.0->spacy>=3.0.0->textacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from pathy>=0.3.5->spacy>=3.0.0->textacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy>=3.0.0->textacy) (4.1.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from jinja2->spacy>=3.0.0->textacy) (2.0.1)\n",
      "Building wheels for collected packages: cytoolz\n",
      "  Building wheel for cytoolz (setup.py): started\n",
      "  Building wheel for cytoolz (setup.py): finished with status 'done'\n",
      "  Created wheel for cytoolz: filename=cytoolz-0.11.2-cp39-cp39-win_amd64.whl size=329018 sha256=1832c7d0dcf7eccc1e4ce1dc69072adf5298fa428d88ec1d46d34d8396a9fdba\n",
      "  Stored in directory: c:\\users\\ajey kumar\\appdata\\local\\pip\\cache\\wheels\\20\\01\\4d\\82db586a6c6fab67f177e84588b8ce14b50081e4b1e56b1109\n",
      "Successfully built cytoolz\n",
      "Installing collected packages: toolz, pyphen, jellyfish, cachetools, cytoolz, textacy\n",
      "Successfully installed cachetools-5.2.0 cytoolz-0.11.2 jellyfish-0.9.0 pyphen-0.12.0 textacy-0.12.0 toolz-0.11.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ngrams in module textacy.extract.basics:\n",
      "\n",
      "ngrams(doclike: 'types.DocLike', n: 'int | Collection[int]', *, filter_stops: 'bool' = True, filter_punct: 'bool' = True, filter_nums: 'bool' = False, include_pos: 'Optional[str | Collection[str]]' = None, exclude_pos: 'Optional[str | Collection[str]]' = None, min_freq: 'int' = 1) -> 'Iterable[Span]'\n",
      "    Extract an ordered sequence of n-grams (``n`` consecutive tokens) from a spaCy\n",
      "    ``Doc`` or ``Span``, for one or multiple ``n`` values, optionally filtering n-grams\n",
      "    by the types and parts-of-speech of the constituent tokens.\n",
      "    \n",
      "    Args:\n",
      "        doclike\n",
      "        n: Number of tokens included per n-gram; for example, ``2`` yields bigrams\n",
      "            and ``3`` yields trigrams. If multiple values are specified, then the\n",
      "            collections of n-grams are concatenated together; for example, ``(2, 3)``\n",
      "            yields bigrams and then trigrams.\n",
      "        filter_stops: If True, remove ngrams that start or end with a stop word.\n",
      "        filter_punct: If True, remove ngrams that contain any punctuation-only tokens.\n",
      "        filter_nums: If True, remove ngrams that contain any numbers\n",
      "            or number-like tokens (e.g. 10, 'ten').\n",
      "        include_pos: Remove ngrams if any constituent tokens' part-of-speech tags\n",
      "            ARE NOT included in this param.\n",
      "        exclude_pos: Remove ngrams if any constituent tokens' part-of-speech tags\n",
      "            ARE included in this param.\n",
      "        min_freq: Remove ngrams that occur in ``doclike`` fewer than ``min_freq`` times\n",
      "    \n",
      "    Yields:\n",
      "        Next ngram from ``doclike`` passing all specified filters, in order of appearance\n",
      "        in the document.\n",
      "    \n",
      "    Raises:\n",
      "        ValueError: if any ``n`` < 1\n",
      "        TypeError: if ``include_pos`` or ``exclude_pos`` is not a str, a set of str,\n",
      "            or a falsy value\n",
      "    \n",
      "    Note:\n",
      "        Filtering by part-of-speech tag uses the universal POS tag set; for details,\n",
      "        check spaCy's docs: https://spacy.io/api/annotation#pos-tagging\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(textacy.extract.ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[little Lamb]\n"
     ]
    }
   ],
   "source": [
    "Ngrams = list(textacy.extract.ngrams(doc, 2, min_freq=1))\n",
    "print(Ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('natural', 'language', 'processing', 'nlp', 'is'),\n",
       " ('language', 'processing', 'nlp', 'is', 'an'),\n",
       " ('processing', 'nlp', 'is', 'an', 'area'),\n",
       " ('nlp', 'is', 'an', 'area', 'of'),\n",
       " ('is', 'an', 'area', 'of', 'computer'),\n",
       " ('an', 'area', 'of', 'computer', 'science'),\n",
       " ('area', 'of', 'computer', 'science', 'and'),\n",
       " ('of', 'computer', 'science', 'and', 'artificial'),\n",
       " ('computer', 'science', 'and', 'artificial', 'intelligence'),\n",
       " ('science', 'and', 'artificial', 'intelligence', 'concerned'),\n",
       " ('and', 'artificial', 'intelligence', 'concerned', 'with'),\n",
       " ('artificial', 'intelligence', 'concerned', 'with', 'the'),\n",
       " ('intelligence', 'concerned', 'with', 'the', 'interactions'),\n",
       " ('concerned', 'with', 'the', 'interactions', 'between'),\n",
       " ('with', 'the', 'interactions', 'between', 'computers'),\n",
       " ('the', 'interactions', 'between', 'computers', 'and'),\n",
       " ('interactions', 'between', 'computers', 'and', 'human'),\n",
       " ('between', 'computers', 'and', 'human', 'natural'),\n",
       " ('computers', 'and', 'human', 'natural', 'languages')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "s = \"Natural-language processing (NLP) is an area of computer science \" \\\n",
    "    \"and artificial intelligence concerned with the interactions \" \\\n",
    "    \"between computers and human (natural) languages.\"\n",
    " \n",
    "s = s.lower()\n",
    "s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "output = list(ngrams(tokens, 5))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2=\"Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.\"\n",
    "\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram:  ['Machine', 'learning', 'is', 'the', 'science', 'of', 'getting', 'computers', 'to', 'act', 'without', 'being', 'explicitly', 'programmed', '.', 'In', 'the', 'past', 'decade', ',', 'machine', 'learning', 'has', 'given', 'us', 'self-driving', 'cars', ',', 'practical', 'speech', 'recognition', ',', 'effective', 'web', 'search', ',', 'and', 'a', 'vastly', 'improved', 'understanding', 'of', 'the', 'human', 'genome', '.', 'Machine', 'learning', 'is', 'so', 'pervasive', 'today', 'that', 'you', 'probably', 'use', 'it', 'dozens', 'of', 'times', 'a', 'day', 'without', 'knowing', 'it', '.', 'Many', 'researchers', 'also', 'think', 'it', 'is', 'the', 'best', 'way', 'to', 'make', 'progress', 'towards', 'human-level', 'AI', '.', 'In', 'this', 'class', ',', 'you', 'will', 'learn', 'about', 'the', 'most', 'effective', 'machine', 'learning', 'techniques', ',', 'and', 'gain', 'practice', 'implementing', 'them', 'and', 'getting', 'them', 'to', 'work', 'for', 'yourself', '.', 'More', 'importantly', ',', 'you', \"'ll\", 'learn', 'about', 'not', 'only', 'the', 'theoretical', 'underpinnings', 'of', 'learning', ',', 'but', 'also', 'gain', 'the', 'practical', 'know-how', 'needed', 'to', 'quickly', 'and', 'powerfully', 'apply', 'these', 'techniques', 'to', 'new', 'problems', '.', 'Finally', ',', 'you', \"'ll\", 'learn', 'about', 'some', 'of', 'Silicon', 'Valley', \"'s\", 'best', 'practices', 'in', 'innovation', 'as', 'it', 'pertains', 'to', 'machine', 'learning', 'and', 'AI', '.']\n",
      "\n",
      "\n",
      "2-gram:  ['Machine learning', 'learning is', 'is the', 'the science', 'science of', 'of getting', 'getting computers', 'computers to', 'to act', 'act without', 'without being', 'being explicitly', 'explicitly programmed', 'programmed .', '. In', 'In the', 'the past', 'past decade', 'decade ,', ', machine', 'machine learning', 'learning has', 'has given', 'given us', 'us self-driving', 'self-driving cars', 'cars ,', ', practical', 'practical speech', 'speech recognition', 'recognition ,', ', effective', 'effective web', 'web search', 'search ,', ', and', 'and a', 'a vastly', 'vastly improved', 'improved understanding', 'understanding of', 'of the', 'the human', 'human genome', 'genome .', '. Machine', 'Machine learning', 'learning is', 'is so', 'so pervasive', 'pervasive today', 'today that', 'that you', 'you probably', 'probably use', 'use it', 'it dozens', 'dozens of', 'of times', 'times a', 'a day', 'day without', 'without knowing', 'knowing it', 'it .', '. Many', 'Many researchers', 'researchers also', 'also think', 'think it', 'it is', 'is the', 'the best', 'best way', 'way to', 'to make', 'make progress', 'progress towards', 'towards human-level', 'human-level AI', 'AI .', '. In', 'In this', 'this class', 'class ,', ', you', 'you will', 'will learn', 'learn about', 'about the', 'the most', 'most effective', 'effective machine', 'machine learning', 'learning techniques', 'techniques ,', ', and', 'and gain', 'gain practice', 'practice implementing', 'implementing them', 'them and', 'and getting', 'getting them', 'them to', 'to work', 'work for', 'for yourself', 'yourself .', '. More', 'More importantly', 'importantly ,', ', you', \"you 'll\", \"'ll learn\", 'learn about', 'about not', 'not only', 'only the', 'the theoretical', 'theoretical underpinnings', 'underpinnings of', 'of learning', 'learning ,', ', but', 'but also', 'also gain', 'gain the', 'the practical', 'practical know-how', 'know-how needed', 'needed to', 'to quickly', 'quickly and', 'and powerfully', 'powerfully apply', 'apply these', 'these techniques', 'techniques to', 'to new', 'new problems', 'problems .', '. Finally', 'Finally ,', ', you', \"you 'll\", \"'ll learn\", 'learn about', 'about some', 'some of', 'of Silicon', 'Silicon Valley', \"Valley 's\", \"'s best\", 'best practices', 'practices in', 'in innovation', 'innovation as', 'as it', 'it pertains', 'pertains to', 'to machine', 'machine learning', 'learning and', 'and AI', 'AI .']\n",
      "\n",
      "\n",
      "3-gram:  ['Machine learning is', 'learning is the', 'is the science', 'the science of', 'science of getting', 'of getting computers', 'getting computers to', 'computers to act', 'to act without', 'act without being', 'without being explicitly', 'being explicitly programmed', 'explicitly programmed .', 'programmed . In', '. In the', 'In the past', 'the past decade', 'past decade ,', 'decade , machine', ', machine learning', 'machine learning has', 'learning has given', 'has given us', 'given us self-driving', 'us self-driving cars', 'self-driving cars ,', 'cars , practical', ', practical speech', 'practical speech recognition', 'speech recognition ,', 'recognition , effective', ', effective web', 'effective web search', 'web search ,', 'search , and', ', and a', 'and a vastly', 'a vastly improved', 'vastly improved understanding', 'improved understanding of', 'understanding of the', 'of the human', 'the human genome', 'human genome .', 'genome . Machine', '. Machine learning', 'Machine learning is', 'learning is so', 'is so pervasive', 'so pervasive today', 'pervasive today that', 'today that you', 'that you probably', 'you probably use', 'probably use it', 'use it dozens', 'it dozens of', 'dozens of times', 'of times a', 'times a day', 'a day without', 'day without knowing', 'without knowing it', 'knowing it .', 'it . Many', '. Many researchers', 'Many researchers also', 'researchers also think', 'also think it', 'think it is', 'it is the', 'is the best', 'the best way', 'best way to', 'way to make', 'to make progress', 'make progress towards', 'progress towards human-level', 'towards human-level AI', 'human-level AI .', 'AI . In', '. In this', 'In this class', 'this class ,', 'class , you', ', you will', 'you will learn', 'will learn about', 'learn about the', 'about the most', 'the most effective', 'most effective machine', 'effective machine learning', 'machine learning techniques', 'learning techniques ,', 'techniques , and', ', and gain', 'and gain practice', 'gain practice implementing', 'practice implementing them', 'implementing them and', 'them and getting', 'and getting them', 'getting them to', 'them to work', 'to work for', 'work for yourself', 'for yourself .', 'yourself . More', '. More importantly', 'More importantly ,', 'importantly , you', \", you 'll\", \"you 'll learn\", \"'ll learn about\", 'learn about not', 'about not only', 'not only the', 'only the theoretical', 'the theoretical underpinnings', 'theoretical underpinnings of', 'underpinnings of learning', 'of learning ,', 'learning , but', ', but also', 'but also gain', 'also gain the', 'gain the practical', 'the practical know-how', 'practical know-how needed', 'know-how needed to', 'needed to quickly', 'to quickly and', 'quickly and powerfully', 'and powerfully apply', 'powerfully apply these', 'apply these techniques', 'these techniques to', 'techniques to new', 'to new problems', 'new problems .', 'problems . Finally', '. Finally ,', 'Finally , you', \", you 'll\", \"you 'll learn\", \"'ll learn about\", 'learn about some', 'about some of', 'some of Silicon', 'of Silicon Valley', \"Silicon Valley 's\", \"Valley 's best\", \"'s best practices\", 'best practices in', 'practices in innovation', 'in innovation as', 'innovation as it', 'as it pertains', 'it pertains to', 'pertains to machine', 'to machine learning', 'machine learning and', 'learning and AI', 'and AI .']\n",
      "\n",
      "\n",
      "4-gram:  ['Machine learning is the', 'learning is the science', 'is the science of', 'the science of getting', 'science of getting computers', 'of getting computers to', 'getting computers to act', 'computers to act without', 'to act without being', 'act without being explicitly', 'without being explicitly programmed', 'being explicitly programmed .', 'explicitly programmed . In', 'programmed . In the', '. In the past', 'In the past decade', 'the past decade ,', 'past decade , machine', 'decade , machine learning', ', machine learning has', 'machine learning has given', 'learning has given us', 'has given us self-driving', 'given us self-driving cars', 'us self-driving cars ,', 'self-driving cars , practical', 'cars , practical speech', ', practical speech recognition', 'practical speech recognition ,', 'speech recognition , effective', 'recognition , effective web', ', effective web search', 'effective web search ,', 'web search , and', 'search , and a', ', and a vastly', 'and a vastly improved', 'a vastly improved understanding', 'vastly improved understanding of', 'improved understanding of the', 'understanding of the human', 'of the human genome', 'the human genome .', 'human genome . Machine', 'genome . Machine learning', '. Machine learning is', 'Machine learning is so', 'learning is so pervasive', 'is so pervasive today', 'so pervasive today that', 'pervasive today that you', 'today that you probably', 'that you probably use', 'you probably use it', 'probably use it dozens', 'use it dozens of', 'it dozens of times', 'dozens of times a', 'of times a day', 'times a day without', 'a day without knowing', 'day without knowing it', 'without knowing it .', 'knowing it . Many', 'it . Many researchers', '. Many researchers also', 'Many researchers also think', 'researchers also think it', 'also think it is', 'think it is the', 'it is the best', 'is the best way', 'the best way to', 'best way to make', 'way to make progress', 'to make progress towards', 'make progress towards human-level', 'progress towards human-level AI', 'towards human-level AI .', 'human-level AI . In', 'AI . In this', '. In this class', 'In this class ,', 'this class , you', 'class , you will', ', you will learn', 'you will learn about', 'will learn about the', 'learn about the most', 'about the most effective', 'the most effective machine', 'most effective machine learning', 'effective machine learning techniques', 'machine learning techniques ,', 'learning techniques , and', 'techniques , and gain', ', and gain practice', 'and gain practice implementing', 'gain practice implementing them', 'practice implementing them and', 'implementing them and getting', 'them and getting them', 'and getting them to', 'getting them to work', 'them to work for', 'to work for yourself', 'work for yourself .', 'for yourself . More', 'yourself . More importantly', '. More importantly ,', 'More importantly , you', \"importantly , you 'll\", \", you 'll learn\", \"you 'll learn about\", \"'ll learn about not\", 'learn about not only', 'about not only the', 'not only the theoretical', 'only the theoretical underpinnings', 'the theoretical underpinnings of', 'theoretical underpinnings of learning', 'underpinnings of learning ,', 'of learning , but', 'learning , but also', ', but also gain', 'but also gain the', 'also gain the practical', 'gain the practical know-how', 'the practical know-how needed', 'practical know-how needed to', 'know-how needed to quickly', 'needed to quickly and', 'to quickly and powerfully', 'quickly and powerfully apply', 'and powerfully apply these', 'powerfully apply these techniques', 'apply these techniques to', 'these techniques to new', 'techniques to new problems', 'to new problems .', 'new problems . Finally', 'problems . Finally ,', '. Finally , you', \"Finally , you 'll\", \", you 'll learn\", \"you 'll learn about\", \"'ll learn about some\", 'learn about some of', 'about some of Silicon', 'some of Silicon Valley', \"of Silicon Valley 's\", \"Silicon Valley 's best\", \"Valley 's best practices\", \"'s best practices in\", 'best practices in innovation', 'practices in innovation as', 'in innovation as it', 'innovation as it pertains', 'as it pertains to', 'it pertains to machine', 'pertains to machine learning', 'to machine learning and', 'machine learning and AI', 'learning and AI .']\n"
     ]
    }
   ],
   "source": [
    "print(\"1-gram: \", extract_ngrams(data2, 1))\n",
    "print(\"\\n\")\n",
    "print(\"2-gram: \", extract_ngrams(data2, 2))\n",
    "print(\"\\n\")\n",
    "print(\"3-gram: \", extract_ngrams(data2, 3))\n",
    "print(\"\\n\")\n",
    "print(\"4-gram: \", extract_ngrams(data2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

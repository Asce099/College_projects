{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Stemming and Lemmatization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'the',\n",
       " 'science',\n",
       " 'of',\n",
       " 'getting',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'act',\n",
       " 'without',\n",
       " 'being',\n",
       " 'explicitly',\n",
       " 'programmed',\n",
       " '.',\n",
       " 'In',\n",
       " 'the',\n",
       " 'past',\n",
       " 'decade',\n",
       " ',',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'has',\n",
       " 'given',\n",
       " 'us',\n",
       " 'self-driving',\n",
       " 'cars',\n",
       " ',',\n",
       " 'practical',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " ',',\n",
       " 'effective',\n",
       " 'web',\n",
       " 'search',\n",
       " ',',\n",
       " 'and',\n",
       " 'a',\n",
       " 'vastly',\n",
       " 'improved',\n",
       " 'understanding',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'genome',\n",
       " '.',\n",
       " 'Machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'so',\n",
       " 'pervasive',\n",
       " 'today',\n",
       " 'that',\n",
       " 'you',\n",
       " 'probably',\n",
       " 'use',\n",
       " 'it',\n",
       " 'dozens',\n",
       " 'of',\n",
       " 'times',\n",
       " 'a',\n",
       " 'day',\n",
       " 'without',\n",
       " 'knowing',\n",
       " 'it',\n",
       " '.',\n",
       " 'Many',\n",
       " 'researchers',\n",
       " 'also',\n",
       " 'think',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'way',\n",
       " 'to',\n",
       " 'make',\n",
       " 'progress',\n",
       " 'towards',\n",
       " 'human-level',\n",
       " 'AI',\n",
       " '.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'class',\n",
       " ',',\n",
       " 'you',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'the',\n",
       " 'most',\n",
       " 'effective',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'techniques',\n",
       " ',',\n",
       " 'and',\n",
       " 'gain',\n",
       " 'practice',\n",
       " 'implementing',\n",
       " 'them',\n",
       " 'and',\n",
       " 'getting',\n",
       " 'them',\n",
       " 'to',\n",
       " 'work',\n",
       " 'for',\n",
       " 'yourself',\n",
       " '.',\n",
       " 'More',\n",
       " 'importantly',\n",
       " ',',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'about',\n",
       " 'not',\n",
       " 'only',\n",
       " 'the',\n",
       " 'theoretical',\n",
       " 'underpinnings',\n",
       " 'of',\n",
       " 'learning',\n",
       " ',',\n",
       " 'but',\n",
       " 'also',\n",
       " 'gain',\n",
       " 'the',\n",
       " 'practical',\n",
       " 'know-how',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'quickly',\n",
       " 'and',\n",
       " 'powerfully',\n",
       " 'apply',\n",
       " 'these',\n",
       " 'techniques',\n",
       " 'to',\n",
       " 'new',\n",
       " 'problems',\n",
       " '.',\n",
       " 'Finally',\n",
       " ',',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'about',\n",
       " 'some',\n",
       " 'of',\n",
       " 'Silicon',\n",
       " 'Valley',\n",
       " \"'s\",\n",
       " 'best',\n",
       " 'practices',\n",
       " 'in',\n",
       " 'innovation',\n",
       " 'as',\n",
       " 'it',\n",
       " 'pertains',\n",
       " 'to',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'AI',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machin',\n",
       " 'learn',\n",
       " 'is',\n",
       " 'the',\n",
       " 'sci',\n",
       " 'of',\n",
       " 'get',\n",
       " 'comput',\n",
       " 'to',\n",
       " 'act',\n",
       " 'without',\n",
       " 'being',\n",
       " 'explicit',\n",
       " 'program',\n",
       " '.',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " 'decad',\n",
       " ',',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'has',\n",
       " 'giv',\n",
       " 'us',\n",
       " 'self-driving',\n",
       " 'car',\n",
       " ',',\n",
       " 'pract',\n",
       " 'speech',\n",
       " 'recognit',\n",
       " ',',\n",
       " 'effect',\n",
       " 'web',\n",
       " 'search',\n",
       " ',',\n",
       " 'and',\n",
       " 'a',\n",
       " 'vast',\n",
       " 'improv',\n",
       " 'understand',\n",
       " 'of',\n",
       " 'the',\n",
       " 'hum',\n",
       " 'genom',\n",
       " '.',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'is',\n",
       " 'so',\n",
       " 'pervas',\n",
       " 'today',\n",
       " 'that',\n",
       " 'you',\n",
       " 'prob',\n",
       " 'us',\n",
       " 'it',\n",
       " 'doz',\n",
       " 'of',\n",
       " 'tim',\n",
       " 'a',\n",
       " 'day',\n",
       " 'without',\n",
       " 'know',\n",
       " 'it',\n",
       " '.',\n",
       " 'many',\n",
       " 'research',\n",
       " 'also',\n",
       " 'think',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'way',\n",
       " 'to',\n",
       " 'mak',\n",
       " 'progress',\n",
       " 'toward',\n",
       " 'human-level',\n",
       " 'ai',\n",
       " '.',\n",
       " 'in',\n",
       " 'thi',\n",
       " 'class',\n",
       " ',',\n",
       " 'you',\n",
       " 'wil',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'the',\n",
       " 'most',\n",
       " 'effect',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'techn',\n",
       " ',',\n",
       " 'and',\n",
       " 'gain',\n",
       " 'pract',\n",
       " 'impl',\n",
       " 'them',\n",
       " 'and',\n",
       " 'get',\n",
       " 'them',\n",
       " 'to',\n",
       " 'work',\n",
       " 'for',\n",
       " 'yourself',\n",
       " '.',\n",
       " 'mor',\n",
       " 'import',\n",
       " ',',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'about',\n",
       " 'not',\n",
       " 'on',\n",
       " 'the',\n",
       " 'theoret',\n",
       " 'underpin',\n",
       " 'of',\n",
       " 'learn',\n",
       " ',',\n",
       " 'but',\n",
       " 'also',\n",
       " 'gain',\n",
       " 'the',\n",
       " 'pract',\n",
       " 'know-how',\n",
       " 'nee',\n",
       " 'to',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'pow',\n",
       " 'apply',\n",
       " 'thes',\n",
       " 'techn',\n",
       " 'to',\n",
       " 'new',\n",
       " 'problem',\n",
       " '.',\n",
       " 'fin',\n",
       " ',',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'about',\n",
       " 'som',\n",
       " 'of',\n",
       " 'silicon',\n",
       " 'valley',\n",
       " \"'s\",\n",
       " 'best',\n",
       " 'pract',\n",
       " 'in',\n",
       " 'innov',\n",
       " 'as',\n",
       " 'it',\n",
       " 'pertain',\n",
       " 'to',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'ai',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "stem1 = [lancaster.stem(i) for i in tokens]\n",
    "stem1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Word               LancasterStemmer()\n",
      "Machine               machin\n",
      "learning               learn\n",
      "is               is\n",
      "the               the\n",
      "science               sci\n",
      "of               of\n",
      "getting               get\n",
      "computers               comput\n",
      "to               to\n",
      "act               act\n",
      "without               without\n",
      "being               being\n",
      "explicitly               explicit\n",
      "programmed               program\n",
      ".               .\n",
      "In               in\n",
      "the               the\n",
      "past               past\n",
      "decade               decad\n",
      ",               ,\n",
      "machine               machin\n",
      "learning               learn\n",
      "has               has\n",
      "given               giv\n",
      "us               us\n",
      "self-driving               self-driving\n",
      "cars               car\n",
      ",               ,\n",
      "practical               pract\n",
      "speech               speech\n",
      "recognition               recognit\n",
      ",               ,\n",
      "effective               effect\n",
      "web               web\n",
      "search               search\n",
      ",               ,\n",
      "and               and\n",
      "a               a\n",
      "vastly               vast\n",
      "improved               improv\n",
      "understanding               understand\n",
      "of               of\n",
      "the               the\n",
      "human               hum\n",
      "genome               genom\n",
      ".               .\n",
      "Machine               machin\n",
      "learning               learn\n",
      "is               is\n",
      "so               so\n",
      "pervasive               pervas\n",
      "today               today\n",
      "that               that\n",
      "you               you\n",
      "probably               prob\n",
      "use               us\n",
      "it               it\n",
      "dozens               doz\n",
      "of               of\n",
      "times               tim\n",
      "a               a\n",
      "day               day\n",
      "without               without\n",
      "knowing               know\n",
      "it               it\n",
      ".               .\n",
      "Many               many\n",
      "researchers               research\n",
      "also               also\n",
      "think               think\n",
      "it               it\n",
      "is               is\n",
      "the               the\n",
      "best               best\n",
      "way               way\n",
      "to               to\n",
      "make               mak\n",
      "progress               progress\n",
      "towards               toward\n",
      "human-level               human-level\n",
      "AI               ai\n",
      ".               .\n",
      "In               in\n",
      "this               thi\n",
      "class               class\n",
      ",               ,\n",
      "you               you\n",
      "will               wil\n",
      "learn               learn\n",
      "about               about\n",
      "the               the\n",
      "most               most\n",
      "effective               effect\n",
      "machine               machin\n",
      "learning               learn\n",
      "techniques               techn\n",
      ",               ,\n",
      "and               and\n",
      "gain               gain\n",
      "practice               pract\n",
      "implementing               impl\n",
      "them               them\n",
      "and               and\n",
      "getting               get\n",
      "them               them\n",
      "to               to\n",
      "work               work\n",
      "for               for\n",
      "yourself               yourself\n",
      ".               .\n",
      "More               mor\n",
      "importantly               import\n",
      ",               ,\n",
      "you               you\n",
      "'ll               'll\n",
      "learn               learn\n",
      "about               about\n",
      "not               not\n",
      "only               on\n",
      "the               the\n",
      "theoretical               theoret\n",
      "underpinnings               underpin\n",
      "of               of\n",
      "learning               learn\n",
      ",               ,\n",
      "but               but\n",
      "also               also\n",
      "gain               gain\n",
      "the               the\n",
      "practical               pract\n",
      "know-how               know-how\n",
      "needed               nee\n",
      "to               to\n",
      "quickly               quick\n",
      "and               and\n",
      "powerfully               pow\n",
      "apply               apply\n",
      "these               thes\n",
      "techniques               techn\n",
      "to               to\n",
      "new               new\n",
      "problems               problem\n",
      ".               .\n",
      "Finally               fin\n",
      ",               ,\n",
      "you               you\n",
      "'ll               'll\n",
      "learn               learn\n",
      "about               about\n",
      "some               som\n",
      "of               of\n",
      "Silicon               silicon\n",
      "Valley               valley\n",
      "'s               's\n",
      "best               best\n",
      "practices               pract\n",
      "in               in\n",
      "innovation               innov\n",
      "as               as\n",
      "it               it\n",
      "pertains               pertain\n",
      "to               to\n",
      "machine               machin\n",
      "learning               learn\n",
      "and               and\n",
      "AI               ai\n",
      ".               .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print( \" Word               LancasterStemmer()\")\n",
    "for i in range (len(tokens)):\n",
    "    print( tokens[i]+ \"               \"+ stem1[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machin',\n",
       " 'learn',\n",
       " 'is',\n",
       " 'the',\n",
       " 'scienc',\n",
       " 'of',\n",
       " 'get',\n",
       " 'comput',\n",
       " 'to',\n",
       " 'act',\n",
       " 'without',\n",
       " 'be',\n",
       " 'explicitli',\n",
       " 'program',\n",
       " '.',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " 'decad',\n",
       " ',',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'ha',\n",
       " 'given',\n",
       " 'us',\n",
       " 'self-driv',\n",
       " 'car',\n",
       " ',',\n",
       " 'practic',\n",
       " 'speech',\n",
       " 'recognit',\n",
       " ',',\n",
       " 'effect',\n",
       " 'web',\n",
       " 'search',\n",
       " ',',\n",
       " 'and',\n",
       " 'a',\n",
       " 'vastli',\n",
       " 'improv',\n",
       " 'understand',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'genom',\n",
       " '.',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'is',\n",
       " 'so',\n",
       " 'pervas',\n",
       " 'today',\n",
       " 'that',\n",
       " 'you',\n",
       " 'probabl',\n",
       " 'use',\n",
       " 'it',\n",
       " 'dozen',\n",
       " 'of',\n",
       " 'time',\n",
       " 'a',\n",
       " 'day',\n",
       " 'without',\n",
       " 'know',\n",
       " 'it',\n",
       " '.',\n",
       " 'mani',\n",
       " 'research',\n",
       " 'also',\n",
       " 'think',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'way',\n",
       " 'to',\n",
       " 'make',\n",
       " 'progress',\n",
       " 'toward',\n",
       " 'human-level',\n",
       " 'ai',\n",
       " '.',\n",
       " 'in',\n",
       " 'thi',\n",
       " 'class',\n",
       " ',',\n",
       " 'you',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'the',\n",
       " 'most',\n",
       " 'effect',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'techniqu',\n",
       " ',',\n",
       " 'and',\n",
       " 'gain',\n",
       " 'practic',\n",
       " 'implement',\n",
       " 'them',\n",
       " 'and',\n",
       " 'get',\n",
       " 'them',\n",
       " 'to',\n",
       " 'work',\n",
       " 'for',\n",
       " 'yourself',\n",
       " '.',\n",
       " 'more',\n",
       " 'importantli',\n",
       " ',',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'about',\n",
       " 'not',\n",
       " 'onli',\n",
       " 'the',\n",
       " 'theoret',\n",
       " 'underpin',\n",
       " 'of',\n",
       " 'learn',\n",
       " ',',\n",
       " 'but',\n",
       " 'also',\n",
       " 'gain',\n",
       " 'the',\n",
       " 'practic',\n",
       " 'know-how',\n",
       " 'need',\n",
       " 'to',\n",
       " 'quickli',\n",
       " 'and',\n",
       " 'power',\n",
       " 'appli',\n",
       " 'these',\n",
       " 'techniqu',\n",
       " 'to',\n",
       " 'new',\n",
       " 'problem',\n",
       " '.',\n",
       " 'final',\n",
       " ',',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'about',\n",
       " 'some',\n",
       " 'of',\n",
       " 'silicon',\n",
       " 'valley',\n",
       " \"'s\",\n",
       " 'best',\n",
       " 'practic',\n",
       " 'in',\n",
       " 'innov',\n",
       " 'as',\n",
       " 'it',\n",
       " 'pertain',\n",
       " 'to',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'ai',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "stem2 = [porter.stem(i) for i in tokens]\n",
    "stem2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'the',\n",
       " 'science',\n",
       " 'of',\n",
       " 'getting',\n",
       " 'computer',\n",
       " 'to',\n",
       " 'act',\n",
       " 'without',\n",
       " 'being',\n",
       " 'explicitly',\n",
       " 'programmed',\n",
       " '.',\n",
       " 'In',\n",
       " 'the',\n",
       " 'past',\n",
       " 'decade',\n",
       " ',',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'ha',\n",
       " 'given',\n",
       " 'u',\n",
       " 'self-driving',\n",
       " 'car',\n",
       " ',',\n",
       " 'practical',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " ',',\n",
       " 'effective',\n",
       " 'web',\n",
       " 'search',\n",
       " ',',\n",
       " 'and',\n",
       " 'a',\n",
       " 'vastly',\n",
       " 'improved',\n",
       " 'understanding',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'genome',\n",
       " '.',\n",
       " 'Machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'so',\n",
       " 'pervasive',\n",
       " 'today',\n",
       " 'that',\n",
       " 'you',\n",
       " 'probably',\n",
       " 'use',\n",
       " 'it',\n",
       " 'dozen',\n",
       " 'of',\n",
       " 'time',\n",
       " 'a',\n",
       " 'day',\n",
       " 'without',\n",
       " 'knowing',\n",
       " 'it',\n",
       " '.',\n",
       " 'Many',\n",
       " 'researcher',\n",
       " 'also',\n",
       " 'think',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'way',\n",
       " 'to',\n",
       " 'make',\n",
       " 'progress',\n",
       " 'towards',\n",
       " 'human-level',\n",
       " 'AI',\n",
       " '.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'class',\n",
       " ',',\n",
       " 'you',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'the',\n",
       " 'most',\n",
       " 'effective',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'technique',\n",
       " ',',\n",
       " 'and',\n",
       " 'gain',\n",
       " 'practice',\n",
       " 'implementing',\n",
       " 'them',\n",
       " 'and',\n",
       " 'getting',\n",
       " 'them',\n",
       " 'to',\n",
       " 'work',\n",
       " 'for',\n",
       " 'yourself',\n",
       " '.',\n",
       " 'More',\n",
       " 'importantly',\n",
       " ',',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'about',\n",
       " 'not',\n",
       " 'only',\n",
       " 'the',\n",
       " 'theoretical',\n",
       " 'underpinnings',\n",
       " 'of',\n",
       " 'learning',\n",
       " ',',\n",
       " 'but',\n",
       " 'also',\n",
       " 'gain',\n",
       " 'the',\n",
       " 'practical',\n",
       " 'know-how',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'quickly',\n",
       " 'and',\n",
       " 'powerfully',\n",
       " 'apply',\n",
       " 'these',\n",
       " 'technique',\n",
       " 'to',\n",
       " 'new',\n",
       " 'problem',\n",
       " '.',\n",
       " 'Finally',\n",
       " ',',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'about',\n",
       " 'some',\n",
       " 'of',\n",
       " 'Silicon',\n",
       " 'Valley',\n",
       " \"'s\",\n",
       " 'best',\n",
       " 'practice',\n",
       " 'in',\n",
       " 'innovation',\n",
       " 'a',\n",
       " 'it',\n",
       " 'pertains',\n",
       " 'to',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'AI',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma = nltk.WordNetLemmatizer()\n",
    "lemma1 = [lemma.lemmatize(i) for i in tokens]\n",
    "lemma1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>LancasterStemmer</th>\n",
       "      <th>PorterStemmer</th>\n",
       "      <th>WordNetLemmatizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine</td>\n",
       "      <td>machin</td>\n",
       "      <td>machin</td>\n",
       "      <td>Machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>learning</td>\n",
       "      <td>learn</td>\n",
       "      <td>learn</td>\n",
       "      <td>learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>sci</td>\n",
       "      <td>scienc</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>machine</td>\n",
       "      <td>machin</td>\n",
       "      <td>machin</td>\n",
       "      <td>machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>learning</td>\n",
       "      <td>learn</td>\n",
       "      <td>learn</td>\n",
       "      <td>learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>AI</td>\n",
       "      <td>ai</td>\n",
       "      <td>ai</td>\n",
       "      <td>AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word LancasterStemmer PorterStemmer WordNetLemmatizer\n",
       "0     Machine           machin        machin           Machine\n",
       "1    learning            learn         learn          learning\n",
       "2          is               is            is                is\n",
       "3         the              the           the               the\n",
       "4     science              sci        scienc           science\n",
       "..        ...              ...           ...               ...\n",
       "162   machine           machin        machin           machine\n",
       "163  learning            learn         learn          learning\n",
       "164       and              and           and               and\n",
       "165        AI               ai            ai                AI\n",
       "166         .                .             .                 .\n",
       "\n",
       "[167 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp = {'Word': tokens, 'LancasterStemmer': stem1, 'PorterStemmer': stem2,'WordNetLemmatizer': lemma1} \n",
    "    \n",
    "df_comp = pd.DataFrame(comp)\n",
    "    \n",
    "df_comp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Short Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Women in technology are amazing at coding'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = \"Women in technology are amazing at coding\"\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['women', 'in', 'technology', 'are', 'amazing', 'at', 'coding']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = [i.lower() for i in text2.split()]\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woman', 'in', 'technology', 'are', 'amazing', 'at', 'coding']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas1 = [lemma.lemmatize(i) for i in ex]\n",
    "lemmas1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wom', 'in', 'technolog', 'ar', 'amaz', 'at', 'cod']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemsL = [lancaster.stem(i) for i in ex]\n",
    "stemsL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['women', 'in', 'technolog', 'are', 'amaz', 'at', 'code']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemP= [porter.stem(i) for i in ex]\n",
    "stemP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['women', 'in', 'technolog', 'are', 'amaz', 'at', 'code']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stemS= [stemmer.stem(i) for i in ex]\n",
    "stemS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Stemming and Lemmatization using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Women in technology are amazing at coding"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u\"Women in technology are amazing at coding\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy.lemmatizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-34881fc8f799>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#lemmatizer = Lemmatizer()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#lemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mLemmS\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mLemmS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy.lemmatizer'"
     ]
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "#lemmatizer = Lemmatizer()\n",
    "#lemmatizer\n",
    "LemmS= [Lemmatizer(i.text) for i in doc]\n",
    "LemmS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woman', 'in', 'technology', 'be', 'amazing', 'at', 'code']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LemmS1= [i.lemma_ for i in doc]\n",
    "         \n",
    "LemmS1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Stemming and Lemmatization using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Women in technology are amazing at coding.\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = TextBlob(\"Women in technology are amazing at coding.\")\n",
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Women', 'in', 'technology', 'are', 'amazing', 'at', 'coding']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LemmaT=[w.lemmatize() for w in wiki.words]\n",
    "LemmaT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat with example text  SnowballStemmer,Stemming and Lemmatization using SpaCy & textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machin',\n",
       " 'learn',\n",
       " 'is',\n",
       " 'the',\n",
       " 'scienc',\n",
       " 'of',\n",
       " 'get',\n",
       " 'comput',\n",
       " 'to',\n",
       " 'act',\n",
       " 'without',\n",
       " 'be',\n",
       " 'explicit',\n",
       " 'program',\n",
       " '.',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " 'decad',\n",
       " ',',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'has',\n",
       " 'given',\n",
       " 'us',\n",
       " 'self-driv',\n",
       " 'car',\n",
       " ',',\n",
       " 'practic',\n",
       " 'speech',\n",
       " 'recognit',\n",
       " ',',\n",
       " 'effect',\n",
       " 'web',\n",
       " 'search',\n",
       " ',',\n",
       " 'and',\n",
       " 'a',\n",
       " 'vast',\n",
       " 'improv',\n",
       " 'understand',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'genom',\n",
       " '.',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'is',\n",
       " 'so',\n",
       " 'pervas',\n",
       " 'today',\n",
       " 'that',\n",
       " 'you',\n",
       " 'probabl',\n",
       " 'use',\n",
       " 'it',\n",
       " 'dozen',\n",
       " 'of',\n",
       " 'time',\n",
       " 'a',\n",
       " 'day',\n",
       " 'without',\n",
       " 'know',\n",
       " 'it',\n",
       " '.',\n",
       " 'mani',\n",
       " 'research',\n",
       " 'also',\n",
       " 'think',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'way',\n",
       " 'to',\n",
       " 'make',\n",
       " 'progress',\n",
       " 'toward',\n",
       " 'human-level',\n",
       " 'ai',\n",
       " '.',\n",
       " 'in',\n",
       " 'this',\n",
       " 'class',\n",
       " ',',\n",
       " 'you',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'the',\n",
       " 'most',\n",
       " 'effect',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'techniqu',\n",
       " ',',\n",
       " 'and',\n",
       " 'gain',\n",
       " 'practic',\n",
       " 'implement',\n",
       " 'them',\n",
       " 'and',\n",
       " 'get',\n",
       " 'them',\n",
       " 'to',\n",
       " 'work',\n",
       " 'for',\n",
       " 'yourself',\n",
       " '.',\n",
       " 'more',\n",
       " 'import',\n",
       " ',',\n",
       " 'you',\n",
       " 'll',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'not',\n",
       " 'onli',\n",
       " 'the',\n",
       " 'theoret',\n",
       " 'underpin',\n",
       " 'of',\n",
       " 'learn',\n",
       " ',',\n",
       " 'but',\n",
       " 'also',\n",
       " 'gain',\n",
       " 'the',\n",
       " 'practic',\n",
       " 'know-how',\n",
       " 'need',\n",
       " 'to',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'power',\n",
       " 'appli',\n",
       " 'these',\n",
       " 'techniqu',\n",
       " 'to',\n",
       " 'new',\n",
       " 'problem',\n",
       " '.',\n",
       " 'final',\n",
       " ',',\n",
       " 'you',\n",
       " 'll',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'some',\n",
       " 'of',\n",
       " 'silicon',\n",
       " 'valley',\n",
       " \"'s\",\n",
       " 'best',\n",
       " 'practic',\n",
       " 'in',\n",
       " 'innov',\n",
       " 'as',\n",
       " 'it',\n",
       " 'pertain',\n",
       " 'to',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'ai',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Snowballstemmer\n",
    "stemS1= [stemmer.stem(i) for i in tokens]\n",
    "stemS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spacy Lemmatizer\n",
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine',\n",
       " 'learning',\n",
       " 'be',\n",
       " 'the',\n",
       " 'science',\n",
       " 'of',\n",
       " 'get',\n",
       " 'computer',\n",
       " 'to',\n",
       " 'act',\n",
       " 'without',\n",
       " 'be',\n",
       " 'explicitly',\n",
       " 'program',\n",
       " '.',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " 'decade',\n",
       " ',',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'have',\n",
       " 'give',\n",
       " 'we',\n",
       " 'self',\n",
       " '-',\n",
       " 'drive',\n",
       " 'car',\n",
       " ',',\n",
       " 'practical',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " ',',\n",
       " 'effective',\n",
       " 'web',\n",
       " 'search',\n",
       " ',',\n",
       " 'and',\n",
       " 'a',\n",
       " 'vastly',\n",
       " 'improve',\n",
       " 'understanding',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'genome',\n",
       " '.',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'be',\n",
       " 'so',\n",
       " 'pervasive',\n",
       " 'today',\n",
       " 'that',\n",
       " 'you',\n",
       " 'probably',\n",
       " 'use',\n",
       " 'it',\n",
       " 'dozen',\n",
       " 'of',\n",
       " 'time',\n",
       " 'a',\n",
       " 'day',\n",
       " 'without',\n",
       " 'know',\n",
       " 'it',\n",
       " '.',\n",
       " 'many',\n",
       " 'researcher',\n",
       " 'also',\n",
       " 'think',\n",
       " 'it',\n",
       " 'be',\n",
       " 'the',\n",
       " 'good',\n",
       " 'way',\n",
       " 'to',\n",
       " 'make',\n",
       " 'progress',\n",
       " 'towards',\n",
       " 'human',\n",
       " '-',\n",
       " 'level',\n",
       " 'AI',\n",
       " '.',\n",
       " 'in',\n",
       " 'this',\n",
       " 'class',\n",
       " ',',\n",
       " 'you',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'the',\n",
       " 'most',\n",
       " 'effective',\n",
       " 'machine',\n",
       " 'learn',\n",
       " 'technique',\n",
       " ',',\n",
       " 'and',\n",
       " 'gain',\n",
       " 'practice',\n",
       " 'implement',\n",
       " 'they',\n",
       " 'and',\n",
       " 'get',\n",
       " 'they',\n",
       " 'to',\n",
       " 'work',\n",
       " 'for',\n",
       " 'yourself',\n",
       " '.',\n",
       " 'more',\n",
       " 'importantly',\n",
       " ',',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'about',\n",
       " 'not',\n",
       " 'only',\n",
       " 'the',\n",
       " 'theoretical',\n",
       " 'underpinning',\n",
       " 'of',\n",
       " 'learning',\n",
       " ',',\n",
       " 'but',\n",
       " 'also',\n",
       " 'gain',\n",
       " 'the',\n",
       " 'practical',\n",
       " 'know',\n",
       " '-',\n",
       " 'how',\n",
       " 'need',\n",
       " 'to',\n",
       " 'quickly',\n",
       " 'and',\n",
       " 'powerfully',\n",
       " 'apply',\n",
       " 'these',\n",
       " 'technique',\n",
       " 'to',\n",
       " 'new',\n",
       " 'problem',\n",
       " '.',\n",
       " 'finally',\n",
       " ',',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'about',\n",
       " 'some',\n",
       " 'of',\n",
       " 'Silicon',\n",
       " 'Valley',\n",
       " \"'s\",\n",
       " 'good',\n",
       " 'practice',\n",
       " 'in',\n",
       " 'innovation',\n",
       " 'as',\n",
       " 'it',\n",
       " 'pertain',\n",
       " 'to',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'AI',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LemmS2= [i.lemma_ for i in doc]\n",
    "         \n",
    "LemmS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LemmS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.\")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#textBlob Lemmatizer\n",
    "wiki = TextBlob(text)\n",
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'the',\n",
       " 'science',\n",
       " 'of',\n",
       " 'getting',\n",
       " 'computer',\n",
       " 'to',\n",
       " 'act',\n",
       " 'without',\n",
       " 'being',\n",
       " 'explicitly',\n",
       " 'programmed',\n",
       " 'In',\n",
       " 'the',\n",
       " 'past',\n",
       " 'decade',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'ha',\n",
       " 'given',\n",
       " 'u',\n",
       " 'self-driving',\n",
       " 'car',\n",
       " 'practical',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'effective',\n",
       " 'web',\n",
       " 'search',\n",
       " 'and',\n",
       " 'a',\n",
       " 'vastly',\n",
       " 'improved',\n",
       " 'understanding',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'genome',\n",
       " 'Machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'so',\n",
       " 'pervasive',\n",
       " 'today',\n",
       " 'that',\n",
       " 'you',\n",
       " 'probably',\n",
       " 'use',\n",
       " 'it',\n",
       " 'dozen',\n",
       " 'of',\n",
       " 'time',\n",
       " 'a',\n",
       " 'day',\n",
       " 'without',\n",
       " 'knowing',\n",
       " 'it',\n",
       " 'Many',\n",
       " 'researcher',\n",
       " 'also',\n",
       " 'think',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'way',\n",
       " 'to',\n",
       " 'make',\n",
       " 'progress',\n",
       " 'towards',\n",
       " 'human-level',\n",
       " 'AI',\n",
       " 'In',\n",
       " 'this',\n",
       " 'class',\n",
       " 'you',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'the',\n",
       " 'most',\n",
       " 'effective',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'technique',\n",
       " 'and',\n",
       " 'gain',\n",
       " 'practice',\n",
       " 'implementing',\n",
       " 'them',\n",
       " 'and',\n",
       " 'getting',\n",
       " 'them',\n",
       " 'to',\n",
       " 'work',\n",
       " 'for',\n",
       " 'yourself',\n",
       " 'More',\n",
       " 'importantly',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'about',\n",
       " 'not',\n",
       " 'only',\n",
       " 'the',\n",
       " 'theoretical',\n",
       " 'underpinnings',\n",
       " 'of',\n",
       " 'learning',\n",
       " 'but',\n",
       " 'also',\n",
       " 'gain',\n",
       " 'the',\n",
       " 'practical',\n",
       " 'know-how',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'quickly',\n",
       " 'and',\n",
       " 'powerfully',\n",
       " 'apply',\n",
       " 'these',\n",
       " 'technique',\n",
       " 'to',\n",
       " 'new',\n",
       " 'problem',\n",
       " 'Finally',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'about',\n",
       " 'some',\n",
       " 'of',\n",
       " 'Silicon',\n",
       " 'Valley',\n",
       " \"'s\",\n",
       " 'best',\n",
       " 'practice',\n",
       " 'in',\n",
       " 'innovation',\n",
       " 'a',\n",
       " 'it',\n",
       " 'pertains',\n",
       " 'to',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'AI']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LemmaT2=[w.lemmatize() for w in wiki.words]\n",
    "LemmaT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LemmaT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>LancasterStemmer</th>\n",
       "      <th>PorterStemmer</th>\n",
       "      <th>SnowballStemmer</th>\n",
       "      <th>WordNetLemmatizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine</td>\n",
       "      <td>machin</td>\n",
       "      <td>machin</td>\n",
       "      <td>machin</td>\n",
       "      <td>Machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>learning</td>\n",
       "      <td>learn</td>\n",
       "      <td>learn</td>\n",
       "      <td>learn</td>\n",
       "      <td>learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>sci</td>\n",
       "      <td>scienc</td>\n",
       "      <td>scienc</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>machine</td>\n",
       "      <td>machin</td>\n",
       "      <td>machin</td>\n",
       "      <td>machin</td>\n",
       "      <td>machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>learning</td>\n",
       "      <td>learn</td>\n",
       "      <td>learn</td>\n",
       "      <td>learn</td>\n",
       "      <td>learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>AI</td>\n",
       "      <td>ai</td>\n",
       "      <td>ai</td>\n",
       "      <td>ai</td>\n",
       "      <td>AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word LancasterStemmer PorterStemmer SnowballStemmer WordNetLemmatizer\n",
       "0     Machine           machin        machin          machin           Machine\n",
       "1    learning            learn         learn           learn          learning\n",
       "2          is               is            is              is                is\n",
       "3         the              the           the             the               the\n",
       "4     science              sci        scienc          scienc           science\n",
       "..        ...              ...           ...             ...               ...\n",
       "162   machine           machin        machin          machin           machine\n",
       "163  learning            learn         learn           learn          learning\n",
       "164       and              and           and             and               and\n",
       "165        AI               ai            ai              ai                AI\n",
       "166         .                .             .               .                 .\n",
       "\n",
       "[167 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp1 = {'Word': tokens, 'LancasterStemmer': stem1, 'PorterStemmer': stem2, 'SnowballStemmer': stemS1,'WordNetLemmatizer': lemma1} \n",
    "    \n",
    "df_comp1 = pd.DataFrame(comp1)\n",
    "    \n",
    "df_comp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-707071ea2f9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf_comp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomp2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdf_comp2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\envs\\textAnalytics\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\envs\\textAnalytics\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         ]\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\envs\\textAnalytics\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\envs\\textAnalytics\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "#comp2 = {'Word': doc.token, 'SpacyLemmatizer': LemmS2,'TextblobLemmatizer': LemmaT2} \n",
    "\n",
    "comp2 = {'SpacyLemmatizer': LemmS2,'TextblobLemmatizer': LemmaT2} \n",
    "    \n",
    "    \n",
    "df_comp2 = pd.DataFrame(comp2)\n",
    "    \n",
    "df_comp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

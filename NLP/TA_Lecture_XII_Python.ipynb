{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://pypi.org/project/rank-bm25/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank-bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from rank-bm25) (1.22.4)\n",
      "Installing collected packages: rank-bm25\n",
      "Successfully installed rank-bm25-0.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module rank_bm25:\n",
      "\n",
      "NAME\n",
      "    rank_bm25\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        BM25\n",
      "            BM25L\n",
      "            BM25Okapi\n",
      "            BM25Plus\n",
      "    \n",
      "    class BM25(builtins.object)\n",
      "     |  BM25(corpus, tokenizer=None)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, corpus, tokenizer=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_batch_scores(self, query, doc_ids)\n",
      "     |  \n",
      "     |  get_scores(self, query)\n",
      "     |  \n",
      "     |  get_top_n(self, query, documents, n=5)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class BM25L(BM25)\n",
      "     |  BM25L(corpus, tokenizer=None, k1=1.5, b=0.75, delta=0.5)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BM25L\n",
      "     |      BM25\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, delta=0.5)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_batch_scores(self, query, doc_ids)\n",
      "     |      Calculate bm25 scores between query and subset of all docs\n",
      "     |  \n",
      "     |  get_scores(self, query)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BM25:\n",
      "     |  \n",
      "     |  get_top_n(self, query, documents, n=5)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BM25:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class BM25Okapi(BM25)\n",
      "     |  BM25Okapi(corpus, tokenizer=None, k1=1.5, b=0.75, epsilon=0.25)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BM25Okapi\n",
      "     |      BM25\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, epsilon=0.25)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_batch_scores(self, query, doc_ids)\n",
      "     |      Calculate bm25 scores between query and subset of all docs\n",
      "     |  \n",
      "     |  get_scores(self, query)\n",
      "     |      The ATIRE BM25 variant uses an idf function which uses a log(idf) score. To prevent negative idf scores,\n",
      "     |      this algorithm also adds a floor to the idf value of epsilon.\n",
      "     |      See [Trotman, A., X. Jia, M. Crane, Towards an Efficient and Effective Search Engine] for more info\n",
      "     |      :param query:\n",
      "     |      :return:\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BM25:\n",
      "     |  \n",
      "     |  get_top_n(self, query, documents, n=5)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BM25:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class BM25Plus(BM25)\n",
      "     |  BM25Plus(corpus, tokenizer=None, k1=1.5, b=0.75, delta=1)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BM25Plus\n",
      "     |      BM25\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, delta=1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_batch_scores(self, query, doc_ids)\n",
      "     |      Calculate bm25 scores between query and subset of all docs\n",
      "     |  \n",
      "     |  get_scores(self, query)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BM25:\n",
      "     |  \n",
      "     |  get_top_n(self, query, documents, n=5)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BM25:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FILE\n",
      "    c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages\\rank_bm25.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(rank_bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rank_bm25.BM25Okapi at 0x1c4ec4bda00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "corpus = [\n",
    "    \"Hello there good man!\",\n",
    "    \"It is quite windy in London\",\n",
    "    \"How is the weather today?\"\n",
    "]\n",
    "\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.93729472, 0.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"windy London\"\n",
    "tokenized_query = query.split(\" \")\n",
    "\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "# array([0.        , 0.93729472, 0.        ])\n",
    "doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It is quite windy in London']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.get_top_n(tokenized_query, corpus, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It is quite windy in London', 'How is the weather today?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.get_top_n(tokenized_query, corpus, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rank_bm25.BM25L at 0x1c4ec498d60>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25l = BM25L(tokenized_corpus)\n",
    "bm25l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 2.34061526, 0.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"windy London\"\n",
    "tokenized_query = query.split(\" \")\n",
    "\n",
    "doc_scoresl = bm25l.get_scores(tokenized_query)\n",
    "# array([0.        , 0.93729472, 0.        ])\n",
    "doc_scoresl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It is quite windy in London']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25l.get_top_n(tokenized_query, corpus, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It is quite windy in London', 'How is the weather today?']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25l.get_top_n(tokenized_query, corpus, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rank_bm25.BM25Plus at 0x1c4ee0db2b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25pl = BM25Plus(tokenized_corpus)\n",
    "bm25pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.77258872, 5.3162481 , 2.77258872])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_scorespl = bm25pl.get_scores(tokenized_query)\n",
    "# array([0.        , 0.93729472, 0.        ])\n",
    "doc_scorespl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It is quite windy in London']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25pl.get_top_n(tokenized_query, corpus, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It is quite windy in London', 'How is the weather today?']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25pl.get_top_n(tokenized_query, corpus, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trump Black History Month Speech.txt',\n",
       " 'Trump CIA Speech.txt',\n",
       " 'Trump Congressional Address.txt',\n",
       " 'Trump CPAC Speech.txt',\n",
       " 'Trump Florida Rally 2-18-17.txt',\n",
       " 'Trump Immigration Speech 8-31-16.txt',\n",
       " 'Trump Inauguration Speech.txt',\n",
       " 'Trump National Prayer Breakfast.txt',\n",
       " 'Trump Nomination Speech.txt',\n",
       " 'Trump Police Chiefs Speech.txt',\n",
       " 'Trump Response to Healthcare Bill Failure.txt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"F:\\\\donald_trump_speeches\"\n",
    "os.chdir(path)\n",
    "os.listdir()\n",
    "files=os.listdir()\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 2863: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-cf26a2db7211>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#read_text_file(file)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\anaconda\\envs\\textAnalytics\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2863: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "corpus=[]\n",
    "\n",
    "# iterate through all file\n",
    "for file in os.listdir():\n",
    "    #read_text_file(file)\n",
    "    with open(file, 'r') as f:\n",
    "        corpus.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"stopwords.txt\", 'r')\n",
    "#each_file_text=fin.read().strip()\n",
    "line = file1.read()\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 2863: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir():\n\u001b[0;32m      5\u001b[0m     f\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     corpus\u001b[38;5;241m.\u001b[39mappend(\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      7\u001b[0m     f\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\.conda\\envs\\TA\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2863: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "corpus=[]\n",
    "\n",
    "# iterate through all file\n",
    "for file in os.listdir():\n",
    "    f= open(file, 'r')\n",
    "    corpus.append(f.read())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 2863: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-4ef8a02f5d90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Could not read file:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\envs\\textAnalytics\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2863: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "try:\n",
    "    with open(fName, 'rb') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            pass #do stuff here\n",
    "    \n",
    "except IOError:\n",
    "    print \"Could not read file:\", fName\n",
    "    \n",
    "\"\"\"  \n",
    "\n",
    "corpus=[]\n",
    "line=\" \"\n",
    "\n",
    "# iterate through all file\n",
    "for file in os.listdir():\n",
    "    f= open(file, 'r')\n",
    "    try:\n",
    "            line=f.read()\n",
    "    except IOError:\n",
    "            print(\"Could not read file:\", file)    \n",
    "    corpus.append(line)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Well, the election, it came out really well. Next time weâ€™ll triple the number or quadruple it. We want to get it over 51, right? At least 51.\\n\\nWell this is Black History Month, so this is our little breakfast, our little get-together. Hi Lynn, how are you? Just a few notes. During this month, we honor the tremendous history of African-Americans throughout our country. Throughout the world, if you really think about it, right? And their story is one of unimaginable sacrifice, hard work, and faith in America. Iâ€™ve gotten a real glimpseâ€”during the campaign, Iâ€™d go around with Ben to a lot of different places I wasnâ€™t so familiar with. Theyâ€™re incredible people. And I want to thank Ben Carson, whoâ€™s gonna be heading up HUD. Thatâ€™s a big job. Thatâ€™s a job thatâ€™s not only housing, but itâ€™s mind and spirit. Right, Ben? And you understand, nobodyâ€™s gonna be better than Ben.\\n\\nLast month, we celebrated the life of Reverend Martin Luther King, Jr., whose incredible example is unique in American history. You read all about Dr. Martin Luther King a week ago when somebody said I took the statue out of my office. It turned out that that was fake news. Fake news. The statue is cherished, itâ€™s one of the favorite things in theâ€”and we have some good ones. We have Lincoln, and we have Jefferson, and we have Dr. Martin Luther King. But they said the statue, the bust of Martin Luther King, was taken out of the office. And it was never even touched. So I think it was a disgrace, but thatâ€™s the way the press is. Very unfortunate.\\n\\nI am very proud now that we have a museum on the National Mall where people can learn about Reverend King, so many other things. Frederick Douglass is an example of somebody whoâ€™s done an amazing job and is being recognized more and more, I noticed. Harriet Tubman, Rosa Parks, and millions more black Americans who made America what it is today. Big impact.\\n\\nIâ€™m proud to honor this heritage and will be honoring it more and more. The folks at the table in almost all cases have been great friends and supporters. Darrellâ€”I met Darrell when he was defending me on television. And the people that were on the other side of the argument didnâ€™t have a chance, right? And Paris has done an amazing job in a very hostile CNN community. Heâ€™s all by himself. Youâ€™ll have seven people, and Paris. And Iâ€™ll take Paris over the seven. But I donâ€™t watch CNN, so I donâ€™t get to see you as much as I used to. I donâ€™t like watching fake news. But Fox has treated me very nice. Wherever Fox is, thank you.\\n\\nWeâ€™re gonna need better schools and we need them soon. We need more jobs, we need better wages, a lot better wages. Weâ€™re gonna work very hard on the inner city. Ben is gonna be doing that, big league. Thatâ€™s one of the big things that youâ€™re gonna be looking at. We need safer communities and weâ€™re going to do that with law enforcement. Weâ€™re gonna make it safe. Weâ€™re gonna make it much better than it is right now. Right now itâ€™s terrible, and I saw you talking about it the other night, Paris, on something else that was reallyâ€”you did a fantastic job the other night on a very unrelated show.\\n\\nIâ€™m ready to do my part, and I will say this: Weâ€™re gonna work together. This is a great group, this is a group thatâ€™s been so special to me. You really helped me a lot. If you remember I wasnâ€™t going to do well with the African-American community, and after they heard me speaking and talking about the inner city and lots of other things, we ended up gettingâ€”and I wonâ€™t go into detailsâ€”but we ended up getting substantially more than other candidates who had run in the past years. And now weâ€™re gonna take that to new levels. I want to thank my television star over hereâ€”Omarosaâ€™s actually a very nice person, nobody knows that. I donâ€™t want to destroy her reputation but sheâ€™s a very good person, and sheâ€™s been helpful right from the beginning of the campaign, and I appreciate it. I really do. Very special.\\n\\nSo I want to thank everybody for being here.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    f= open(files[10], 'r')   # 1, 4, 7,8,9 \n",
    "    try:\n",
    "            line=f.read()\n",
    "    except IOError:\n",
    "            print(\"Could not read file:\", file)    \n",
    "    corpus.append(line)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in [1,4,7,8,9,10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "line=\" \"\n",
    "for i in [0,2,3,5,6,10]:\n",
    "    f= open(files[i], 'r')   # 1, 4, 7,8,9, Not working \n",
    "    try:\n",
    "            line=f.read()\n",
    "    except IOError:\n",
    "            print(\"Could not read file:\", file)    \n",
    "    corpus.append(line)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Well, the election, it came out really well. Next time weâ€™ll triple the number or quadruple it. We want to get it over 51, right? At least 51.\\n\\nWell this is Black History Month, so this is our little breakfast, our little get-together. Hi Lynn, how are you? Just a few notes. During this month, we honor the tremendous history of African-Americans throughout our country. Throughout the world, if you really think about it, right? And their story is one of unimaginable sacrifice, hard work, and faith in America. Iâ€™ve gotten a real glimpseâ€”during the campaign, Iâ€™d go around with Ben to a lot of different places I wasnâ€™t so familiar with. Theyâ€™re incredible people. And I want to thank Ben Carson, whoâ€™s gonna be heading up HUD. Thatâ€™s a big job. Thatâ€™s a job thatâ€™s not only housing, but itâ€™s mind and spirit. Right, Ben? And you understand, nobodyâ€™s gonna be better than Ben.\\n\\nLast month, we celebrated the life of Reverend Martin Luther King, Jr., whose incredible example is unique in American history. You read all about Dr. Martin Luther King a week ago when somebody said I took the statue out of my office. It turned out that that was fake news. Fake news. The statue is cherished, itâ€™s one of the favorite things in theâ€”and we have some good ones. We have Lincoln, and we have Jefferson, and we have Dr. Martin Luther King. But they said the statue, the bust of Martin Luther King, was taken out of the office. And it was never even touched. So I think it was a disgrace, but thatâ€™s the way the press is. Very unfortunate.\\n\\nI am very proud now that we have a museum on the National Mall where people can learn about Reverend King, so many other things. Frederick Douglass is an example of somebody whoâ€™s done an amazing job and is being recognized more and more, I noticed. Harriet Tubman, Rosa Parks, and millions more black Americans who made America what it is today. Big impact.\\n\\nIâ€™m proud to honor this heritage and will be honoring it more and more. The folks at the table in almost all cases have been great friends and supporters. Darrellâ€”I met Darrell when he was defending me on television. And the people that were on the other side of the argument didnâ€™t have a chance, right? And Paris has done an amazing job in a very hostile CNN community. Heâ€™s all by himself. Youâ€™ll have seven people, and Paris. And Iâ€™ll take Paris over the seven. But I donâ€™t watch CNN, so I donâ€™t get to see you as much as I used to. I donâ€™t like watching fake news. But Fox has treated me very nice. Wherever Fox is, thank you.\\n\\nWeâ€™re gonna need better schools and we need them soon. We need more jobs, we need better wages, a lot better wages. Weâ€™re gonna work very hard on the inner city. Ben is gonna be doing that, big league. Thatâ€™s one of the big things that youâ€™re gonna be looking at. We need safer communities and weâ€™re going to do that with law enforcement. Weâ€™re gonna make it safe. Weâ€™re gonna make it much better than it is right now. Right now itâ€™s terrible, and I saw you talking about it the other night, Paris, on something else that was reallyâ€”you did a fantastic job the other night on a very unrelated show.\\n\\nIâ€™m ready to do my part, and I will say this: Weâ€™re gonna work together. This is a great group, this is a group thatâ€™s been so special to me. You really helped me a lot. If you remember I wasnâ€™t going to do well with the African-American community, and after they heard me speaking and talking about the inner city and lots of other things, we ended up gettingâ€”and I wonâ€™t go into detailsâ€”but we ended up getting substantially more than other candidates who had run in the past years. And now weâ€™re gonna take that to new levels. I want to thank my television star over hereâ€”Omarosaâ€™s actually a very nice person, nobody knows that. I donâ€™t want to destroy her reputation but sheâ€™s a very good person, and sheâ€™s been helpful right from the beginning of the campaign, and I appreciate it. I really do. Very special.\\n\\nSo I want to thank everybody for being here.',\n",
       " 'Mr. Speaker, Mr. Vice President, Members of Congress, the First Lady of the United States, and Citizens of America:\\nTonight, as we mark the conclusion of our celebration of Black History Month, we are reminded of our Nation\\'s path toward civil rights and the work that still remains. Recent threats targeting Jewish Community Centers and vandalism of Jewish cemeteries, as well as last week\\'s shooting in Kansas City, remind us that while we may be a Nation divided on policies, we are a country that stands united in condemning hate and evil in all its forms.\\nEach American generation passes the torch of truth, liberty and justice --- in an unbroken chain all the way down to the present.\\nThat torch is now in our hands. And we will use it to light up the world. I am here tonight to deliver a message of unity and strength, and it is a message deeply delivered from my heart.\\nA new chapter of American Greatness is now beginning.\\nA new national pride is sweeping across our Nation.\\nAnd a new surge of optimism is placing impossible dreams firmly within our grasp.\\nWhat we are witnessing today is the Renewal of the American Spirit.\\nOur allies will find that America is once again ready to lead.\\nAll the nations of the world -- friend or foe -- will find that America is strong, America is proud, and America is free.\\nIn 9 years, the United States will celebrate the 250th anniversary of our founding -- 250 years since the day we declared our Independence.\\nIt will be one of the great milestones in the history of the world.\\nBut what will America look like as we reach our 250th year? What kind of country will we leave for our children?\\nI will not allow the mistakes of recent decades past to define the course of our future.\\nFor too long, we\\'ve watched our middle class shrink as we\\'ve exported our jobs and wealth to foreign countries.\\nWe\\'ve financed and built one global project after another, but ignored the fates of our children in the inner cities of Chicago, Baltimore, Detroit -- and so many other places throughout our land.\\nWe\\'ve defended the borders of other nations, while leaving our own borders wide open, for anyone to cross -- and for drugs to pour in at a now unprecedented rate.\\nAnd we\\'ve spent trillions of dollars overseas, while our infrastructure at home has so badly crumbled.\\nThen, in 2016, the earth shifted beneath our feet. The rebellion started as a quiet protest, spoken by families of all colors and creeds --- families who just wanted a fair shot for their children, and a fair hearing for their concerns.\\nBut then the quiet voices became a loud chorus -- as thousands of citizens now spoke out together, from cities small and large, all across our country.\\nFinally, the chorus became an earthquake -- and the people turned out by the tens of millions, and they were all united by one very simple, but crucial demand, that America must put its own citizens first ... because only then, can we truly MAKE AMERICA GREAT AGAIN.\\nDying industries will come roaring back to life. Heroic veterans will get the care they so desperately need.\\nOur military will be given the resources its brave warriors so richly deserve.\\nCrumbling infrastructure will be replaced with new roads, bridges, tunnels, airports and railways gleaming across our beautiful land.\\nOur terrible drug epidemic will slow down and ultimately, stop.\\nAnd our neglected inner cities will see a rebirth of hope, safety, and opportunity.\\nAbove all else, we will keep our promises to the American people.\\nIt\\'s been a little over a month since my inauguration, and I want to take this moment to update the Nation on the progress I\\'ve made in keeping those promises.\\nSince my election, Ford, Fiat-Chrysler, General Motors, Sprint, Softbank, Lockheed, Intel, Walmart, and many others, have announced that they will invest billions of dollars in the United States and will create tens of thousands of new American jobs.\\nThe stock market has gained almost three trillion dollars in value since the election on November 8th, a record. We\\'ve saved taxpayers hundreds of millions of dollars by bringing down the price of the fantastic new F-35 jet fighter, and will be saving billions more dollars on contracts all across our Government. We have placed a hiring freeze on non-military and non-essential Federal workers.\\nWe have begun to drain the swamp of government corruption by imposing a 5 year ban on lobbying by executive branch officials --- and a lifetime ban on becoming lobbyists for a foreign government.\\nWe have undertaken a historic effort to massively reduce jobâ€‘crushing regulations, creating a deregulation task force inside of every Government agency; imposing a new rule which mandates that for every 1 new regulation, 2 old regulations must be eliminated; and stopping a regulation that threatens the future and livelihoods of our great coal miners.\\nWe have cleared the way for the construction of the Keystone and Dakota Access Pipelines -- thereby creating tens of thousands of jobs -- and I\\'ve issued a new directive that new American pipelines be made with American steel.\\nWe have withdrawn the United States from the job-killing Trans-Pacific Partnership.\\nWith the help of Prime Minister Justin Trudeau, we have formed a Council with our neighbors in Canada to help ensure that women entrepreneurs have access to the networks, markets and capital they need to start a business and live out their financial dreams.\\nTo protect our citizens, I have directed the Department of Justice to form a Task Force on Reducing Violent Crime.\\nI have further ordered the Departments of Homeland Security and Justice, along with the Department of State and the Director of National Intelligence, to coordinate an aggressive strategy to dismantle the criminal cartels that have spread across our Nation.\\nWe will stop the drugs from pouring into our country and poisoning our youth -- and we will expand treatment for those who have become so badly addicted.\\nAt the same time, my Administration has answered the pleas of the American people for immigration enforcement and border security. By finally enforcing our immigration laws, we will raise wages, help the unemployed, save billions of dollars, and make our communities safer for everyone. We want all Americans to succeed --- but that can\\'t happen in an environment of lawless chaos. We must restore integrity and the rule of law to our borders.\\nFor that reason, we will soon begin the construction of a great wall along our southern border. It will be started ahead of schedule and, when finished, it will be a very effective weapon against drugs and crime.\\nAs we speak, we are removing gang members, drug dealers and criminals that threaten our communities and prey on our citizens. Bad ones are going out as I speak tonight and as I have promised.\\nTo any in Congress who do not believe we should enforce our laws, I would ask you this question: what would you say to the American family that loses their jobs, their income, or a loved one, because America refused to uphold its laws and defend its borders?\\nOur obligation is to serve, protect, and defend the citizens of the United States. We are also taking strong measures to protect our Nation from Radical Islamic Terrorism.\\nAccording to data provided by the Department of Justice, the vast majority of individuals convicted for terrorism-related offenses since 9/11 came here from outside of our country. We have seen the attacks at home --- from Boston to San Bernardino to the Pentagon and yes, even the World Trade Center.\\nWe have seen the attacks in France, in Belgium, in Germany and all over the world.\\nIt is not compassionate, but reckless, to allow uncontrolled entry from places where proper vetting cannot occur. Those given the high honor of admission to the United States should support this country and love its people and its values.\\nWe cannot allow a beachhead of terrorism to form inside America -- we cannot allow our Nation to become a sanctuary for extremists.\\nThat is why my Administration has been working on improved vetting procedures, and we will shortly take new steps to keep our Nation safe -- and to keep out those who would do us harm.\\nAs promised, I directed the Department of Defense to develop a plan to demolish and destroy ISIS -- a network of lawless savages that have slaughtered Muslims and Christians, and men, women, and children of all faiths and beliefs. We will work with our allies, including our friends and allies in the Muslim world, to extinguish this vile enemy from our planet.\\nI have also imposed new sanctions on entities and individuals who support Iran\\'s ballistic missile program, and reaffirmed our unbreakable alliance with the State of Israel.\\nFinally, I have kept my promise to appoint a Justice to the United States Supreme Court -- from my list of 20 judges -- who will defend our Constitution. I am honored to have Maureen Scalia with us in the gallery tonight. Her late, great husband, Antonin Scalia, will forever be a symbol of American justice. To fill his seat, we have chosen Judge Neil Gorsuch, a man of incredible skill, and deep devotion to the law. He was confirmed unanimously to the Court of Appeals, and I am asking the Senate to swiftly approve his nomination.\\nTonight, as I outline the next steps we must take as a country, we must honestly acknowledge the circumstances we inherited.\\nNinety-four million Americans are out of the labor force.\\nOver 43 million people are now living in poverty, and over 43 million Americans are on food stamps.\\nMore than 1 in 5 people in their prime working years are not working.\\nWe have the worst financial recovery in 65 years.\\nIn the last 8 years, the past Administration has put on more new debt than nearly all other Presidents combined.\\nWe\\'ve lost more than one-fourth of our manufacturing jobs since NAFTA was approved, and we\\'ve lost 60,000 factories since China joined the World Trade Organization in 2001.\\nOur trade deficit in goods with the world last year was nearly $800 billion dollars.\\nAnd overseas, we have inherited a series of tragic foreign policy disasters.\\nSolving these, and so many other pressing problems, will require us to work past the differences of party. It will require us to tap into the American spirit that has overcome every challenge throughout our long and storied history.\\nBut to accomplish our goals at home and abroad, we must restart the engine of the American economy -- making it easier for companies to do business in the United States, and much harder for companies to leave.\\nRight now, American companies are taxed at one of the highest rates anywhere in the world.\\nMy economic team is developing historic tax reform that will reduce the tax rate on our companies so they can compete and thrive anywhere and with anyone. At the same time, we will provide massive tax relief for the middle class.\\nWe must create a level playing field for American companies and workers.\\nCurrently, when we ship products out of America, many other countries make us pay very high tariffs and taxes -- but when foreign companies ship their products into America, we charge them almost nothing.\\nI just met with officials and workers from a great American company, Harley-Davidson. In fact, they proudly displayed five of their magnificent motorcycles, made in the USA, on the front lawn of the White House.\\nAt our meeting, I asked them, how are you doing, how is business? They said that it\\'s good. I asked them further how they are doing with other countries, mainly international sales. They told me -- without even complaining because they have been mistreated for so long that they have become used to it -- that it is very hard to do business with other countries because they tax our goods at such a high rate. They said that in one case another country taxed their motorcycles at 100 percent.\\nThey weren\\'t even asking for change. But I am.\\nI believe strongly in free trade but it also has to be FAIR TRADE.\\nThe first Republican President, Abraham Lincoln, warned that the \"abandonment of the protective policy by the American Government [will] produce want and ruin among our people.\"\\nLincoln was right -- and it is time we heeded his words. I am not going to let America and its great companies and workers, be taken advantage of anymore.\\nI am going to bring back millions of jobs. Protecting our workers also means reforming our system of legal immigration. The current, outdated system depresses wages for our poorest workers, and puts great pressure on taxpayers.\\nNations around the world, like Canada, Australia and many others --- have a merit-based immigration system. It is a basic principle that those seeking to enter a country ought to be able to support themselves financially. Yet, in America, we do not enforce this rule, straining the very public resources that our poorest citizens rely upon. According to the National Academy of Sciences, our current immigration system costs America\\'s taxpayers many billions of dollars a year.\\nSwitching away from this current system of lower-skilled immigration, and instead adopting a merit-based system, will have many benefits: it will save countless dollars, raise workers\\' wages, and help struggling families --- including immigrant families --- enter the middle class.\\nI believe that real and positive immigration reform is possible, as long as we focus on the following goals: to improve jobs and wages for Americans, to strengthen our nation\\'s security, and to restore respect for our laws.\\nIf we are guided by the well-being of American citizens then I believe Republicans and Democrats can work together to achieve an outcome that has eluded our country for decades.\\nAnother Republican President, Dwight D. Eisenhower, initiated the last truly great national infrastructure program --- the building of the interstate highway system. The time has come for a new program of national rebuilding.\\nAmerica has spent approximately six trillion dollars in the Middle East, all this while our infrastructure at home is crumbling. With this six trillion dollars we could have rebuilt our country --- twice. And maybe even three times if we had people who had the ability to negotiate.\\nTo launch our national rebuilding, I will be asking the Congress to approve legislation that produces a $1 trillion investment in the infrastructure of the United States -- financed through both public and private capital --- creating millions of new jobs.\\nThis effort will be guided by two core principles: Buy American, and Hire American.\\nTonight, I am also calling on this Congress to repeal and replace Obamacare with reforms that expand choice, increase access, lower costs, and at the same time, provide better Healthcare.\\nMandating every American to buy government-approved health insurance was never the right solution for America. The way to make health insurance available to everyone is to lower the cost of health insurance, and that is what we will do.\\nObamacare premiums nationwide have increased by double and triple digits. As an example, Arizona went up 116 percent last year alone. Governor Matt Bevin of Kentucky just said Obamacare is failing in his State -- it is unsustainable and collapsing.\\nOne third of counties have only one insurer on the exchanges --- leaving many Americans with no choice at all.\\nRemember when you were told that you could keep your doctor, and keep your plan?\\nWe now know that all of those promises have been broken.\\nObamacare is collapsing --- and we must act decisively to protect all Americans. Action is not a choice --- it is a necessity.\\nSo I am calling on all Democrats and Republicans in the Congress to work with us to save Americans from this imploding Obamacare disaster.\\nHere are the principles that should guide the Congress as we move to create a better healthcare system for all Americans:\\nFirst, we should ensure that Americans with pre-existing conditions have access to coverage, and that we have a stable transition for Americans currently enrolled in the healthcare exchanges.\\nSecondly, we should help Americans purchase their own coverage, through the use of tax credits and expanded Health Savings Accounts --- but it must be the plan they want, not the plan forced on them by the Government.\\nThirdly, we should give our great State Governors the resources and flexibility they need with Medicaid to make sure no one is left out.\\nFourthly, we should implement legal reforms that protect patients and doctors from unnecessary costs that drive up the price of insurance -- and work to bring down the artificially high price of drugs and bring them down immediately.\\nFinally, the time has come to give Americans the freedom to purchase health insurance across State lines --- creating a truly competitive national marketplace that will bring cost way down and provide far better care.\\nEverything that is broken in our country can be fixed. Every problem can be solved. And every hurting family can find healing, and hope.\\nOur citizens deserve this, and so much more --- so why not join forces to finally get it done? On this and so many other things, Democrats and Republicans should get together and unite for the good of our country, and for the good of the American people.\\nMy administration wants to work with members in both parties to make childcare accessible and affordable, to help ensure new parents have paid family leave, to invest in women\\'s health, and to promote clean air and clear water, and to rebuild our military and our infrastructure.\\nTrue love for our people requires us to find common ground, to advance the common good, and to cooperate on behalf of every American child who deserves a brighter future.\\nAn incredible young woman is with us this evening who should serve as an inspiration to us all.\\nToday is Rare Disease day, and joining us in the gallery is a Rare Disease Survivor, Megan Crowley. Megan was diagnosed with Pompe Disease, a rare and serious illness, when she was 15 months old. She was not expected to live past 5.\\nOn receiving this news, Megan\\'s dad, John, fought with everything he had to save the life of his precious child. He founded a company to look for a cure, and helped develop the drug that saved Megan\\'s life. Today she is 20 years old -- and a sophomore at Notre Dame.\\nMegan\\'s story is about the unbounded power of a father\\'s love for a daughter.\\nBut our slow and burdensome approval process at the Food and Drug Administration keeps too many advances, like the one that saved Megan\\'s life, from reaching those in need.\\nIf we slash the restraints, not just at the FDA but across our Government, then we will be blessed with far more miracles like Megan.\\nIn fact, our children will grow up in a Nation of miracles.\\nBut to achieve this future, we must enrich the mind --- and the souls --- of every American child.\\nEducation is the civil rights issue of our time.\\nI am calling upon Members of both parties to pass an education bill that funds school choice for disadvantaged youth, including millions of African-American and Latino children. These families should be free to choose the public, private, charter, magnet, religious or home school that is right for them.\\nJoining us tonight in the gallery is a remarkable woman, Denisha Merriweather. As a young girl, Denisha struggled in school and failed third grade twice. But then she was able to enroll in a private center for learning, with the help of a tax credit scholarship program. Today, she is the first in her family to graduate, not just from high school, but from college. Later this year she will get her masters degree in social work.\\nWe want all children to be able to break the cycle of poverty just like Denisha.\\nBut to break the cycle of poverty, we must also break the cycle of violence.\\nThe murder rate in 2015 experienced its largest single-year increase in nearly half a century.\\nIn Chicago, more than 4,000 people were shot last year alone --- and the murder rate so far this year has been even higher.\\nThis is not acceptable in our society.\\nEvery American child should be able to grow up in a safe community, to attend a great school, and to have access to a high-paying job.\\nBut to create this future, we must work with --- not against --- the men and women of law enforcement.\\nWe must build bridges of cooperation and trust --- not drive the wedge of disunity and division.\\nPolice and sheriffs are members of our community. They are friends and neighbors, they are mothers and fathers, sons and daughters -- and they leave behind loved ones every day who worry whether or not they\\'ll come home safe and sound.\\nWe must support the incredible men and women of law enforcement.\\nAnd we must support the victims of crime.\\nI have ordered the Department of Homeland Security to create an office to serve American Victims. The office is called VOICE --- Victims Of Immigration Crime Engagement. We are providing a voice to those who have been ignored by our media, and silenced by special interests.\\nJoining us in the audience tonight are four very brave Americans whose government failed them.\\nTheir names are Jamiel Shaw, Susan Oliver, Jenna Oliver, and Jessica Davis.\\nJamiel\\'s 17-year-old son was viciously murdered by an illegal immigrant gang member, who had just been released from prison. Jamiel Shaw Jr. was an incredible young man, with unlimited potential who was getting ready to go to college where he would have excelled as a great quarterback. But he never got the chance. His father, who is in the audience tonight, has become a good friend of mine.\\nAlso with us are Susan Oliver and Jessica Davis. Their husbands --- Deputy Sheriff Danny Oliver and Detective Michael Davis --- were slain in the line of duty in California. They were pillars of their community. These brave men were viciously gunned down by an illegal immigrant with a criminal record and two prior deportations.\\nSitting with Susan is her daughter, Jenna. Jenna: I want you to know that your father was a hero, and that tonight you have the love of an entire country supporting you and praying for you.\\nTo Jamiel, Jenna, Susan and Jessica: I want you to know --- we will never stop fighting for justice. Your loved ones will never be forgotten, we will always honor their memory.\\nFinally, to keep America Safe we must provide the men and women of the United States military with the tools they need to prevent war and --- if they must --- to fight and to win.\\nI am sending the Congress a budget that rebuilds the military, eliminates the Defense sequester, and calls for one of the largest increases in national defense spending in American history.\\nMy budget will also increase funding for our veterans.\\nOur veterans have delivered for this Nation --- and now we must deliver for them.\\nThe challenges we face as a Nation are great. But our people are even greater.\\nAnd none are greater or braver than those who fight for America in uniform.\\nWe are blessed to be joined tonight by Carryn Owens, the widow of a U.S. Navy Special Operator, Senior Chief William \"Ryan\" Owens. Ryan died as he lived: a warrior, and a hero --- battling against terrorism and securing our Nation.\\nI just spoke to General Mattis, who reconfirmed that, and I quote, \"Ryan was a part of a highly successful raid that generated large amounts of vital intelligence that will lead to many more victories in the future against our enemies.\" Ryan\\'s legacy is etched into eternity. For as the Bible teaches us, there is no greater act of love than to lay down one\\'s life for one\\'s friends. Ryan laid down his life for his friends, for his country, and for our freedom --- we will never forget him.\\nTo those allies who wonder what kind of friend America will be, look no further than the heroes who wear our uniform.\\nOur foreign policy calls for a direct, robust and meaningful engagement with the world. It is American leadership based on vital security interests that we share with our allies across the globe.\\nWe strongly support NATO, an alliance forged through the bonds of two World Wars that dethroned fascism, and a Cold War that defeated communism.\\nBut our partners must meet their financial obligations.\\nAnd now, based on our very strong and frank discussions, they are beginning to do just that.\\nWe expect our partners, whether in NATO, in the Middle East, or the Pacific --- to take a direct and meaningful role in both strategic and military operations, and pay their fair share of the cost.\\nWe will respect historic institutions, but we will also respect the sovereign rights of nations.\\nFree nations are the best vehicle for expressing the will of the people --- and America respects the right of all nations to chart their own path. My job is not to represent the world. My job is to represent the United States of America. But we know that America is better off, when there is less conflict -- not more.\\nWe must learn from the mistakes of the past --- we have seen the war and destruction that have raged across our world.\\nThe only long-term solution for these humanitarian disasters is to create the conditions where displaced persons can safely return home and begin the long process of rebuilding.\\nAmerica is willing to find new friends, and to forge new partnerships, where shared interests align. We want harmony and stability, not war and conflict.\\nWe want peace, wherever peace can be found. America is friends today with former enemies. Some of our closest allies, decades ago, fought on the opposite side of these World Wars. This history should give us all faith in the possibilities for a better world.\\nHopefully, the 250th year for America will see a world that is more peaceful, more just and more free.\\nOn our 100th anniversary, in 1876, citizens from across our Nation came to Philadelphia to celebrate America\\'s centennial. At that celebration, the country\\'s builders and artists and inventors showed off their creations.\\nAlexander Graham Bell displayed his telephone for the first time.\\nRemington unveiled the first typewriter. An early attempt was made at electric light.\\nThomas Edison showed an automatic telegraph and an electric pen.\\nImagine the wonders our country could know in America\\'s 250th year.\\nThink of the marvels we can achieve if we simply set free the dreams of our people.\\nCures to illnesses that have always plagued us are not too much to hope.\\nAmerican footprints on distant worlds are not too big a dream.\\nMillions lifted from welfare to work is not too much to expect.\\nAnd streets where mothers are safe from fear -- schools where children learn in peace -- and jobs where Americans prosper and grow -- are not too much to ask.\\nWhen we have all of this, we will have made America greater than ever before. For all Americans.\\nThis is our vision. This is our mission.\\nBut we can only get there together.\\nWe are one people, with one destiny.\\nWe all bleed the same blood.\\nWe all salute the same flag.\\nAnd we are all made by the same God.\\nAnd when we fulfill this vision; when we celebrate our 250 years of glorious freedom, we will look back on tonight as when this new chapter of American Greatness began.\\nThe time for small thinking is over. The time for trivial fights is behind us.\\nWe just need the courage to share the dreams that fill our hearts.\\nThe bravery to express the hopes that stir our souls.\\nAnd the confidence to turn those hopes and dreams to action.\\nFrom now on, America will be empowered by our aspirations, not burdened by our fears ---\\ninspired by the future, not bound by the failures of the past ---\\nand guided by our vision, not blinded by our doubts.\\nI am asking all citizens to embrace this Renewal of the American Spirit. I am asking all members of Congress to join me in dreaming big, and bold and daring things for our country. And I am asking everyone watching tonight to seize this moment and --\\nBelieve in yourselves.\\nBelieve in your future.\\nAnd believe, once more, in America.\\nThank you, God bless you, and God Bless these United States.',\n",
       " ' Thank you, everybody. So great to be with you. Thank you. \\nGreat to be back at CPAC. It\\'s a place I have really...\\n\\n I love this place.\\n\\nLove you people. So thank you -- thank you very much.\\nFirst of all, I want to thank Matt Schlapp and his very, very incredible wife, and boss, Mercedes, who have been fantastic friends and supporters and so great. When I watch them on television defending me, nobody has a chance.\\nSo I want to thank Matt and Mercedes.\\n\\nAnd when Matt called and asked I said, \"Absolutely I\\'ll be there with you.\" I mean, the real reason I said it, I didn\\'t want him to go against me cause that...\\n\\n... that one you can\\'t beat.\\nSo I said absolutely.\\nAnd it really is an honor to be here. I wouldn\\'t miss a chance to talk to my friends. These are my friends.\\n\\nAnd we\\'ll see you again next year and the year after that and I\\'ll be doing this...\\n\\n... I\\'ll be doing this with CPAC whenever I can and I\\'ll make sure that we\\'re here a lot.\\n\\n You know if you remember my first major speech -- sit down everybody. C\\'mon.\\n\\nYou know the dishonest media they\\'ll say, \"He didn\\'t get a standing ovation.\" You know why?\\n\\nNo -- you know why? Because everybody stood and nobody sat. So they will say, \"He never got a standing ovation,\" right? \\nThey are the worst.\\n\\n\\n So -- sit down.\\n\\n\"Donald Trump did not get a standing ovation.\"\\n\\nThey leave out the part, \"They never sat down.\" They leave that out.\\nSo I just want to thank -- but you know my first major speak was at CPAC and -- probably five or six years ago. First major political speech. And you were there. And it was -- I loved it. I loved the people. I loved the commotion.\\nAnd then they did these polls were I went through the roof and I wasn\\'t even running, right?\\n\\nBut it gave me an idea. And I got a little bit concerned when I saw what was happening in the country. And I said, \"Let\\'s go to it.\"\\nSo, it was very exciting. I walked the stage on CPAC. I\\'ll never forget it, really. I had very little notes and even less preparation. So when you have practically no notes and no preparation and then you leave and everybody was thrilled, I said, \"I think I like this business.\"\\nRead More: President Trump Just Boasted About Winning Polls at CPAC. He Lost 6 Times\\n\\nI would\\'ve come last year but I was worried that I would be, at that time, too controversial. We wanted border security. We wanted very, very strong military. We wanted all of the things that we\\'re going to get...\\n\\n... and people consider that controversial but you didn\\'t consider it controversial.\\n\\n So, I\\'ve been with CPAC for a long time. All of these years we\\'ve been together. And now you finally have a president, finally. Took you a long time.\\n\\nTook you a long time.\\n\\nAnd it\\'s patriots like you that made it happen, believe me. Believe me.\\nYou did it because you love your country, because you want a better future for your children and because you want to make America great again.\\n\\nThe media didn\\'t think we would win. The...\\n ... pundits -- you\\'re right. They had an idea.\\nThe pundits didn\\'t think we\\'d win. The consultants that suck up all that money -- oh, they suck it up. They\\'re so good.\\n\\nThey\\'re not good at politics, but they\\'re really good at sucking up people\\'s money. Especially my opponents, because I kept them down to a minimum.\\nBut the consultants didn\\'t think we would win.\\nBut they all underestimated the power of the people: You. And the people proved them...\\n\\n... totally wrong. Never -- and -- and this is so true. And this is what\\'s been happening. Never underestimate the people. Never.\\n\\nI don\\'t think it\\'ll ever happen again.\\nAnd I want you all to know that we are fighting the fake news. It\\'s fake, phony, fake.\\n\\nA few days ago I called the fake news the enemy of the people. And they are. They are the enemy of the people.\\n\\nBecause they have no sources, they just make \\'em up when there are none. I saw one story recently where they said, \"Nine people have confirmed.\" There\\'re no nine people. I don\\'t believe there was one or two people. Nine people.\\nAnd I said, \"Give me a break.\" Because I know the people, I know who they talk to. There were no nine people.\\nBut they say \"nine people.\" And somebody reads it and they think, \"Oh, nine people. They have nine sources.\" They make up sources.\\nThey\\'re very dishonest people. In fact, in covering my comments, the dishonest media did not explain that I called the fake news the enemy of the people. The fake news. They dropped off the word \"fake.\" And all of a sudden the story became the media is the enemy.\\nThey take the word \"fake\" out. And now I\\'m saying, \"Oh no, this is no good.\" But that\\'s the way they are.\\nSo I\\'m not against the media, I\\'m not against the press. I don\\'t mind bad stories if I deserve them.\\nAnd I tell ya, I love good stories, but we don\\'t go...\\n\\nI don\\'t get too many of them.\\nBut I am only against the fake news, media or press. Fake, fake. They have to leave that word.\\nI\\'m against the people that make up stories and make up sources.\\nThey shouldn\\'t be allowed to use sources unless they use somebody\\'s name. Let their name be put out there. Let their name be put out.\\n\\n\"A source says that Donald Trump is a horrible, horrible human being.\" Let \\'em say it to my face.\\n\\nLet there be no more sources.\\nAnd remember this -- and in not -- in all cases. I mean, I had a story written yesterday about me in Reuters by a very honorable man. It was a very fair story.\\nThere are some great reporters around. They\\'re talented, they\\'re honest as the day is long. They\\'re great.\\nBut there are some terrible dishonest people and they do a tremendous disservice to our country and to our people. A tremendous disservice. They are very dishonest people.\\nAnd they shouldn\\'t use sources. They should put the name of the person. You will see stories dry up like you\\'ve never seen before.\\nSo you have no idea how bad it is, because if you are not part of the story -- and I put myself in your position sometimes. Because many of you, you\\'re not part of the story. And if you\\'re not part of the story, you know, then you, sort of, know -- if you are part of the story, you know what they\\'re saying is true or not.\\n So when they make it up -- and they make up something else, and you saw that before the election: polls, polls. The polls. They come out with these polls and everybody was so surprised.\\nActually, a couple polls got it right. I must say Los Angeles Times did a great job, shocking because, you know, they did a great job.\\n\\nAnd we had a couple of others that were right, but generally speaking, I mean, can tell you the network, somebody said a poll came out. And I say, what network is it? And they\\'ll say, a certain -- let\\'s not even mention names, right? Shall we?\\n\\nWell, you have a lot of them. Look, the Clinton News Network is one.\\n\\nTotally. Take a look. Honestly. Take a look, honestly. Take a look at polls over the last two years. Now you\\'d think they would fire the pollster, right? After years and years of getting battered, but I -- who knows, maybe they are just bad at polling or maybe they\\'re not legit, but it\\'s one or the other, look at how inaccurate -- look at CBS, look at ABC, also, look at NBC, take a look at some of these polls. They\\'re so bad, so inaccurate and what that does is it creates a false narrative.\\nIt creates like this narrative that is just like we\\'re not going to win, people say, I love Trump, but you know, I\\'m not feeling great today, he can\\'t win, so I won\\'t go and vote. I won\\'t go and vote. It creates a whole false deal. And we have to fight it, folks, we have to fight it. They\\'re very smart, they\\'re very cunning and they\\'re very dishonest.\\nSo just to conclude, I mean, it\\'s a very sensitive topic and they get upset when we expose their false stories. They say that we can\\'t criticize their dishonest coverage because of the First Amendment, you know, they always bring up the First Amendment.\\n\\nAnd I love the First Amendment; nobody loves it better than me. Nobody.\\n\\nI mean, who use its more than I do? But the First Amendment gives all of us -- it gives it to me, it gives it to you, it gives it to all Americans, the right to speak our minds freely. It gives you the right and me the right to criticize fake news and criticize it strongly.\\n\\nAnd many of these groups are part the large media corporations that have their own agenda and it\\'s not your agenda and it\\'s not the country\\'s agenda, it\\'s their own agenda. They have a professional obligation as members of the press to report honestly. But as you saw throughout the entire campaign, and even now, the fake news doesn\\'t tell the truth. Doesn\\'t tell the truth.\\nSo just in finishing, I say it doesn\\'t represent the people, it doesn\\'t tell the never will represent the people, and we\\'re going to do something about it because we have to go out and have to speak our minds and we have to be honest. Our victory was a win like nobody has ever seen before.\\n\\nI\\'m here fighting for you and I will continue to fight for you. The victory and the win were something that really was dedicated to a country and people that believe in freedom, security and the rule of law. Our victory...\\n\\n... was a victory and the win for conservative values.\\n\\nAnd our victory was a win for everyone who believes it\\'s time to stand up for America, to stand up for the American worker and to stand up for the American flag.\\n\\nYeah, there we should stand up. Come on. There we should stand up. OK. And by the way, we love our flag. By the way, you folks are in here -- this place is packed, there are lines that go back six blocks and I tell you that because you won\\'t read about it, OK.\\n\\nBut there are lines that go back six blocks, there is such love in this country for everything we stand for, you saw that on Election Day.\\n\\n And you\\'re going to see it more and more.\\n\\nSo we\\'re all part of this very historic movement, a movement the likes of which, actually, the world has never seen before. There\\'s never been anything like this. There\\'s been some movements, but there\\'s never been anything like this.\\nThere\\'s been some movements that petered out like Bernie, petered out.\\n\\nBut it was a little rigged against him, you know, super delegate, super delegate. She had so many delegates before the thing even started, I actually said to my people, how does that happen? Not that I\\'m a fan of Bernie, but a lot of Bernie people voted for Trump, you know why? Because he\\'s right on one issue; trade. He was right about trade. Our country is being absolutely devastated with bad trade deals. So he was right about that, but we got a lot of Bernie support. So actually, I like Bernie. OK? I like Bernie.\\n\\nBut I\\'m here today to tell you what this movement means for the future of the Republican party and for the future of America. First we need to define what this great, great unprecedented movement is and what it actually represents. The core conviction of our movement is that we are a nation that put and will put its own citizens first.\\n\\n\\n For too long, we\\'ve traded away our jobs to other countries. So terrible. We\\'ve defended other nations\\' borders while leaving ours wide open, anybody can come in. We\\'re going to build a wall, don\\'t worry about it. We\\'re building the wall. We\\'re building the wall. In fact, it\\'s going to start soon. Way ahead of schedule, way ahead of schedule.\\n Way, way, way ahead of schedule. It\\'s going to start very soon. General Kelly by the way has done a fantastic job. Fantastic job he\\'s done.\\n\\nAnd remember, we are getting the bad ones out. These are bad dudes. We\\'re getting the bad ones out, OK? We\\'re getting the bad -- if you watch these people, it\\'s like gee, that\\'s so sad. We\\'re getting bad people out of this country, people that shouldn\\'t be whether it\\'s drugs or murder or other things. We\\'re getting bad ones out, those are the ones that go first and I said it from day one. Basically all I\\'ve done is keep my promise.\\n\\nWe\\'ve spent trillions of dollars overseas, while allowing our own infrastructure to fall into total disrepair and decay. In the Middle East, we\\'ve spent as of four weeks ago, $6 trillion. Think of it.\\n\\nAnd by the way, the Middle East is in -- I mean, it\\'s not even close, it\\'s in much worse shape than it was 15 years ago. If our presidents would have gone to the beach for 15 years, we would be in much better shape than we are right now, that I can tell you.\\n\\nBe a hell of a lot better. We could have rebuilt our country three times with that money. This is the situation that I inherited. I inherited a mess, believe me. We also inherited a failed health care law that threatens our medical system with absolute and total catastrophe. Now, I\\'ve been watching and nobody says it, but Obamacare doesn\\'t work, folks. I mean, I could say, I could talk, it doesn\\'t work.\\nAnd now people are starting to develop a little warm heart, but the people that you\\'re watching, they\\'re not you. They\\'re largely -- many of them are the side that lost, you know they lost the election. It\\'s like, how many elections do we have to have? They lost the election. But I always say, Obamacare doesn\\'t work. And these same people two years ago and a year ago were complaining about Obamacare.\\n And the bottom line, we\\'re changing it. We\\'re going to make it much better, we\\'re going to make it less expensive. We\\'re going to make it much better.\\nObamacare covers very few people -- and remember, deduct from the number all of the people that had great health care that they loved that was taken away from them -- it was taken away from them.\\n\\nMillions of people were very happy with their health care, they had their doctor, they had their plan. Remember the lie, 28 times. You could keep your doctor, you could keep your plan. Over and over and over again, you heard it. So we\\'re going to repeal and replace Obamacare.\\n\\nAnd I tell Paul Ryan and all of the folks that we\\'re working with very hard, Dr. Tom Price, very talented guy.\\n\\nBut I tell them from a purely political standpoint, the single best thing we can do is nothing. Let it implode completely, it\\'s already imploding. You see the carriers are all leaving. I mean, it\\'s a disaster. But two years, don\\'t do anything. The Democrats will come to us and beg for help, they\\'ll beg and it\\'s their problem. But it\\'s not the right thing to do for the American people, it\\'s not the right thing to do.\\n\\nWe inherited a national debt that has doubled in eight years, think of it, $20 trillion, it\\'s doubled. We inherited a foreign policy marked by one disaster after another. We don\\'t win anymore. When was the last time we won? Did we win a war? Did we win anything? Do we win anything? We\\'re going to win. We\\'re going on win big, folks. We\\'re going to start winning again, believe me. We\\'re gonna win.\\n\\n But we\\'re taking a firm, bold and decisive measure, we have to, to turn things around. The era of empty talk is over, it\\'s over. \\nNow is the time for action. So let me tell you about the actions that we\\'re taking right now to deliver on our promise to the American people and on my promise to make America great again. We\\'ve taken swift and strong action to secure the southern border of the United States and to begin the construction of a great, great border wall.\\n\\nBy doing this, and with the help of our great border police, with the help of ICE, with the help of General Kelly and all of the people that are so passionate about this -- our border patrol, I\\'ll tell you what they do, they came and endorsed me, ICE came and endorsed me. They never endorsed a presidential candidate before, they might not even be allowed to.\\n\\nBut they were disgusted with what they saw and we\\'ll stop it. We\\'ll stop the drugs from pouring into our nation and poisoning our youth. Pouring in. Pouring in.\\n\\nWe get the drugs, they get the money. We get the problems, they get the cash. No good. No good. Going to stop. By stopping the flow of illegal immigration, we will save countless tax dollars (ph), and that\\'s so important because the tax -- the dollars we\\'re losing are beyond anything that you can imagine. And the tax dollars that can be used to rebuild struggling American communities, including our inner cities.\\n\\nWe are also going to save countless American lives. As we speak today, immigration offers are finding the gang members, the drug dealers and the criminal aliens and throwing them the hell out of our country.\\n\\n And we will not let them back in. They\\'re not coming back in, folks. They do, they\\'re going to have bigger problems than they ever dreamt of.\\n\\nI\\'m also working with the Department of Justice to being reducing violent crime. I mean, can you believe what\\'s happening in Chicago as an example? Two days ago, seven people were shot and I believe killed. Seven people, seven people, Chicago, a great American city, seven people shot and killed. We will support the incredible men and women of law enforcement.\\n\\nThank you, and thank them. I\\'ve also followed through on my campaigning promise and withdrawn America from the Trans-Pacific Partnership.\\n\\nSo that we can protect our economic freedom. And we\\'re going to make trade deals, but we\\'re going to do one on one, one on one, and if they misbehave, we terminate the deal and then they\\'ll come back and we\\'ll make a better deal. None of these big quagmire deals that are a disaster.\\nJust take a look -- by the way, take a look at NAFTA, one of the worst deals ever made by any country, having to do with economic development. It\\'s economy un-development, as far as our country is concerned.\\nWe\\'re preparing to repeal and replace the disaster known as Obamacare. We\\'re going to save Americans from this crisis, and give them the access to the quality health care they need and deserve. We have authorized the construction, one day, of the Keystone and Dakota Access Pipelines.\\n\\nAnd issued a new rule -- this took place while I was getting ready to sign. I said who makes the pipes for the pipeline? Well sir, it comes from all over the world, isn\\'t that wonderful? I said nope, comes from the United States, or we\\'re not building it. \\nAmerican steel.\\n\\nIf they want a pipeline in the United States, they\\'re going to use pipe that\\'s made in the United States. Do we agree?\\n\\nBut can you imagine -- I told this story the other day -- can you imagine the gentleman -- never met him, don\\'t even know the name of this company. I actually sort of know it, but I want to get it exactly correct. Big, big powerful company, they spend hundreds of millions of dollars on the pipeline, same thing with the Dakota, different vice. They got their approvals, everything, in the case of Dakota, then all of a sudden they couldn\\'t connect it because they had people protesting that never showed up before.\\nBut with the Keystone -- so they spent hundreds of millions of dollars with bloodsucker consultants -- you know, sucking the blood out of the company, don\\'t worry, I\\'ve used them all my life, OK? Don\\'t worry, we\\'re going to get it approved, I\\'m connected, I\\'m a lobbyist, don\\'t worry. Bottom line, Obama didn\\'t sign it, right? Could be 42,000 jobs, somewhere around there -- a lot of jobs. Didn\\'t sign it.\\nBut can you imagine, he gave up. A year ago, it was dead, now he\\'s doing anything, calling his wife, hello darling, I\\'m a little bored, you know that pipeline that has killed us, that has killed our company. Knock, knock. Mr. so and so, the Keystone Pipeline sir, out of nowhere has just been approved.\\n\\nNow, can you imagine the expression? And you know the sad part? Those same blood sucking consultants that hit them for all the money and failed, they\\'re now going to go back to him and say, did we do a great job; we want more money, right? Because that\\'s the way the system works, a little bit off, but that\\'s the way the system works.\\nWe\\'re preparing bold action to lift the restrictions on American energy, including shale oil, natural gas and beautiful clean coal and we\\'re going to put our miners back to work.\\n\\n Miners are going back to work, folks. Sorry to tell you that, but they\\'re going back to work.\\nWe have begun a historic program to reduce the regulations that are crushing our economy, crushing.\\n\\nAnd not only our economy, crushing our jobs because companies can\\'t hire. We\\'re going to put the regulation industry out of work and out of business.\\n\\nAnd by the way, I want regulation. I want to protect our environment, I want regulations for safety, I want all of the regulations that we need and I want them to be so strong and so tough, but we don\\'t need 75 percent of the repetitive, horrible regulations that hurt companies, hurt jobs, make us non-competitive overseas with other companies from other countries, that we don\\'t need.\\nBut we\\'re going to have regulation. It\\'s going to be really strong and really good and we\\'re going to protect our environment and we\\'re going to protect the safety of our people and our workers, OK?\\n\\nAnother major promise is tax reform. We\\'re going to massively lower taxes on the middle class, reduce taxes on American business and make our tax code more simple and much more fair for everyone, including the people and the business.\\n\\nIn anticipation of these and other changes, jobs are already starting to pour back into our country, you see that. In fact, I think I did more than any other pre-president, they say president- elect. President-elect is meeting with Ford, he\\'s meeting with Chrysler, he\\'s meeting with General Motors. I just wanted to save a little time.\\n\\nBecause Ford and Fiat Chrysler, General Motors, Sprint, Intel, and so many others are now, because of the election result, making major investments in the United States, expanding production and hiring more workers. And they\\'re going back to Michigan and they\\'re going back to Ohio and they\\'re going back to Pennsylvania and they\\'re going back to North Carolina and to Florida.\\n\\nIt\\'s time for all Americans to get off of welfare and get back to work, you\\'re going to love it, you\\'re going to love it, you\\'re going to love it.\\n\\nWe\\'re also put nothing a massive budget request for our beloved military.\\n\\nAnd we will be substantially upgrading all of our military, all of our military, offensive, defensive, everything, bigger and better and stronger than ever before. And hopefully, we\\'ll never have to use it, but nobody\\'s gonna mess with us, folks, nobody.\\n\\nIt will be one of the greatest military build-ups in American history. No one will dare question as they have been because we\\'re very depleted, very, very depleted. Sequester. Sequester. Nobody will dare question our military might again. We believe in peace through strength and that\\'s what we will have.\\n\\nAs part of my pledge to restore safety for the American people, I have also directed the defense community to develop a plan to totally obliterate ISIS.\\n\\n Working with our allies, we will eradicate this evil from the face of the Earth.\\n\\nAt the same time, we fully understand that national security begins with border security; foreign terrorists will not be able to strike America if they cannot get into our country.\\n\\nBy the way, take a look at what\\'s happening in Europe, folks, take a look at what\\'s happening in Europe. I took a lot of heat on Sweden.\\n\\nAnd then a day later, I said has anybody reported what\\'s going on? And it turned out that they didn\\'t -- not too many of them did. Take a look at what happened in Sweden. I love Sweden, great country, great people, I love Sweden. But they understand. The people over there understand I\\'m right. Take a look at what\\'s happening in Sweden. Take a look at what\\'s happening in Germany. Take a look at what\\'s happened in France. Take a look at Nice and Paris.\\nI have a friend, he\\'s a very, very substantial guy. He loves the city of lights, he loves Paris. For years, every year during the summer, he would go to Paris, was automatic with his wife and his family. Hadn\\'t seen him in a while. And I said, Jim, let me ask you a question, how\\'s Paris doing? \"Paris? I don\\'t go there anymore, Paris is no longer Paris.\" That was four years -- four or five years hasn\\'t gone there. He wouldn\\'t miss it for anything. Now he doesn\\'t even think in terms of going there.\\nTake a look at what\\'s happening to our world, folks. And we have to be smart. We have to be smart. We can\\'t let it happen to us.\\n\\nSo let me state this as clearly as I can, we are going to keep radical Islamic terrorists the hell out of our country.\\n We will not be deterred from this course, and in a matter of days, we will be taking brand-new action to protect our people and keep America safe, you will see the action. I will never ever apologize for protecting the safety and security of the American people, I won\\'t do it.\\n\\nIf it means I get bad press, if it means people speak badly of me, it\\'s OK, doesn\\'t bother me. The security of our people is number one, is number one.\\n\\nOur administration is running with great efficiency, even though I still don\\'t have my Cabinet approved, nobody mentions that.\\n\\nYou know, I still have people out there waiting to be approved and everyone knows they\\'re gonna be approved. It\\'s just a delay, delay, delay, it\\'s really sad, it\\'s really sad. And these are great people, these are some great people. We still don\\'t have our Cabinet. I assume we\\'re setting records for that. That\\'s the only thing good about it is we\\'re setting records. I love setting records.\\n\\nBut I hate having a Cabinet meeting and I see all these empty seats. I said, Democrats, please approve our Cabinet and get smart on health care, too, if you don\\'t mind.\\n\\n\\nBut we\\'re taking meetings everyday with top leaders in business, in science and industry. Yesterday, I had 29 of the biggest business leaders in the world in my office; Caterpillar, Tractor (ph), Campbell\\'s Soup. We had everybody. We had everybody. I like Campbell\\'s Soup.\\n\\n\\nWe had everybody and we came to a lot of very good conclusions and a lot of those folks that are in that room are gonna be building big, big, massive new plants and lots of jobs. And you know what? They\\'re gonna be building them in this country, not in some other country.\\n\\nWe\\'re meeting with unions, meeting with law enforcement and we\\'re meeting with leaders from all around where the White House doors used to be totally closed. They were closed, folks. You don\\'t realize that, they were closed.\\n They\\'re now wide open and they\\'re open for people doing business for our country and putting people to work.\\n\\nAnd when they come into the White House, we\\'re translating these meetings into action. One by one, we\\'re checking off the promises we made to the people of the United States. One by one, a lot of promises. And we will not stop until the job is done. We will reduce your taxes, we will cut your regulations, we will support our police, we will defend our flag.\\n\\nWe will rebuild our military. We will take care of our great, great veterans. We\\'re taking care of our veterans.\\n\\nWe will fix our broken and embarrassing trade deals that are no good. None of them. You wonder where did the people come from that negotiated these deals. Where did they come from? Well, they came also from campaign contributions, I must be honest. They\\'re not as stupid as you think.\\n\\nWe will cut wasteful spending. We will promote our values. We will rebuild our inner cities. We will bring back our jobs and our dreams. So true. So true.\\n\\nAnd by the way, we will protect our Second Amendment.\\n\\nYou know, Wayne and Chris are here from the NRA and they didn\\'t have that on the list. It\\'s lucky I thought about it.\\n\\nBut we will indeed, and they\\'re great people. And by the way, they love our country. They love our country. The NRA has been a great supporter. They love our country.\\n\\nThe forgotten men and women of America will be forgotten no longer. That is the heart of this new movement and the future of the Republican Party. People came to vote, and these people, the media, they said where are they coming from? What\\'s going on here? These are hard-working, great, great Americans. These are unbelievable people who have not been treated fairly. Hillary called them deplorable. They\\'re not deplorable.\\n\\nAUDIENCE: Lock her up! Lock her up! Lock her up!\\n Who would have thought that a word was gonna play (ph) so badly?\\n\\nThat\\'s the problem in politics. One wrong word and it\\'s over. She also said irredeemable, but we won\\'t mention that.\\n\\nThe GOP will be, from now on, the party also of the American worker.\\n\\nYou know, we haven\\'t been as a group given credit for this, but if you look at how much bigger our party has gotten during this cycle, during the early days when we had 17 people running in the primaries. Millions and millions of people were joining. Now, I won\\'t say it was because of me, but it was. OK?\\n\\n\\nAnd we have an amazing, strong, powerful party that truly does want to see America be great again, and it will see it and it\\'s gonna see it a lot sooner than you think, believe me, a lot sooner than you think.\\n\\n We will not answer to donors or lobbyists, or special interests, but we will serve the citizens of the United States of America, believe me.\\n\\nGlobal cooperation, dealing with other countries, getting along with other countries is good, it\\'s very important. But there is no such thing as a global anthem, a global currency or a global flag. This is the United States of America that I\\'m representing.\\n\\nI\\'m not representing the globe, I\\'m representing your country.\\n\\n\\n There\\'s one allegiance that unites us all, and that is to America. America, it\\'s the allegiance to America.\\n\\nNo matter our background or income, or geography, we\\'re all citizens of this blessed land. And no matter our color or the blood -- the color of the blood we bleed, it\\'s the same red blood of great, great patriots. Remember, great patriots.\\n\\nWe all salute with pride, the same American flag, and we all are equal -- totally equal in the eyes of almighty God, we\\'re equal.\\n\\nThank you.\\n\\nAnd I want to thank, by the way, the Evangelical community, the Christian community.\\n Communities of faith, rabbis and priests and pastors, ministers, because the support for me was a record, as you know, not only in terms of numbers of people, but percentage of those numbers that voted for Trump. So I want to thank you folks, that was amazing -- an amazing outpour...\\n\\n... and I will not disappoint you. As long as we have faith in each other and trust in God, then there is no goal at all beyond our reach. There is no dream too large, no task too great, we are Americans and the future belongs to us.\\n\\nThe future belongs to all of you. And America is coming about, and it\\'s coming back, and it\\'s roaring and you can hear it. It\\'s going to be bigger and better. It is going to be -- it is going to be -- remember and it\\'s roaring. It\\'s going to be bigger and better and stronger than ever before.\\n\\nI want to thank you.\\nAnd Matt and Mercedes, I want to thank the two of you, and all of the supporters that I have.\\nI see them; they\\'re all over the place. You are really great people. I want to thank you.\\n\\nAnd I want to say to you, God bless you and God bless the United States of America. Thank you, folks. Thank you.\\n',\n",
       " \"Wow. Thank you. That's a lot of people, Phoenix, that's a lot of people.\\n\\nThank you very much.\\n\\nThank you, Phoenix. I am so glad to be back in Arizona.\\n\\nThe state that has a very, very special place in my heart. I love people of Arizona and together we are going to win the White House in November.\\n\\nNow, you know this is where it all began for me. Remember that massive crowd also. So, I said let's go and have some fun tonight. We're going to Arizona, OK?\\n\\nThis will be a little bit different. This won't be a rally speech, per se. Instead, I'm going to deliver a detailed policy address on one of the greatest challenges facing our country today, illegal immigration.\\n\\nI've just landed having returned from a very important and special meeting with the President of Mexico, a man I like and respect very much. And a man who truly loves his country, Mexico.\\n\\nAnd, by the way, just like I am a man who loves my country, the United States.\\n\\nWe agree on the importance of ending the illegal flow of drugs, cash, guns, and people across our border, and to put the cartels out of business.\\n\\nWe also discussed the great contributions of Mexican-American citizens to our two countries, my love for the people of Mexico, and the leadership and friendship between Mexico and the United States. It was a thoughtful and substantive conversation and it will go on for awhile. And, in the end we're all going to win. Both countries, we're all going to win.\\n\\n\\nThis is the first of what I expect will be many, many conversations. And, in a Trump administration we're going to go about creating a new relationship between our two countries, but it's going to be a fair relationship. We want fairness.\\n\\nBut to fix our immigration system, we must change our leadership in Washington and we must change it quickly.\\n\\nSadly, sadly there is no other way. The truth is our immigration system is worse than anybody ever realized. But the facts aren't known because the media won't report on them. The politicians won't talk about them and the special interests spend a lot of money trying to cover them up because they are making an absolute fortune. That's the way it is.\\n\\nToday, on a very complicated and very difficult subject, you will get the truth. The fundamental problem with the immigration system in our country is that it serves the needs of wealthy donors, political activists and powerful, powerful politicians. It's all you can do. Thank you. Thank you.\\n\\nLet me tell you who it does not serve. It does not serve you the American people. Doesn't serve you. When politicians talk about immigration reform, they usually mean the following, amnesty, open borders, lower wages. Immigration reform should mean something else entirely. It should mean improvements to our laws and policies to make life better for American citizens.\\n\\nThank you. But if we're going to make our immigration system work, then we have to be prepared to talk honestly and without fear about these important and very sensitive issues. For instance, we have to listen to the concerns that working people, our forgotten working people, have over the record pace of immigration and it's impact on their jobs, wages, housing, schools, tax bills and general living conditions.\\n\\nThese are valid concerns expressed by decent and patriotic citizens from all backgrounds, all over. We also have to be honest about the fact that not everyone who seeks to join our country will be able to successfully assimilate. Sometimes it's just not going to work out. It's our right, as a sovereign nation to chose immigrants that we think are the likeliest to thrive and flourish and love us.\\n\\nThen there is the issue of security. Countless innocent American lives have been stolen because our politicians have failed in their duty to secure our borders and enforce our laws like they have to be enforced. I have met with many of the great parents who lost their children to sanctuary cities and open borders. So many people, so many, many people. So sad. They will be joining me on this stage in a little while and I look forward to introducing, these are amazing, amazing people.\\n\\nCountless Americans who have died in recent years would be alive today if not for the open border policies of this administration and the administration that causes this horrible, horrible thought process, called Hillary Clinton.\\n\\nThis includes incredible Americans like 21 year old Sarah Root. The man who killed her arrived at the border, entered Federal custody and then was released into the U.S., think of it, into the U.S. community under the policies of the White House Barack Obama and Hillary Clinton. Weak, weak policies. Weak and foolish policies.\\n\\nHe was released again after the crime, and now he's out there at large. Sarah had graduated from college with a 4.0, top student in her class one day before her death.\\n\\nAlso among the victims of the Obama-Clinton open borders policy was Grant Ronnebeck, a 21-year-old convenience store clerk and a really good guy from Mesa, Arizona. A lot of you have known about Grant. \\n\\nHe was murdered by an illegal immigrant gang member previously convicted of burglary, who had also been released from federal custody, and they knew it was going to happen again.\\n\\nAnother victim is Kate Steinle. Gunned down in the sanctuary city of San Francisco, by an illegal immigrant, deported five previous times. And they knew he was no good.\\n\\n\\n\\nThen there is the case of 90-year-old Earl Olander, who was brutally beaten and left to bleed to death in his home, 90 years old and defenseless. The perpetrators were illegal immigrants with criminal records a mile long, who did not meet Obama administration standards for removal. And they knew it was going to happen.\\n\\nIn California, a 64-year-old Air Force veteran, a great woman, according to everybody that knew her, Marilyn Pharis, was sexually assaulted and beaten to death with a hammer. Her killer had been arrested on multiple occasions but was never, ever deported, despite the fact that everybody wanted him out.\\n\\nA 2011 report from the Government Accountability Office found that illegal immigrants and other non-citizens, in our prisons and jails together, had around 25,000 homicide arrests to their names, 25,000.\\n\\nOn top of that, illegal immigration costs our country more than $113 billion dollars a year.\\n\\nAnd this is what we get. For the money we are going to spend on illegal immigration over the next 10 years, we could provide 1 million at-risk students with a school voucher, which so many people are wanting.\\n\\nWhile there are many illegal immigrants in our country who are good people, many, many, this doesn't change the fact that most illegal immigrants are lower skilled workers with less education, who compete directly against vulnerable American workers, and that these illegal workers draw much more out from the system than they can ever possibly pay back.\\n\\nAnd they're hurting a lot of our people that cannot get jobs under any circumstances.\\n\\nBut these facts are never reported. Instead, the media and my opponent discuss one thing and only one thing, the needs of people living here illegally. In many cases, by the way, they're treated better than our vets.\\n\\nNot going to happen anymore, folks. November 8th. Not going to happen anymore.\\n\\nThe truth is, the central issue is not the needs of the 11 million illegal immigrants or however many there may be -- and honestly we've been hearing that number for years. It's always 11 million.\\n\\nOur government has no idea. It could be 3 million. It could be 30 million. They have no idea what the number is.\\n\\nFrankly our government has no idea what they're doing on many, many fronts, folks.\\n\\nBut whatever the number, that's never really been the central issue. It will never be a central issue. It doesn't matter from that standpoint. Anyone who tells you that the core issue is the needs of those living here illegally has simply spent too much time in Washington.\\n\\nOnly the out of touch media elites think the biggest problems facing America -- you know this, this is what they talk about, facing American society today is that there are 11 million illegal immigrants who don't have legal status. And, they also think the biggest thing, and you know this, it's not nuclear, and it's not ISIS, it's not Russia, it's not China, it's global warming.\\n\\nTo all the politicians, donors, and special interests, hear these words from me and all of you today. There is only one core issue in the immigration debate, and that issue is the well being of the American people.\\n\\nNothing even comes a close second. Hillary Clinton, for instance, talks constantly about her fears that families will be separated, but she's not talking about the American families who have been permanently separated from their loved ones because of a preventable homicide, because of a preventable death, because of murder.\\n\\nNo, she's only talking about families who come here in violation of the law. We will treat everyone living or residing in our country with great dignity. So important.\\n\\nWe will be fair, just, and compassionate to all, but our greatest compassion must be for our American citizens.\\n\\nThank you.\\n\\nPresident Obama and Hillary Clinton have engaged in gross dereliction of duty by surrendering the safety of the American people to open borders, and you know it better than anybody right here in Arizona. You know it.\\n\\nPresident Obama and Hillary Clinton support sanctuary cities. They support catch and release on the border. they support visa overstays. They support the release of dangerous, dangerous, dangerous, criminals from detention. And, they support unconstitutional executive amnesty.\\n\\nHillary Clinton has pledged amnesty in her first 100 days, and her plan will provide Obamacare, Social Security, and Medicare for illegal immigrants, breaking the federal budget.\\n\\nOn top of that she promises uncontrolled, low-skilled immigration that continues to reduce jobs and wages for American workers, and especially for African-American and Hispanic workers within our country. Our citizens.\\n\\nMost incredibly, because to me this is unbelievable, we have no idea who these people are, where they come from. I always say Trojan Horse. Watch what's going to happen, folks. It's not going to be pretty.\\n\\nThis includes her plan to bring in 620,000 new refugees from Syria and that region over a short period of time. And even yesterday, when you were watching the news, you saw thousands and thousands of people coming in from Syria. What is wrong with our politicians, our leaders if we can call them that. What the hell are we doing?\\n\\nHard to believe. Hard to believe. Now that you've heard about Hillary Clinton's plan, about which she has not answered a single question, let me tell you about my plan. And do you notice --\\n\\nAnd do you notice all the time for weeks and weeks of debating my plan, debating, talking about it, what about this, what about that. They never even mentioned her plan on immigration because she doesn't want to get into the quagmire. It's a tough one, she doesn't know what she's doing except open borders and let everybody come in and destroy our country by the way.\\n\\nWhile Hillary Clinton meets only with donors and lobbyists, my plan was crafted with the input from Federal Immigration offices, very great people. Among the top immigration experts anywhere in this country, who represent workers, not corporations, very important to us.\\n\\nI also worked with lawmakers, who've led on this issue on behalf of American citizens for many years. And most importantly I've met with the people directly impacted by these policies. So important.\\n\\n \\n\\n\\nNumber one, are you ready? Are you ready?\\n\\nWe will build a great wall along the southern border. \\n\\nAnd Mexico will pay for the wall.\\n\\nOne hundred percent. They don't know it yet, but they're going to pay for it. And they're great people and great leaders but they're going to pay for the wall. On day one, we will begin working on intangible, physical, tall, power, beautiful southern border wall.\\n\\nWe will use the best technology, including above and below ground sensors that's the tunnels. Remember that, above and below.\\n\\nAbove and below ground sensors. Towers, aerial surveillance and manpower to supplement the wall, find and dislocate tunnels and keep out criminal cartels and Mexico you know that, will work with us. I really believe it. Mexico will work with us. I absolutely believe it. And especially after meeting with their wonderful, wonderful president today. I really believe they want to solve this problem along with us, and I'm sure they will.\\n\\nNumber two, we are going to end catch and release. We catch them, oh go ahead. We catch them, go ahead.\\n\\nUnder my administration, anyone who illegally crosses the border will be detained until they are removed out of our country and back to the country from which they came.\\n\\nAnd they'll be brought great distances. We're not dropping them right across. They learned that. President Eisenhower. They'd drop them across, right across, and they'd come back. And across.\\n\\nThen when they flew them to a long distance, all of a sudden that was the end. We will take them great distances. But we will take them to the country where they came from, OK?\\n\\nNumber three. Number three, this is the one, I think it's so great. It's hard to believe, people don't even talk about it. Zero tolerance for criminal aliens. Zero. Zero.\\n\\nZero. They don't come in here. They don't come in here.\\n\\nAccording to federal data, there are at least 2 million, 2 million, think of it, criminal aliens now inside of our country, 2 million people criminal aliens. We will begin moving them out day one. As soon as I take office. Day one. In joint operation with local, state, and federal law enforcement.\\n\\nNow, just so you understand, the police, who we all respect -- say hello to the police. Boy, they don't get the credit they deserve. I can tell you. They're great people. But the police and law enforcement, they know who these people are.\\n\\nThey live with these people. They get mocked by these people. They can't do anything about these people, and they want to. They know who these people are. Day one, my first hour in office, those people are gone.\\n\\nAnd you can call it deported if you want. The press doesn't like that term. You can call it whatever the hell you want. They're gone.\\n\\nBeyond the 2 million, and there are vast numbers of additional criminal illegal immigrants who have fled, but their days have run out in this country. The crime will stop. They're going to be gone. It will be over.\\n\\nThey're going out. They're going out fast.\\n\\nMoving forward. We will issue detainers for illegal immigrants who are arrested for any crime whatsoever, and they will be placed into immediate removal proceedings if we even have to do that.\\n\\nWe will terminate the Obama administration's deadly, and it is deadly, non-enforcement policies that allow thousands of criminal aliens to freely roam our streets, walk around, do whatever they want to do, crime all over the place.\\n\\nThat's over. That's over, folks. That's over.\\n\\nSince 2013 alone, the Obama administration has allowed 300,000 criminal aliens to return back into United States communities. These are individuals encountered or identified by ICE, but who were not detained or processed for deportation because it wouldn't have been politically correct.\\n\\nMy plan also includes cooperating closely with local jurisdictions to remove criminal aliens immediately. We will restore the highly successful Secure Communities Program. Good program. We will expand and revitalize the popular 287(g) partnerships, which will help to identify hundreds of thousands of deportable aliens in local jails that we don't even know about.\\n\\nBoth of these programs have been recklessly gutted by this administration. And those were programs that worked.\\n\\nThis is yet one more area where we are headed in a totally opposite direction. There's no common sense, there's no brain power in our administration by our leader, or our leaders. None, none, none.\\n\\nHereâ€™s an interesting graphic from a study published this summer on voting attitudes among people who heard incumbents use restrictive rhetoric on immigration -- tough enforcement, no amnesty, deportations -- and its affect on their own opinions.\\n\\nOn my first day in office I am also going to ask Congress to pass Kate's Law, named for Kate Steinle.\\n\\nTo ensure that criminal aliens convicted of illegal reentry receive strong mandatory minimum sentences. Strong.\\n\\nAnd then we get them out.\\n\\nAnother reform I'm proposing is the passage of legislation named for Detective Michael Davis and Deputy Sheriff Danny Oliver, to law enforcement officers recently killed by a previously deported illegal immigrant.\\n\\nThe Davis-Oliver bill will enhance cooperation with state and local authorities to ensure that criminal immigrants and terrorists are swiftly, really swiftly, identified and removed. And they will go face, believe me. They're going to go.\\n\\nWe're going to triple the number of ICE deportation officers.\\n\\nWithin ICE I am going to create a new special deportation task force focused on identifying and quickly removing the most dangerous criminal illegal immigrants in America who have evaded justice just like Hillary Clinton has evaded justice, OK?\\n\\nMaybe they'll be able to deport her.\\n\\nThe local police who know every one of these criminals, and they know each and every one by name, by crime, where they live, they will work so fast. And our local police will be so happy that they don't have to be abused by these thugs anymore. There's no great mystery to it, they've put up with it for years, and no finally we will turn the tables and law enforcement and our police will be allowed to clear up this dangerous and threatening mess.\\n\\nWe're also going to hire 5,000 more Border Patrol agents.\\n\\nWho gave me their endorsement, 16,500 gave me their endorsement.\\n\\nAnd put more of them on the border instead of behind desks which is good. We will expand the number of border patrol stations significantly.\\n\\nI've had a chance to spend time with these incredible law enforcement officers, and I want to take a moment to thank them. What they do is incredible.\\n\\nAnd getting their endorsement means so much to me. More to me really than I can say. Means so much. First time they've ever endorsed a presidential candidate.\\n\\nNumber four, block funding for sanctuary cities. We block the funding. No more funds.\\n\\nWe will end the sanctuary cities that have resulted in so many needless deaths. Cities that refuse to cooperate with federal authorities will not receive taxpayer dollars, and we will work with Congress to pass legislation to protect those jurisdictions that do assist federal authorities.\\n\\nNumber five, cancel unconstitutional executive orders and enforce all immigration laws.\\n\\nWe will immediately terminate President Obama's two illegal executive amnesties in which he defied federal law and the Constitution to give amnesty to approximately five million illegal immigrants, five million.\\n\\nAnd how about all the millions that are waiting on line, going through the process legally? So unfair.\\n\\nHillary Clinton has pledged to keep both of these illegal amnesty programs, including the 2014 amnesty which has been blocked by the United States Supreme Court. Great.\\n\\nClinton has also pledged to add a third executive amnesty. And by the way, folks, she will be a disaster for our country, a disaster in so many other ways.\\n\\nAnd don't forget the Supreme Court of the United States. Don't forget that when you go to vote on November 8. And don't forget your Second Amendment. And don't forget the repeal and replacement of Obamacare.\\n\\nAnd don't forget building up our depleted military. And don't forget taking care of our vets. Don't forget our vets. They have been forgotten.\\n\\nClinton's plan would trigger a constitutional crisis unlike almost anything we have ever seen before. In effect, she would be abolishing the lawmaking powers of Congress in order to write her own laws from the Oval Office. And you see what bad judgment she has. She has seriously bad judgment.\\n\\nCan you imagine? In a Trump administration all immigration laws will be enforced, will be enforced. As with any law enforcement activity, we will set priorities. But unlike this administration, no one will be immune or exempt from enforcement. And ICE and Border Patrol officers will be allowed to do their jobs the way their jobs are supposed to be done.\\n\\nAnyone who has entered the United States illegally is subject to deportation. That is what it means to have laws and to have a country. Otherwise we don't have a country.\\n\\nOur enforcement priorities will include removing criminals, gang members, security threats, visa overstays, public charges. That is those relying on public welfare or straining the safety net along with millions of recent illegal arrivals and overstays who've come here under this current corrupt administration.\\n\\n\\nNumber six, we are going to suspend the issuance of visas to any place where adequate screening cannot occur.\\n\\nAccording to data provided by the Senate Subcommittee on Immigration, and the national interest between 9/11 and the end of 2014, at least 380 foreign born individuals were convicted in terror cases inside the United States. And even right now the largest number of people are under investigation for exactly this that we've ever had in the history of our country.\\n\\nOur country is a mess. We don't even know what to look for anymore, folks. Our country has to straighten out. And we have to straighten out fast.\\n\\nThe number is likely higher. But the administration refuses to provide this information, even to Congress. As soon as I enter office I am going to ask the Department of State, which has been brutalized by Hillary Clinton, brutalized.\\n\\nHomeland Security and the Department of Justice to begin a comprehensive review of these cases in order to develop a list of regions and countries from which immigration must be suspended until proven and effective vetting mechanisms can be put in place.\\n\\nI call it extreme vetting right? Extreme vetting. I want extreme. It's going to be so tough, and if somebody comes in that's fine but they're going to be good. It's extreme.\\n\\nAnd if people don't like it, we've got have a country folks. Got to have a country. Countries in which immigration will be suspended would include places like Syria and Libya. And we are going to stop the tens of thousands of people coming in from Syria. We have no idea who they are, where they come from. There's no documentation. There's no paperwork. It's going to end badly folks. It's going to end very, very badly.\\n\\nFor the price of resettling, one refugee in the United States, 12 could be resettled in a safe zone in their home region. Which I agree with 100 percent. We have to build safe zones and we'll get the money from Gulf states. We don't want to put up the money. We owe almost $20 trillion. Doubled since Obama took office, our national debt.\\n\\nBut we will get the money from Gulf states and others. We'll supervise it. We'll build safe zones which is something that I think all of us want to see.\\n\\nAnother reform, involves new screening tests for all applicants that include, and this is so important, especially if you get the right people. And we will get the right people. An ideological certification to make sure that those we are admitting to our country share our values and love our people.\\n\\nThank you. We're very proud of our country. Aren't we? Really? With all it's going through, we're very proud of our country. For instance, in the last five years, we've admitted nearly 100,000 immigrants from Iraq and Afghanistan. And these two countries according to Pew Research, a majority of residents say that the barbaric practice of honor killings against women are often or sometimes justified. That's what they say.\\n\\nThat's what they say. They're justified. Right? And we're admitting them to our country. Applicants will be asked their views about honor killings, about respect for women and gays and minorities. Attitudes on radical Islam, which our President refuses to say and many other topics as part of this vetting procedure. And if we have the right people doing it, believe me, very, very few will slip through the cracks. Hopefully, none.\\n\\n\\nNumber seven, we will insure that other countries take their people back when they order them deported.\\n\\nThere are at least 23 countries that refuse to take their people back after they've been ordered to leave the United States. Including large numbers of violent criminals, they won't take them back. So we say, OK, we'll keep them. Not going to happen with me, not going to happen with me.\\n\\nDue to a Supreme Court decision, if these violent offenders cannot be sent home, our law enforcement officers have to release them into your communities.\\n\\nAnd by the way, the results are horrific, horrific. There are often terrible consequences, such as Casey Chadwick's tragic death in Connecticut just last year. Yet despite the existence of a law that commands the Secretary of State to stop issuing visas to these countries.\\n\\nSecretary Hillary Clinton ignored this law and refused to use this powerful tool to bring nations into compliance. And, they would comply if we would act properly.\\n\\nIn other words, if we had leaders that knew what they were doing, which we don't.\\n\\nThe result of her misconduct was the release of thousands and thousands of dangerous criminal aliens who should have been sent home to their countries. Instead we have them all over the place. Probably a couple in this room as a matter of fact, but I hope not.\\n\\nAccording to a report for the Boston Globe from the year 2008 to 2014 nearly 13,000 criminal aliens were released back into U.S. communities because their home countries would not, under any circumstances, take them back. Hard to believe with the power we have. Hard to believe.\\n\\nWe're like the big bully that keeps getting beat up. You ever see that? The big bully that keeps getting beat up.\\n\\nThese 13,000 release occurred on Hillary Clinton's watch. She had the power and the duty to stop it cold, and she decided she would not do it.\\n\\nAnd, Arizona knows better than most exactly what I'm talking about.\\n\\n\\nThose released include individuals convicted of killings, sexual assaults, and some of the most heinous crimes imaginable.\\n\\nThe Boston Globe writes that a Globe review of 323 criminals released in New England from 2008 to 2012 found that as many as 30 percent committed new offenses, including rape, attempted murder, and child molestation. We take them, we take them.\\n\\nNumber eight, we will finally complete the biometric entry-exit visa tracking system which we need desperately. For years Congress has required biometric entry-exit visa tracking systems, but it has never been completed. The politicians are all talk, no action, never happens. Never happens.\\n\\nHillary Clinton, all talk. Unfortunately when there is action it's always the wrong decision. You ever notice? In my administration we will ensure that this system is in place. And, I will tell you, it will be on land, it will be on sea, it will be in air. We will have a proper tracking system.\\n\\nApproximately half of new illegal immigrants came on temporary visas and then never, ever left. Why should the? Nobody's telling them to leave. Stay as long as you want, we'll take care of you.\\n\\nBeyond violating our laws, visa overstays, pose -- and they really are a big problem, pose a substantial threat to national security. The 9/11 Commission said that this tracking system would be a high priority and would have assisted law enforcement and intelligence officials in august and September in 2001 in conducting a search for two of the 9/11 hijackers that were in the United States expired visas.\\n\\nAnd, you know what that would have meant, what that could have meant. Wouldn't that have been wonderful, right? What that could have meant?\\n\\nLast year alone nearly half a million individuals overstayed their temporary visas. Removing these overstays will be a top priority of my administration.\\n\\nIf people around the world believe they can just come on a temporary visa and never, ever leave, the Obama-Clinton policy, that's what it is, then we have a completely open border, and we no longer have a country.\\n\\nWe must send a message that visa expiration dates will be strongly enforced.\\n\\nNumber nine, we will turn off the jobs and benefits magnet.\\n\\nWe will ensure that E-Verify is used to the fullest extent possible under existing law, and we will work with Congress to strengthen and expand its use across the country.\\n\\nImmigration law doesn't exist for the purpose of keeping criminals out. It exists to protect all aspects of American life. The work site, the welfare office, the education system, and everything else.\\n\\nThat is why immigration limits are established in the first place. If we only enforced the laws against crime, then we have an open border to the entire world. We will enforce all of our immigration laws.\\n\\nAnd the same goes for government benefits. The Center for Immigration Studies estimates that 62 percent of households headed by illegal immigrants use some form of cash or non-cash welfare programs like food stamps or housing assistance.\\n\\nTremendous costs, by the way, to our country. Tremendous costs. This directly violates the federal public charge law designed to protect the United States Treasury. Those who abuse our welfare system will be priorities for immediate removal.\\n\\nNumber 10, we will reform legal immigration to serve the best interests of America and its workers, the forgotten people. Workers. We're going to take care of our workers.\\n\\nAnd by the way, and by the way, we're going to make great trade deals. We're going to renegotiate trade deals. We're going to bring our jobs back home. We're going to bring our jobs back home.\\n\\nWe have the most incompetently worked trade deals ever negotiated probably in the history of the world, and that starts with NAFTA. And now they want to go TPP, one of the great disasters.\\n\\nWe're going to bring our jobs back home. And if companies want to leave Arizona and if they want to leave other states, there's going to be a lot of trouble for them. It's not going to be so easy. There will be consequence. Remember that. There will be consequence. They're not going to be leaving, go to another country, make the product, sell it into the United States, and all we end up with is no taxes and total unemployment. It's not going to happen. There will be consequences.\\n\\nWe've admitted 59 million immigrants to the United States between 1965 and 2015. Many of these arrivals have greatly enriched our country. So true. But we now have an obligation to them and to their children to control future immigration as we are following, if you think, previous immigration waves.\\n\\nWe've had some big waves. And tremendously positive things have happened. Incredible things have happened. To ensure assimilation we want to ensure that it works. Assimilation, an important word. Integration and upward mobility.\\n\\nWithin just a few years immigration as a share of national population is set to break all historical records. The time has come for a new immigration commission to develop a new set of reforms to our legal immigration system in order to achieve the following goals.\\n\\nTo keep immigration levels measured by population share within historical norms. To select immigrants based on their likelihood of success in U.S. society and their ability to be financially self- sufficient.\\n\\nWe take anybody. Come on in, anybody. Just come on in. Not anymore.\\n\\nYou know, folks, it's called a two-way street. It is a two-way street, right? We need a system that serves our needs, not the needs of others. Remember, under a Trump administration it's called America first. Remember that.\\n\\nTo choose immigrants based on merit. Merit, skill, and proficiency. Doesn't that sound nice? And to establish new immigration controls to boost wages and to ensure that open jobs are offered to American workers first. And that in particular African- American and Latino workers who are being shut out in this process so unfairly.\\n\\nAnd Hillary Clinton is going to do nothing for the African- American worker, the Latino worker. She's going to do nothing. Give me your vote, she says, on November eighth. And then she'll say, so long, see you in four years. That's what it is.\\n\\nShe is going to do nothing. And just look at the past. She's done nothing. She's been there for 35 years. She's done nothing. And I say what do you have to lose? Choose me. Watch how good we're going to do together. Watch.\\n\\nYou watch. We want people to come into our country, but they have to come into our country legally and properly vetted, and in a manner that serves the national interest. We've been living under outdated immigration rules from decades ago. They're decades and decades old.\\n\\nTo avoid this happening in the future, I believe we should sunset our visa laws so that Congress is forced to periodically revise and revisit them to bring them up to date. They're archaic. They're ancient. We wouldn't put our entire federal budget on auto pilot for decades, so why should we do the same for the very, very complex subject of immigration?\\n\\nSo let's now talk about the big picture. These 10 steps, if rigorously followed and enforced, will accomplish more in a matter of months than our politicians have accomplished on this issue in the last 50 years. It's going to happen, folks. Because I am proudly not a politician, because I am not behold to any special interest, I've spent a lot of money on my campaign, I'll tell you. I write those checks. Nobody owns Trump.\\n\\nI will get this done for you and for your family. We'll do it right. You'll be proud of our country again. We'll do it right. We will accomplish all of the steps outlined above. And, when we do, peace and law and justice and prosperity will prevail. Crime will go down. Border crossings will plummet. Gangs will disappear.\\n\\nAnd the gangs are all over the place. And welfare use will decrease. We will have a peace dividend to spend on rebuilding America, beginning with our American inner cities. We're going to rebuild them, for once and for all.\\n\\nFor those here illegally today, who are seeking legal status, they will have one route and one route only. To return home and apply for reentry like everybody else, under the rules of the new legal immigration system that I have outlined above. Those who have left to seek entry --\\n\\nThank you. Thank you. Those who have left to seek entry under this new system -- and it will be an efficient system -- will not be awarded surplus visas, but will have to apply for entry under the immigration caps or limits that will be established in the future.\\n\\nWe will break the cycle of amnesty and illegal immigration. We will break the cycle. There will be no amnesty.\\n\\nOur message to the world will be this. You cannot obtain legal status or become a citizen of the United States by illegally entering our country. Can't do it.\\n\\nThis declaration alone will help stop the crisis of illegal crossings and illegal overstays, very importantly. People will know that you can't just smuggle in, hunker down and wait to be legalized. It's not going to work that way. Those days are over.\\n\\nImportantly, in several years when we have accomplished all of our enforcement and deportation goals and truly ended illegal immigration for good, including the construction of a great wall, which we will have built in record time. And at a reasonable cost, which you never hear from the government.\\n\\nAnd the establishment of our new lawful immigration system then and only then will we be in a position to consider the appropriate disposition of those individuals who remain.\\n\\nThat discussion can take place only in an atmosphere in which illegal immigration is a memory of the past, no longer with us, allowing us to weigh the different options available based on the new circumstances at the time.\\n\\nRight now, however, we're in the middle of a jobs crisis, a border crisis and a terrorism crisis like never before. All energies of the federal government and the legislative process must now be focused on immigration security. That is the only conversation we should be having at this time, immigration security. Cut it off.\\n\\nWhether it's dangerous materials being smuggled across the border, terrorists entering on visas or Americans losing their jobs to foreign workers, these are the problems we must now focus on fixing. And the media needs to begin demanding to hear Hillary Clinton's answer on how her policies will affect Americans and their security.\\n\\nThese are matters of life and death for our country and its people, and we deserve answers from Hillary Clinton. And do you notice, she doesn't answer.\\n\\nShe didn't go to Louisiana. She didn't go to Mexico. She was invited.\\n\\nShe doesn't have the strength or the stamina to make America great again. Believe me.\\n\\nWhat we do know, despite the lack of media curiosity, is that Hillary Clinton promises a radical amnesty combined with a radical reduction in immigration enforcement. Just ask the Border Patrol about Hillary Clinton. You won't like what you're hearing.\\n\\nThe result will be millions more illegal immigrants; thousands of more violent, horrible crimes; and total chaos and lawlessness. That's what's going to happen, as sure as you're standing there.\\n\\nThis election, and I believe this, is our last chance to secure the border, stop illegal immigration and reform our laws to make your life better. I really believe this is it. This is our last time. November 8. November 8. You got to get out and vote on November 8.\\n\\nIt's our last chance. It's our last chance. And that includes Supreme Court justices and Second Amendment. Remember that.\\n\\nSo I want to remind everyone what we're fighting for and who we are fighting for.\\n\\nI am going to ask -- these are really special people that I've gotten to know. Iâ€™m going to ask all the Angel Moms to come join me on the stage right now. These are amazing women.\\n\\nThese are amazing people.\\n\\n\\n \\n\\nThank you.\\n\\nThese are amazing people, and I am not asking for their endorsement, believe me that. I just think I've gotten to know so many of them, and many more, from our group. But they are incredible people and what they're going through is incredible, and there's just no reason for it. Let's give them a really tremendous hand.\\n\\nThat's tough stuff, I will tell you. That is tough stuff. Incredible people.\\n\\nSo, now is the time for these voices to be heard. Now is the time for the media to begin asking questions on their behalf. Now is the time for all of us as one country, Democrat, Republican, liberal, conservative to band together to deliver justice, and safety, and security for all Americans.\\n\\nLet's fix this horrible, horrible, problem. It can be fixed quickly. Let's our secure our border.\\n\\nLet's stop the drugs and the crime from pouring into our country. Let's protect our social security and Medicare. Let's get unemployed Americans off the welfare and back to work in their own country.\\n\\nThis has been an incredible evening. We're going to remember this evening. November 8, we have to get everybody. This is such an important state. November 8 we have to get everybody to go out and vote.\\n\\nWe're going to bring -- thank you, thank you. We're going to take our country back, folks. This is a movement. We're going to take our country back.\\n\\nThank you.\\n\\nThank you.\\n\\nThis is an incredible movement. The world is talking about it. The world is talking about it and by the way, if you haven't been looking to what's been happening at the polls over the last three or four days I think you should start looking. You should start looking.\\n\\nTogether we can save American lives, American jobs, and American futures. Together we can save America itself. Join me in this mission, we're going to make America great again.\\n\\nThank you. I love you. God bless you, everybody. God bless you. God bless you, thank you.\",\n",
       " '\"Chief Justice Roberts, President Carter, President Clinton, President Bush, President Obama, fellow Americans and people of the world, thank you.\\n\\nWe, the citizens of America, are now joined in a great national effort to rebuild our country and restore its promise for all of our people.\\n\\nTogether, we will determine the course of America and the world for many, many years to come. We will face challenges. We will confront hardships. But we will get the job done.\\n\\nEvery four years we gather on these steps to carry out the orderly and peaceful transfer of power.\\n\\nAnd we are grateful to President Obama and first lady Michelle Obama for their gracious aid throughout this transition.\\n\\nThey have been magnificent.\\n\\nThank you.\\n\\nToday\\'s ceremony, however, has a very special meaning because today we are not merely transferring power from one administration to another or from one party to another, but we are transferring power from Washington, D.C., and giving it back to you, the people.\\n\\nFor too long, a small group in our nation\\'s capital has reaped the rewards of government while the people have bore the cost. Washington flourished, but the people did not share in its wealth. Politicians prospered but the jobs left and the factories closed.\\n\\nThe establishment protected itself, but not the citizens of our country. Their victories have not been your victories. Their triumphs have not been your triumphs. And while they celebrated in our nation\\'s capital, there was little to celebrate for struggling families all across our land.\\n\\nThat all changes starting right here and right now, because this moment is your moment.\\n\\nIt belongs to you.\\n\\nIt belongs to everyone gathered here today and everyone watching all across America.\\n\\nThis is your day.\\n\\nThis is your celebration.\\n\\nAnd this, the United States of America, is your country.\\n\\nWhat truly matters is not which party controls our government, but whether our government is controlled by the people.\\n\\nJanuary 20th, 2017, will be remembered as the day the people became the rulers of this nation again.\\n\\nThe forgotten men and women of our country will be forgotten no longer. Everyone is listening to you now. You came by the tens of millions to become part of a historic movement, the likes of which the world has never seen before.\\n\\nAt the center of this movement is a crucial conviction that a nation exists to serve its citizens. Americans want great schools for their children, safe neighborhoods for their families and good jobs for themselves.\\n\\nThese are just and reasonable demands of righteous people and a righteous public.\\n\\nBut for too many of our citizens, a different reality exists.\\n\\nMothers and children trapped in poverty in our inner cities, rusted out factories scattered like tombstones across the landscape of our nation.\\n\\nAn education system flush with cash but which leaves our young and beautiful students deprived of all knowledge.\\n\\nAnd the crime and the gangs and the drugs that have stolen too many lives and robbed our country of so much unrealized potential. This American carnage stops right here and stops right now.\\n\\nWe are one nation, and their pain is our pain.\\n\\nTheir dreams are our dreams, and their success will be our success. We share one heart, one home and one glorious destiny.\\n\\nThe oath of office I take today is an oath of allegiance to all Americans.\\n\\nFor many decades we\\'ve enriched foreign industry at the expense of American industry, subsidized the armies of other countries while allowing for the very sad depletion of our military.\\n\\nWe\\'ve defended other nations\\' borders while refusing to defend our own. And we\\'ve spent trillions and trillions of dollars overseas while America\\'s infrastructure has fallen into disrepair and decay.\\n\\nWe\\'ve made other countries rich while the wealth, strength and confidence of our country has dissipated over the horizon.\\n\\nOne by one, the factories shuttered and left our shores with not even a thought about the millions and millions of American workers that were left behind.\\n\\nThe wealth of our middle class has been ripped from their homes and then redistributed all across the world. But that is the past, and now we are looking only to the future.\\n\\nWe assembled here today are issuing a new decree to be heard in every city, in every foreign capital and in every hall of power. From this day forward, a new vision will govern our land.\\n\\nFrom this day forward, it\\'s going to be only America first, America first. Every decision on trade, on taxes, on immigration, on foreign affairs will be made to benefit American workers and American families. We must protect our borders from the ravages of other countries making our product, stealing our companies and destroying our jobs.\\n\\nProtection will lead to great prosperity and strength. I will fight for you with every breath in my body, and I will never ever let you down.\\n\\nAmerica will start winning again, winning like never before.\\n\\nWe will bring back our jobs.\\n\\nWe will bring back our borders.\\n\\nWe will bring back our wealth, and we will bring back our dreams.\\n\\nWe will build new roads and highways and bridges and airports and tunnels and railways all across our wonderful nation.\\n\\nWe will get our people off of welfare and back to work, rebuilding our country with American hands and American labor.\\n\\nWe will follow two simple rules: Buy American and hire American.\\n\\nWe will seek friendship and goodwill with the nations of the world, but we do so with the understanding that it is the right of all nations to put their own interests first.\\n\\nWe do not seek to impose our way of life on anyone, but rather to let it shine as an example.\\n\\nWe will shine for everyone to follow.\\n\\nWe will re-enforce old alliances and form new ones and unite the civilized world against radical Islamic terrorism, which we will eradicate completely from the face of the earth.\\n\\nAt the bedrock of our politics will be a total allegiance to the United States of America, and through our loyalty to our country we will rediscover our loyalty to each other.\\n\\nWhen you open your heart to patriotism, there is no room for prejudice.\\n\\nThe Bible tells us how good and pleasant it is when God\\'s people live together in unity. We must speak our minds openly, debate our disagreements honestly, but always pursue solidarity. When America is united, America is totally unstoppable. There should be no fear. We are protected and we will always be protected. We will be protected by the great men and women of our military and law enforcement. And most importantly, we will be protected by God.\\n\\nFinally, we must think big and dream even bigger. In America, we understand that a nation is only living as long as it is striving. We will no longer accept politicians who are all talk and no action, constantly complaining but never doing anything about it.\\n\\nThe time for empty talk is over. Now arrives the hour of action.\\n\\nDo not allow anyone to tell you that it cannot be done. No challenge can match the heart and fight and spirit of America. We will not fail. Our country will thrive and prosper again.\\n\\nWe stand at the birth of a new millennium, ready to unlock the mysteries of space, to free the earth from the miseries of disease, and to harness the energies, industries and technologies of tomorrow.\\n\\nA new national pride will stir ourselves, lift our sights and heal our divisions. It\\'s time to remember that old wisdom our soldiers will never forget, that whether we are black or brown or white, we all bleed the same red blood of patriots.\\n\\nWe all enjoy the same glorious freedoms and we all salute the same great American flag.\\n\\nAnd whether a child is born in the urban sprawl of Detroit or the windswept plains of Nebraska, they look up at the same night sky, they fill their heart with the same dreams and they are infused with the breath of life by the same almighty creator.\\n\\nSo to all Americans in every city near and far, small and large, from mountain to mountain, from ocean to ocean, hear these words: You will never be ignored again. Your voice, your hopes and your dreams will define our American destiny. And your courage and goodness and love will forever guide us along the way.\\n\\nTogether we will make America strong again, we will make America wealthy again, we will make America proud again, we will make America safe again.\\n\\nAnd, yes, together we will make America great again.\\n\\nThank you.\\n\\nGod bless you.\\n\\nAnd God bless America.\"',\n",
       " \" Thank you very much. We were very close, and it was a very, very tight margin. We had no Democrat support. We had no votes from the Democrats. They weren't going to give us a single vote, so it's a very difficult thing to do.\\nI've been saying for the last year and a half that the best thing we can do politically speaking is let Obamacare explode. It is exploding right now. It's -- many states have big problems, almost all states have big problems. I was in Tennessee the other day and they've lost half of their state in terms of an insurer. They have no insurer. And that's happening to many other places. I was in Kentucky the other day and similar things are happening.\\nSo, Obamacare is exploding. With no Democrat support we couldn't quite get there. We were just a very small number of votes short in terms of getting our bill passed. A lot of people don't realize how good our bill was because they were viewing phase one. But when you add phase two, which was mostly the signings of Secretary Price, who is behind me, and you add phase three, which I think we would have gotten, it became a great bill. Premiums would have gone down and it would have been very stable. It would have been very strong. But that's okay. But we're very, very close and, again, I think what will happen is Obamacare, unfortunately, will explode. It's going to have a very bad year. Last year you had over 100 percent increases in various places.\\nIn Arizona I understand it's going up very rapidly again, like it did last year. Last year was 116 percent. Many places 50, 60, 70 percent. I guess it averaged whatever the average was, very, very high. And this year should be much worse for Obamacare.\\nSo, what would be really good with no Democrat support, if the Democrats when it explodes, which it will soon, if they got together with us and got a real health care bill, I'd be totally open to it, and I think that's going to happen. I think the losers are Nancy Pelosi and Chuck Schumer, because now they own Obamacare. They own it, 100 percent own it. And this is not a Republican health care. This is not anything but a Democrat health care.\\nAnd they have Obamacare for a little while longer, until it ceases to exist, which it will at some point in the near future. And just remember this is not our bill, this is their bill.\\nNow, when they all become civilized and get together and try and workout a great health care bill for the people of this country, we're open to it, we're totally open to it. I want to thank the Republican Party.\\nI want to thank Paul Ryan. He worked very, very hard, I will tell you that. He worked very, very hard.\\nTom Price and Mike Pence, who is right here, our vice president, our great vice president. Everybody worked hard. I worked as a team player, and would have loved to have seen it pass. But, again, I think you know I was very clear because I think there wasn't a speech I made, or very few where I didn't mention that perhaps the best thing that could happen is exactly what happened today, because we'll end up with a truly great health care bill in the future after this mess known as Obamacare explodes.\\nSo, I want to thank everybody for being here. It will go very smoothly. I really believe. I think this is something -- it certainly was an interesting period of time. We all learned a lot. We learned a lot about royalty. We learned a lot about the vote- getting process. We learned a lot about some very arcane rules in obviously both the Senate and in the House.\\nSo, it's been certainly, for me, it's been a very interesting experience. But in the end I think it's going to be an experience that leads to an even better health care plan. So, thank you all very much and I'll see you soon. Thank you.\\n\\n We'll probably be going right now for tax reform, which we could have done earlier, but this really would have worked out better if we could have had some Democrat support. Remember, this, we had no Democrat support. So, now we're going to go for tax reform which I've always liked.\\nQUESTION: And you're confident in Speaker Ryan's leadership and his ability to get things done?\\n Yes, am. I like Speaker Ryan. he worked very, very hard. A lot of different groups, he's got a lot of factions, and there's been a long history of liking and disliking, even within the Republican Party, long before I got here.\\nBut I've had a great relationship with the Republican Party. It seems that both sides like Trump, and that's good, and you see that, I guess, more clearly than anybody. But we've had -- I'm not going to speak badly about anybody within the party, but certainly there is a big history.\\nI think Paul really worked hard, and I would say that we will probably start going very, very strongly for the big tax cuts and tax reform. That will be next.\\n\\n Well, it's going to happen. There's not much you can do about it. It's going to -- bad things are going to happen to Obamacare. There's not much you can do to help it. I've been saying that for a year and a half. I said, look, eventually it's not sustainable. The insurance companies are leaving. You know that. They're leaving one by one as quick as you can leave. And you have states in some cases soon will not be covered. So, there's no way out of that.\\nBut the one thing that was happening as we got closer and closer, everybody was talking about how wonderful it was, and now it will go back to real life. People will see how bad it is, and it's getting much worse.\\nYou know, I said the other day when president Obama left, '17, he knew he wasn't going to be here. '17 is going to be a very, very bad year for Obamacare, very, very bad. You're going to have explosive premium increases, and your deductibles are so high people don't even get to use it.\\nSo they'll go with that for a little while. And I honestly believe -- I know some Democrats, and they're good people. I honestly believe the Democrats will come to us and say, look, let's get together and get a great health care bill or plan that's really great for the people of our country. And I think that's going to happen.\\n Well, we were very close. We were just probably anywhere from 10 to 15 votes short. Could have even been closer than that. You'll never know because you never know how though vote. But in the end I think we would have been 10 votes, maybe closer.\\nAnd -- but it's very hard to get almost 100 percent. You know, you're talking about a very, very large number of votes, among any group. And we were very close to doing it. But when you get no votes from the other side, meaning the Democrats, it's really a difficult situation.\\n No, I think we have to let Obamacare go its way for a little while. And we'll see how things go. I would love to see it do well, but it can't. I mean, it can't. I mean, it's not a question of, gee, I hope it does well, I would love it to do well. I want great health care for the people of this nation. But it can't do well.\\nIt's imploding and soon will explode and it's not going to be pretty. So, the Democrats don't want to see that. So, they're going to reach out when they're ready. And whenever they're ready, we're ready.\\n\\n No, not particularly. They're friends of mine. I'm disappointed because we could have had it. So I'm disappointed. I'm a little surprised, to be honest with you. We really had it. It was pretty much there within grasp.\\nBut I'll tell you what's going to come out of it is a better bill. I really believe a better bill, because there were things in this bill I didn't particularly like. And I think it's a better bill. You know, both parties can get together and do real health care, that's the best thing. Obamacare was rammed down everyone's throat, 100 percent Democrat. And I think having bipartisan would be a big, big improvement. So, no, I think that this is going to end up being a very good thing.\\nI'm disappointed, but they're friends of mine. And, you know, they got on -- it was a very hard time for them and a very hard vote. But they're very good people.\\n\\n Well, I think we could have had things that I would have liked more. And if we had bipartisan I really think we could have a health care bill that would be the ultimate. And I think the Democrats know that also.\\nAnd some day in the not too distant future that will happen. And I never said -- I guess I'm here, what, 64 days? I never said repeal and replace Obamacare. You've all heard my speeches. I never said repeal it and replace it within 64 days. I have a long time.\\nBut I want to have a great health care bill and plan, and we will. It will happen. And it won't be in the very distant future. I really believe there will be some Democrat support and that will happen, and it will be an even better bill.\\nI think this was a very good bill. I think it will be even better the next time around. And I don't think that's going to be in too long a period of time.\\n\\n No, I mean, I don't want to speak about specifics, but there are things I could have -- I would have liked even more. But I thought overall this was a very, very good bill.\\nAnd I thought Tom Price -- Dr. Tom Price, who really is amazing on health care and his knowledge, I thought he did a fantastic job. Same with Mike Pence. I think these two guys, they worked so hard and really did a fantastic job.\\nThank you very much. Thank you. Thank you very much. Appreciate it.\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rank_bm25.BM25Okapi at 0x1c4ec4bd5e0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.44114013, 3.29799168, 1.56003948, 1.5711523 , 1.55431461,\n",
       "       1.05335683])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"American Greatness is now beginning\"\n",
    "tokenized_query = query.split(\" \")\n",
    "\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr. Speaker, Mr. Vice President, Members of Congress, the First Lady of the United States, and Citizens of America:\\nTonight, as we mark the conclusion of our celebration of Black History Month, we are reminded of our Nation\\'s path toward civil rights and the work that still remains. Recent threats targeting Jewish Community Centers and vandalism of Jewish cemeteries, as well as last week\\'s shooting in Kansas City, remind us that while we may be a Nation divided on policies, we are a country that stands united in condemning hate and evil in all its forms.\\nEach American generation passes the torch of truth, liberty and justice --- in an unbroken chain all the way down to the present.\\nThat torch is now in our hands. And we will use it to light up the world. I am here tonight to deliver a message of unity and strength, and it is a message deeply delivered from my heart.\\nA new chapter of American Greatness is now beginning.\\nA new national pride is sweeping across our Nation.\\nAnd a new surge of optimism is placing impossible dreams firmly within our grasp.\\nWhat we are witnessing today is the Renewal of the American Spirit.\\nOur allies will find that America is once again ready to lead.\\nAll the nations of the world -- friend or foe -- will find that America is strong, America is proud, and America is free.\\nIn 9 years, the United States will celebrate the 250th anniversary of our founding -- 250 years since the day we declared our Independence.\\nIt will be one of the great milestones in the history of the world.\\nBut what will America look like as we reach our 250th year? What kind of country will we leave for our children?\\nI will not allow the mistakes of recent decades past to define the course of our future.\\nFor too long, we\\'ve watched our middle class shrink as we\\'ve exported our jobs and wealth to foreign countries.\\nWe\\'ve financed and built one global project after another, but ignored the fates of our children in the inner cities of Chicago, Baltimore, Detroit -- and so many other places throughout our land.\\nWe\\'ve defended the borders of other nations, while leaving our own borders wide open, for anyone to cross -- and for drugs to pour in at a now unprecedented rate.\\nAnd we\\'ve spent trillions of dollars overseas, while our infrastructure at home has so badly crumbled.\\nThen, in 2016, the earth shifted beneath our feet. The rebellion started as a quiet protest, spoken by families of all colors and creeds --- families who just wanted a fair shot for their children, and a fair hearing for their concerns.\\nBut then the quiet voices became a loud chorus -- as thousands of citizens now spoke out together, from cities small and large, all across our country.\\nFinally, the chorus became an earthquake -- and the people turned out by the tens of millions, and they were all united by one very simple, but crucial demand, that America must put its own citizens first ... because only then, can we truly MAKE AMERICA GREAT AGAIN.\\nDying industries will come roaring back to life. Heroic veterans will get the care they so desperately need.\\nOur military will be given the resources its brave warriors so richly deserve.\\nCrumbling infrastructure will be replaced with new roads, bridges, tunnels, airports and railways gleaming across our beautiful land.\\nOur terrible drug epidemic will slow down and ultimately, stop.\\nAnd our neglected inner cities will see a rebirth of hope, safety, and opportunity.\\nAbove all else, we will keep our promises to the American people.\\nIt\\'s been a little over a month since my inauguration, and I want to take this moment to update the Nation on the progress I\\'ve made in keeping those promises.\\nSince my election, Ford, Fiat-Chrysler, General Motors, Sprint, Softbank, Lockheed, Intel, Walmart, and many others, have announced that they will invest billions of dollars in the United States and will create tens of thousands of new American jobs.\\nThe stock market has gained almost three trillion dollars in value since the election on November 8th, a record. We\\'ve saved taxpayers hundreds of millions of dollars by bringing down the price of the fantastic new F-35 jet fighter, and will be saving billions more dollars on contracts all across our Government. We have placed a hiring freeze on non-military and non-essential Federal workers.\\nWe have begun to drain the swamp of government corruption by imposing a 5 year ban on lobbying by executive branch officials --- and a lifetime ban on becoming lobbyists for a foreign government.\\nWe have undertaken a historic effort to massively reduce jobâ€‘crushing regulations, creating a deregulation task force inside of every Government agency; imposing a new rule which mandates that for every 1 new regulation, 2 old regulations must be eliminated; and stopping a regulation that threatens the future and livelihoods of our great coal miners.\\nWe have cleared the way for the construction of the Keystone and Dakota Access Pipelines -- thereby creating tens of thousands of jobs -- and I\\'ve issued a new directive that new American pipelines be made with American steel.\\nWe have withdrawn the United States from the job-killing Trans-Pacific Partnership.\\nWith the help of Prime Minister Justin Trudeau, we have formed a Council with our neighbors in Canada to help ensure that women entrepreneurs have access to the networks, markets and capital they need to start a business and live out their financial dreams.\\nTo protect our citizens, I have directed the Department of Justice to form a Task Force on Reducing Violent Crime.\\nI have further ordered the Departments of Homeland Security and Justice, along with the Department of State and the Director of National Intelligence, to coordinate an aggressive strategy to dismantle the criminal cartels that have spread across our Nation.\\nWe will stop the drugs from pouring into our country and poisoning our youth -- and we will expand treatment for those who have become so badly addicted.\\nAt the same time, my Administration has answered the pleas of the American people for immigration enforcement and border security. By finally enforcing our immigration laws, we will raise wages, help the unemployed, save billions of dollars, and make our communities safer for everyone. We want all Americans to succeed --- but that can\\'t happen in an environment of lawless chaos. We must restore integrity and the rule of law to our borders.\\nFor that reason, we will soon begin the construction of a great wall along our southern border. It will be started ahead of schedule and, when finished, it will be a very effective weapon against drugs and crime.\\nAs we speak, we are removing gang members, drug dealers and criminals that threaten our communities and prey on our citizens. Bad ones are going out as I speak tonight and as I have promised.\\nTo any in Congress who do not believe we should enforce our laws, I would ask you this question: what would you say to the American family that loses their jobs, their income, or a loved one, because America refused to uphold its laws and defend its borders?\\nOur obligation is to serve, protect, and defend the citizens of the United States. We are also taking strong measures to protect our Nation from Radical Islamic Terrorism.\\nAccording to data provided by the Department of Justice, the vast majority of individuals convicted for terrorism-related offenses since 9/11 came here from outside of our country. We have seen the attacks at home --- from Boston to San Bernardino to the Pentagon and yes, even the World Trade Center.\\nWe have seen the attacks in France, in Belgium, in Germany and all over the world.\\nIt is not compassionate, but reckless, to allow uncontrolled entry from places where proper vetting cannot occur. Those given the high honor of admission to the United States should support this country and love its people and its values.\\nWe cannot allow a beachhead of terrorism to form inside America -- we cannot allow our Nation to become a sanctuary for extremists.\\nThat is why my Administration has been working on improved vetting procedures, and we will shortly take new steps to keep our Nation safe -- and to keep out those who would do us harm.\\nAs promised, I directed the Department of Defense to develop a plan to demolish and destroy ISIS -- a network of lawless savages that have slaughtered Muslims and Christians, and men, women, and children of all faiths and beliefs. We will work with our allies, including our friends and allies in the Muslim world, to extinguish this vile enemy from our planet.\\nI have also imposed new sanctions on entities and individuals who support Iran\\'s ballistic missile program, and reaffirmed our unbreakable alliance with the State of Israel.\\nFinally, I have kept my promise to appoint a Justice to the United States Supreme Court -- from my list of 20 judges -- who will defend our Constitution. I am honored to have Maureen Scalia with us in the gallery tonight. Her late, great husband, Antonin Scalia, will forever be a symbol of American justice. To fill his seat, we have chosen Judge Neil Gorsuch, a man of incredible skill, and deep devotion to the law. He was confirmed unanimously to the Court of Appeals, and I am asking the Senate to swiftly approve his nomination.\\nTonight, as I outline the next steps we must take as a country, we must honestly acknowledge the circumstances we inherited.\\nNinety-four million Americans are out of the labor force.\\nOver 43 million people are now living in poverty, and over 43 million Americans are on food stamps.\\nMore than 1 in 5 people in their prime working years are not working.\\nWe have the worst financial recovery in 65 years.\\nIn the last 8 years, the past Administration has put on more new debt than nearly all other Presidents combined.\\nWe\\'ve lost more than one-fourth of our manufacturing jobs since NAFTA was approved, and we\\'ve lost 60,000 factories since China joined the World Trade Organization in 2001.\\nOur trade deficit in goods with the world last year was nearly $800 billion dollars.\\nAnd overseas, we have inherited a series of tragic foreign policy disasters.\\nSolving these, and so many other pressing problems, will require us to work past the differences of party. It will require us to tap into the American spirit that has overcome every challenge throughout our long and storied history.\\nBut to accomplish our goals at home and abroad, we must restart the engine of the American economy -- making it easier for companies to do business in the United States, and much harder for companies to leave.\\nRight now, American companies are taxed at one of the highest rates anywhere in the world.\\nMy economic team is developing historic tax reform that will reduce the tax rate on our companies so they can compete and thrive anywhere and with anyone. At the same time, we will provide massive tax relief for the middle class.\\nWe must create a level playing field for American companies and workers.\\nCurrently, when we ship products out of America, many other countries make us pay very high tariffs and taxes -- but when foreign companies ship their products into America, we charge them almost nothing.\\nI just met with officials and workers from a great American company, Harley-Davidson. In fact, they proudly displayed five of their magnificent motorcycles, made in the USA, on the front lawn of the White House.\\nAt our meeting, I asked them, how are you doing, how is business? They said that it\\'s good. I asked them further how they are doing with other countries, mainly international sales. They told me -- without even complaining because they have been mistreated for so long that they have become used to it -- that it is very hard to do business with other countries because they tax our goods at such a high rate. They said that in one case another country taxed their motorcycles at 100 percent.\\nThey weren\\'t even asking for change. But I am.\\nI believe strongly in free trade but it also has to be FAIR TRADE.\\nThe first Republican President, Abraham Lincoln, warned that the \"abandonment of the protective policy by the American Government [will] produce want and ruin among our people.\"\\nLincoln was right -- and it is time we heeded his words. I am not going to let America and its great companies and workers, be taken advantage of anymore.\\nI am going to bring back millions of jobs. Protecting our workers also means reforming our system of legal immigration. The current, outdated system depresses wages for our poorest workers, and puts great pressure on taxpayers.\\nNations around the world, like Canada, Australia and many others --- have a merit-based immigration system. It is a basic principle that those seeking to enter a country ought to be able to support themselves financially. Yet, in America, we do not enforce this rule, straining the very public resources that our poorest citizens rely upon. According to the National Academy of Sciences, our current immigration system costs America\\'s taxpayers many billions of dollars a year.\\nSwitching away from this current system of lower-skilled immigration, and instead adopting a merit-based system, will have many benefits: it will save countless dollars, raise workers\\' wages, and help struggling families --- including immigrant families --- enter the middle class.\\nI believe that real and positive immigration reform is possible, as long as we focus on the following goals: to improve jobs and wages for Americans, to strengthen our nation\\'s security, and to restore respect for our laws.\\nIf we are guided by the well-being of American citizens then I believe Republicans and Democrats can work together to achieve an outcome that has eluded our country for decades.\\nAnother Republican President, Dwight D. Eisenhower, initiated the last truly great national infrastructure program --- the building of the interstate highway system. The time has come for a new program of national rebuilding.\\nAmerica has spent approximately six trillion dollars in the Middle East, all this while our infrastructure at home is crumbling. With this six trillion dollars we could have rebuilt our country --- twice. And maybe even three times if we had people who had the ability to negotiate.\\nTo launch our national rebuilding, I will be asking the Congress to approve legislation that produces a $1 trillion investment in the infrastructure of the United States -- financed through both public and private capital --- creating millions of new jobs.\\nThis effort will be guided by two core principles: Buy American, and Hire American.\\nTonight, I am also calling on this Congress to repeal and replace Obamacare with reforms that expand choice, increase access, lower costs, and at the same time, provide better Healthcare.\\nMandating every American to buy government-approved health insurance was never the right solution for America. The way to make health insurance available to everyone is to lower the cost of health insurance, and that is what we will do.\\nObamacare premiums nationwide have increased by double and triple digits. As an example, Arizona went up 116 percent last year alone. Governor Matt Bevin of Kentucky just said Obamacare is failing in his State -- it is unsustainable and collapsing.\\nOne third of counties have only one insurer on the exchanges --- leaving many Americans with no choice at all.\\nRemember when you were told that you could keep your doctor, and keep your plan?\\nWe now know that all of those promises have been broken.\\nObamacare is collapsing --- and we must act decisively to protect all Americans. Action is not a choice --- it is a necessity.\\nSo I am calling on all Democrats and Republicans in the Congress to work with us to save Americans from this imploding Obamacare disaster.\\nHere are the principles that should guide the Congress as we move to create a better healthcare system for all Americans:\\nFirst, we should ensure that Americans with pre-existing conditions have access to coverage, and that we have a stable transition for Americans currently enrolled in the healthcare exchanges.\\nSecondly, we should help Americans purchase their own coverage, through the use of tax credits and expanded Health Savings Accounts --- but it must be the plan they want, not the plan forced on them by the Government.\\nThirdly, we should give our great State Governors the resources and flexibility they need with Medicaid to make sure no one is left out.\\nFourthly, we should implement legal reforms that protect patients and doctors from unnecessary costs that drive up the price of insurance -- and work to bring down the artificially high price of drugs and bring them down immediately.\\nFinally, the time has come to give Americans the freedom to purchase health insurance across State lines --- creating a truly competitive national marketplace that will bring cost way down and provide far better care.\\nEverything that is broken in our country can be fixed. Every problem can be solved. And every hurting family can find healing, and hope.\\nOur citizens deserve this, and so much more --- so why not join forces to finally get it done? On this and so many other things, Democrats and Republicans should get together and unite for the good of our country, and for the good of the American people.\\nMy administration wants to work with members in both parties to make childcare accessible and affordable, to help ensure new parents have paid family leave, to invest in women\\'s health, and to promote clean air and clear water, and to rebuild our military and our infrastructure.\\nTrue love for our people requires us to find common ground, to advance the common good, and to cooperate on behalf of every American child who deserves a brighter future.\\nAn incredible young woman is with us this evening who should serve as an inspiration to us all.\\nToday is Rare Disease day, and joining us in the gallery is a Rare Disease Survivor, Megan Crowley. Megan was diagnosed with Pompe Disease, a rare and serious illness, when she was 15 months old. She was not expected to live past 5.\\nOn receiving this news, Megan\\'s dad, John, fought with everything he had to save the life of his precious child. He founded a company to look for a cure, and helped develop the drug that saved Megan\\'s life. Today she is 20 years old -- and a sophomore at Notre Dame.\\nMegan\\'s story is about the unbounded power of a father\\'s love for a daughter.\\nBut our slow and burdensome approval process at the Food and Drug Administration keeps too many advances, like the one that saved Megan\\'s life, from reaching those in need.\\nIf we slash the restraints, not just at the FDA but across our Government, then we will be blessed with far more miracles like Megan.\\nIn fact, our children will grow up in a Nation of miracles.\\nBut to achieve this future, we must enrich the mind --- and the souls --- of every American child.\\nEducation is the civil rights issue of our time.\\nI am calling upon Members of both parties to pass an education bill that funds school choice for disadvantaged youth, including millions of African-American and Latino children. These families should be free to choose the public, private, charter, magnet, religious or home school that is right for them.\\nJoining us tonight in the gallery is a remarkable woman, Denisha Merriweather. As a young girl, Denisha struggled in school and failed third grade twice. But then she was able to enroll in a private center for learning, with the help of a tax credit scholarship program. Today, she is the first in her family to graduate, not just from high school, but from college. Later this year she will get her masters degree in social work.\\nWe want all children to be able to break the cycle of poverty just like Denisha.\\nBut to break the cycle of poverty, we must also break the cycle of violence.\\nThe murder rate in 2015 experienced its largest single-year increase in nearly half a century.\\nIn Chicago, more than 4,000 people were shot last year alone --- and the murder rate so far this year has been even higher.\\nThis is not acceptable in our society.\\nEvery American child should be able to grow up in a safe community, to attend a great school, and to have access to a high-paying job.\\nBut to create this future, we must work with --- not against --- the men and women of law enforcement.\\nWe must build bridges of cooperation and trust --- not drive the wedge of disunity and division.\\nPolice and sheriffs are members of our community. They are friends and neighbors, they are mothers and fathers, sons and daughters -- and they leave behind loved ones every day who worry whether or not they\\'ll come home safe and sound.\\nWe must support the incredible men and women of law enforcement.\\nAnd we must support the victims of crime.\\nI have ordered the Department of Homeland Security to create an office to serve American Victims. The office is called VOICE --- Victims Of Immigration Crime Engagement. We are providing a voice to those who have been ignored by our media, and silenced by special interests.\\nJoining us in the audience tonight are four very brave Americans whose government failed them.\\nTheir names are Jamiel Shaw, Susan Oliver, Jenna Oliver, and Jessica Davis.\\nJamiel\\'s 17-year-old son was viciously murdered by an illegal immigrant gang member, who had just been released from prison. Jamiel Shaw Jr. was an incredible young man, with unlimited potential who was getting ready to go to college where he would have excelled as a great quarterback. But he never got the chance. His father, who is in the audience tonight, has become a good friend of mine.\\nAlso with us are Susan Oliver and Jessica Davis. Their husbands --- Deputy Sheriff Danny Oliver and Detective Michael Davis --- were slain in the line of duty in California. They were pillars of their community. These brave men were viciously gunned down by an illegal immigrant with a criminal record and two prior deportations.\\nSitting with Susan is her daughter, Jenna. Jenna: I want you to know that your father was a hero, and that tonight you have the love of an entire country supporting you and praying for you.\\nTo Jamiel, Jenna, Susan and Jessica: I want you to know --- we will never stop fighting for justice. Your loved ones will never be forgotten, we will always honor their memory.\\nFinally, to keep America Safe we must provide the men and women of the United States military with the tools they need to prevent war and --- if they must --- to fight and to win.\\nI am sending the Congress a budget that rebuilds the military, eliminates the Defense sequester, and calls for one of the largest increases in national defense spending in American history.\\nMy budget will also increase funding for our veterans.\\nOur veterans have delivered for this Nation --- and now we must deliver for them.\\nThe challenges we face as a Nation are great. But our people are even greater.\\nAnd none are greater or braver than those who fight for America in uniform.\\nWe are blessed to be joined tonight by Carryn Owens, the widow of a U.S. Navy Special Operator, Senior Chief William \"Ryan\" Owens. Ryan died as he lived: a warrior, and a hero --- battling against terrorism and securing our Nation.\\nI just spoke to General Mattis, who reconfirmed that, and I quote, \"Ryan was a part of a highly successful raid that generated large amounts of vital intelligence that will lead to many more victories in the future against our enemies.\" Ryan\\'s legacy is etched into eternity. For as the Bible teaches us, there is no greater act of love than to lay down one\\'s life for one\\'s friends. Ryan laid down his life for his friends, for his country, and for our freedom --- we will never forget him.\\nTo those allies who wonder what kind of friend America will be, look no further than the heroes who wear our uniform.\\nOur foreign policy calls for a direct, robust and meaningful engagement with the world. It is American leadership based on vital security interests that we share with our allies across the globe.\\nWe strongly support NATO, an alliance forged through the bonds of two World Wars that dethroned fascism, and a Cold War that defeated communism.\\nBut our partners must meet their financial obligations.\\nAnd now, based on our very strong and frank discussions, they are beginning to do just that.\\nWe expect our partners, whether in NATO, in the Middle East, or the Pacific --- to take a direct and meaningful role in both strategic and military operations, and pay their fair share of the cost.\\nWe will respect historic institutions, but we will also respect the sovereign rights of nations.\\nFree nations are the best vehicle for expressing the will of the people --- and America respects the right of all nations to chart their own path. My job is not to represent the world. My job is to represent the United States of America. But we know that America is better off, when there is less conflict -- not more.\\nWe must learn from the mistakes of the past --- we have seen the war and destruction that have raged across our world.\\nThe only long-term solution for these humanitarian disasters is to create the conditions where displaced persons can safely return home and begin the long process of rebuilding.\\nAmerica is willing to find new friends, and to forge new partnerships, where shared interests align. We want harmony and stability, not war and conflict.\\nWe want peace, wherever peace can be found. America is friends today with former enemies. Some of our closest allies, decades ago, fought on the opposite side of these World Wars. This history should give us all faith in the possibilities for a better world.\\nHopefully, the 250th year for America will see a world that is more peaceful, more just and more free.\\nOn our 100th anniversary, in 1876, citizens from across our Nation came to Philadelphia to celebrate America\\'s centennial. At that celebration, the country\\'s builders and artists and inventors showed off their creations.\\nAlexander Graham Bell displayed his telephone for the first time.\\nRemington unveiled the first typewriter. An early attempt was made at electric light.\\nThomas Edison showed an automatic telegraph and an electric pen.\\nImagine the wonders our country could know in America\\'s 250th year.\\nThink of the marvels we can achieve if we simply set free the dreams of our people.\\nCures to illnesses that have always plagued us are not too much to hope.\\nAmerican footprints on distant worlds are not too big a dream.\\nMillions lifted from welfare to work is not too much to expect.\\nAnd streets where mothers are safe from fear -- schools where children learn in peace -- and jobs where Americans prosper and grow -- are not too much to ask.\\nWhen we have all of this, we will have made America greater than ever before. For all Americans.\\nThis is our vision. This is our mission.\\nBut we can only get there together.\\nWe are one people, with one destiny.\\nWe all bleed the same blood.\\nWe all salute the same flag.\\nAnd we are all made by the same God.\\nAnd when we fulfill this vision; when we celebrate our 250 years of glorious freedom, we will look back on tonight as when this new chapter of American Greatness began.\\nThe time for small thinking is over. The time for trivial fights is behind us.\\nWe just need the courage to share the dreams that fill our hearts.\\nThe bravery to express the hopes that stir our souls.\\nAnd the confidence to turn those hopes and dreams to action.\\nFrom now on, America will be empowered by our aspirations, not burdened by our fears ---\\ninspired by the future, not bound by the failures of the past ---\\nand guided by our vision, not blinded by our doubts.\\nI am asking all citizens to embrace this Renewal of the American Spirit. I am asking all members of Congress to join me in dreaming big, and bold and daring things for our country. And I am asking everyone watching tonight to seize this moment and --\\nBelieve in yourselves.\\nBelieve in your future.\\nAnd believe, once more, in America.\\nThank you, God bless you, and God Bless these United States.',\n",
       " \"Wow. Thank you. That's a lot of people, Phoenix, that's a lot of people.\\n\\nThank you very much.\\n\\nThank you, Phoenix. I am so glad to be back in Arizona.\\n\\nThe state that has a very, very special place in my heart. I love people of Arizona and together we are going to win the White House in November.\\n\\nNow, you know this is where it all began for me. Remember that massive crowd also. So, I said let's go and have some fun tonight. We're going to Arizona, OK?\\n\\nThis will be a little bit different. This won't be a rally speech, per se. Instead, I'm going to deliver a detailed policy address on one of the greatest challenges facing our country today, illegal immigration.\\n\\nI've just landed having returned from a very important and special meeting with the President of Mexico, a man I like and respect very much. And a man who truly loves his country, Mexico.\\n\\nAnd, by the way, just like I am a man who loves my country, the United States.\\n\\nWe agree on the importance of ending the illegal flow of drugs, cash, guns, and people across our border, and to put the cartels out of business.\\n\\nWe also discussed the great contributions of Mexican-American citizens to our two countries, my love for the people of Mexico, and the leadership and friendship between Mexico and the United States. It was a thoughtful and substantive conversation and it will go on for awhile. And, in the end we're all going to win. Both countries, we're all going to win.\\n\\n\\nThis is the first of what I expect will be many, many conversations. And, in a Trump administration we're going to go about creating a new relationship between our two countries, but it's going to be a fair relationship. We want fairness.\\n\\nBut to fix our immigration system, we must change our leadership in Washington and we must change it quickly.\\n\\nSadly, sadly there is no other way. The truth is our immigration system is worse than anybody ever realized. But the facts aren't known because the media won't report on them. The politicians won't talk about them and the special interests spend a lot of money trying to cover them up because they are making an absolute fortune. That's the way it is.\\n\\nToday, on a very complicated and very difficult subject, you will get the truth. The fundamental problem with the immigration system in our country is that it serves the needs of wealthy donors, political activists and powerful, powerful politicians. It's all you can do. Thank you. Thank you.\\n\\nLet me tell you who it does not serve. It does not serve you the American people. Doesn't serve you. When politicians talk about immigration reform, they usually mean the following, amnesty, open borders, lower wages. Immigration reform should mean something else entirely. It should mean improvements to our laws and policies to make life better for American citizens.\\n\\nThank you. But if we're going to make our immigration system work, then we have to be prepared to talk honestly and without fear about these important and very sensitive issues. For instance, we have to listen to the concerns that working people, our forgotten working people, have over the record pace of immigration and it's impact on their jobs, wages, housing, schools, tax bills and general living conditions.\\n\\nThese are valid concerns expressed by decent and patriotic citizens from all backgrounds, all over. We also have to be honest about the fact that not everyone who seeks to join our country will be able to successfully assimilate. Sometimes it's just not going to work out. It's our right, as a sovereign nation to chose immigrants that we think are the likeliest to thrive and flourish and love us.\\n\\nThen there is the issue of security. Countless innocent American lives have been stolen because our politicians have failed in their duty to secure our borders and enforce our laws like they have to be enforced. I have met with many of the great parents who lost their children to sanctuary cities and open borders. So many people, so many, many people. So sad. They will be joining me on this stage in a little while and I look forward to introducing, these are amazing, amazing people.\\n\\nCountless Americans who have died in recent years would be alive today if not for the open border policies of this administration and the administration that causes this horrible, horrible thought process, called Hillary Clinton.\\n\\nThis includes incredible Americans like 21 year old Sarah Root. The man who killed her arrived at the border, entered Federal custody and then was released into the U.S., think of it, into the U.S. community under the policies of the White House Barack Obama and Hillary Clinton. Weak, weak policies. Weak and foolish policies.\\n\\nHe was released again after the crime, and now he's out there at large. Sarah had graduated from college with a 4.0, top student in her class one day before her death.\\n\\nAlso among the victims of the Obama-Clinton open borders policy was Grant Ronnebeck, a 21-year-old convenience store clerk and a really good guy from Mesa, Arizona. A lot of you have known about Grant. \\n\\nHe was murdered by an illegal immigrant gang member previously convicted of burglary, who had also been released from federal custody, and they knew it was going to happen again.\\n\\nAnother victim is Kate Steinle. Gunned down in the sanctuary city of San Francisco, by an illegal immigrant, deported five previous times. And they knew he was no good.\\n\\n\\n\\nThen there is the case of 90-year-old Earl Olander, who was brutally beaten and left to bleed to death in his home, 90 years old and defenseless. The perpetrators were illegal immigrants with criminal records a mile long, who did not meet Obama administration standards for removal. And they knew it was going to happen.\\n\\nIn California, a 64-year-old Air Force veteran, a great woman, according to everybody that knew her, Marilyn Pharis, was sexually assaulted and beaten to death with a hammer. Her killer had been arrested on multiple occasions but was never, ever deported, despite the fact that everybody wanted him out.\\n\\nA 2011 report from the Government Accountability Office found that illegal immigrants and other non-citizens, in our prisons and jails together, had around 25,000 homicide arrests to their names, 25,000.\\n\\nOn top of that, illegal immigration costs our country more than $113 billion dollars a year.\\n\\nAnd this is what we get. For the money we are going to spend on illegal immigration over the next 10 years, we could provide 1 million at-risk students with a school voucher, which so many people are wanting.\\n\\nWhile there are many illegal immigrants in our country who are good people, many, many, this doesn't change the fact that most illegal immigrants are lower skilled workers with less education, who compete directly against vulnerable American workers, and that these illegal workers draw much more out from the system than they can ever possibly pay back.\\n\\nAnd they're hurting a lot of our people that cannot get jobs under any circumstances.\\n\\nBut these facts are never reported. Instead, the media and my opponent discuss one thing and only one thing, the needs of people living here illegally. In many cases, by the way, they're treated better than our vets.\\n\\nNot going to happen anymore, folks. November 8th. Not going to happen anymore.\\n\\nThe truth is, the central issue is not the needs of the 11 million illegal immigrants or however many there may be -- and honestly we've been hearing that number for years. It's always 11 million.\\n\\nOur government has no idea. It could be 3 million. It could be 30 million. They have no idea what the number is.\\n\\nFrankly our government has no idea what they're doing on many, many fronts, folks.\\n\\nBut whatever the number, that's never really been the central issue. It will never be a central issue. It doesn't matter from that standpoint. Anyone who tells you that the core issue is the needs of those living here illegally has simply spent too much time in Washington.\\n\\nOnly the out of touch media elites think the biggest problems facing America -- you know this, this is what they talk about, facing American society today is that there are 11 million illegal immigrants who don't have legal status. And, they also think the biggest thing, and you know this, it's not nuclear, and it's not ISIS, it's not Russia, it's not China, it's global warming.\\n\\nTo all the politicians, donors, and special interests, hear these words from me and all of you today. There is only one core issue in the immigration debate, and that issue is the well being of the American people.\\n\\nNothing even comes a close second. Hillary Clinton, for instance, talks constantly about her fears that families will be separated, but she's not talking about the American families who have been permanently separated from their loved ones because of a preventable homicide, because of a preventable death, because of murder.\\n\\nNo, she's only talking about families who come here in violation of the law. We will treat everyone living or residing in our country with great dignity. So important.\\n\\nWe will be fair, just, and compassionate to all, but our greatest compassion must be for our American citizens.\\n\\nThank you.\\n\\nPresident Obama and Hillary Clinton have engaged in gross dereliction of duty by surrendering the safety of the American people to open borders, and you know it better than anybody right here in Arizona. You know it.\\n\\nPresident Obama and Hillary Clinton support sanctuary cities. They support catch and release on the border. they support visa overstays. They support the release of dangerous, dangerous, dangerous, criminals from detention. And, they support unconstitutional executive amnesty.\\n\\nHillary Clinton has pledged amnesty in her first 100 days, and her plan will provide Obamacare, Social Security, and Medicare for illegal immigrants, breaking the federal budget.\\n\\nOn top of that she promises uncontrolled, low-skilled immigration that continues to reduce jobs and wages for American workers, and especially for African-American and Hispanic workers within our country. Our citizens.\\n\\nMost incredibly, because to me this is unbelievable, we have no idea who these people are, where they come from. I always say Trojan Horse. Watch what's going to happen, folks. It's not going to be pretty.\\n\\nThis includes her plan to bring in 620,000 new refugees from Syria and that region over a short period of time. And even yesterday, when you were watching the news, you saw thousands and thousands of people coming in from Syria. What is wrong with our politicians, our leaders if we can call them that. What the hell are we doing?\\n\\nHard to believe. Hard to believe. Now that you've heard about Hillary Clinton's plan, about which she has not answered a single question, let me tell you about my plan. And do you notice --\\n\\nAnd do you notice all the time for weeks and weeks of debating my plan, debating, talking about it, what about this, what about that. They never even mentioned her plan on immigration because she doesn't want to get into the quagmire. It's a tough one, she doesn't know what she's doing except open borders and let everybody come in and destroy our country by the way.\\n\\nWhile Hillary Clinton meets only with donors and lobbyists, my plan was crafted with the input from Federal Immigration offices, very great people. Among the top immigration experts anywhere in this country, who represent workers, not corporations, very important to us.\\n\\nI also worked with lawmakers, who've led on this issue on behalf of American citizens for many years. And most importantly I've met with the people directly impacted by these policies. So important.\\n\\n \\n\\n\\nNumber one, are you ready? Are you ready?\\n\\nWe will build a great wall along the southern border. \\n\\nAnd Mexico will pay for the wall.\\n\\nOne hundred percent. They don't know it yet, but they're going to pay for it. And they're great people and great leaders but they're going to pay for the wall. On day one, we will begin working on intangible, physical, tall, power, beautiful southern border wall.\\n\\nWe will use the best technology, including above and below ground sensors that's the tunnels. Remember that, above and below.\\n\\nAbove and below ground sensors. Towers, aerial surveillance and manpower to supplement the wall, find and dislocate tunnels and keep out criminal cartels and Mexico you know that, will work with us. I really believe it. Mexico will work with us. I absolutely believe it. And especially after meeting with their wonderful, wonderful president today. I really believe they want to solve this problem along with us, and I'm sure they will.\\n\\nNumber two, we are going to end catch and release. We catch them, oh go ahead. We catch them, go ahead.\\n\\nUnder my administration, anyone who illegally crosses the border will be detained until they are removed out of our country and back to the country from which they came.\\n\\nAnd they'll be brought great distances. We're not dropping them right across. They learned that. President Eisenhower. They'd drop them across, right across, and they'd come back. And across.\\n\\nThen when they flew them to a long distance, all of a sudden that was the end. We will take them great distances. But we will take them to the country where they came from, OK?\\n\\nNumber three. Number three, this is the one, I think it's so great. It's hard to believe, people don't even talk about it. Zero tolerance for criminal aliens. Zero. Zero.\\n\\nZero. They don't come in here. They don't come in here.\\n\\nAccording to federal data, there are at least 2 million, 2 million, think of it, criminal aliens now inside of our country, 2 million people criminal aliens. We will begin moving them out day one. As soon as I take office. Day one. In joint operation with local, state, and federal law enforcement.\\n\\nNow, just so you understand, the police, who we all respect -- say hello to the police. Boy, they don't get the credit they deserve. I can tell you. They're great people. But the police and law enforcement, they know who these people are.\\n\\nThey live with these people. They get mocked by these people. They can't do anything about these people, and they want to. They know who these people are. Day one, my first hour in office, those people are gone.\\n\\nAnd you can call it deported if you want. The press doesn't like that term. You can call it whatever the hell you want. They're gone.\\n\\nBeyond the 2 million, and there are vast numbers of additional criminal illegal immigrants who have fled, but their days have run out in this country. The crime will stop. They're going to be gone. It will be over.\\n\\nThey're going out. They're going out fast.\\n\\nMoving forward. We will issue detainers for illegal immigrants who are arrested for any crime whatsoever, and they will be placed into immediate removal proceedings if we even have to do that.\\n\\nWe will terminate the Obama administration's deadly, and it is deadly, non-enforcement policies that allow thousands of criminal aliens to freely roam our streets, walk around, do whatever they want to do, crime all over the place.\\n\\nThat's over. That's over, folks. That's over.\\n\\nSince 2013 alone, the Obama administration has allowed 300,000 criminal aliens to return back into United States communities. These are individuals encountered or identified by ICE, but who were not detained or processed for deportation because it wouldn't have been politically correct.\\n\\nMy plan also includes cooperating closely with local jurisdictions to remove criminal aliens immediately. We will restore the highly successful Secure Communities Program. Good program. We will expand and revitalize the popular 287(g) partnerships, which will help to identify hundreds of thousands of deportable aliens in local jails that we don't even know about.\\n\\nBoth of these programs have been recklessly gutted by this administration. And those were programs that worked.\\n\\nThis is yet one more area where we are headed in a totally opposite direction. There's no common sense, there's no brain power in our administration by our leader, or our leaders. None, none, none.\\n\\nHereâ€™s an interesting graphic from a study published this summer on voting attitudes among people who heard incumbents use restrictive rhetoric on immigration -- tough enforcement, no amnesty, deportations -- and its affect on their own opinions.\\n\\nOn my first day in office I am also going to ask Congress to pass Kate's Law, named for Kate Steinle.\\n\\nTo ensure that criminal aliens convicted of illegal reentry receive strong mandatory minimum sentences. Strong.\\n\\nAnd then we get them out.\\n\\nAnother reform I'm proposing is the passage of legislation named for Detective Michael Davis and Deputy Sheriff Danny Oliver, to law enforcement officers recently killed by a previously deported illegal immigrant.\\n\\nThe Davis-Oliver bill will enhance cooperation with state and local authorities to ensure that criminal immigrants and terrorists are swiftly, really swiftly, identified and removed. And they will go face, believe me. They're going to go.\\n\\nWe're going to triple the number of ICE deportation officers.\\n\\nWithin ICE I am going to create a new special deportation task force focused on identifying and quickly removing the most dangerous criminal illegal immigrants in America who have evaded justice just like Hillary Clinton has evaded justice, OK?\\n\\nMaybe they'll be able to deport her.\\n\\nThe local police who know every one of these criminals, and they know each and every one by name, by crime, where they live, they will work so fast. And our local police will be so happy that they don't have to be abused by these thugs anymore. There's no great mystery to it, they've put up with it for years, and no finally we will turn the tables and law enforcement and our police will be allowed to clear up this dangerous and threatening mess.\\n\\nWe're also going to hire 5,000 more Border Patrol agents.\\n\\nWho gave me their endorsement, 16,500 gave me their endorsement.\\n\\nAnd put more of them on the border instead of behind desks which is good. We will expand the number of border patrol stations significantly.\\n\\nI've had a chance to spend time with these incredible law enforcement officers, and I want to take a moment to thank them. What they do is incredible.\\n\\nAnd getting their endorsement means so much to me. More to me really than I can say. Means so much. First time they've ever endorsed a presidential candidate.\\n\\nNumber four, block funding for sanctuary cities. We block the funding. No more funds.\\n\\nWe will end the sanctuary cities that have resulted in so many needless deaths. Cities that refuse to cooperate with federal authorities will not receive taxpayer dollars, and we will work with Congress to pass legislation to protect those jurisdictions that do assist federal authorities.\\n\\nNumber five, cancel unconstitutional executive orders and enforce all immigration laws.\\n\\nWe will immediately terminate President Obama's two illegal executive amnesties in which he defied federal law and the Constitution to give amnesty to approximately five million illegal immigrants, five million.\\n\\nAnd how about all the millions that are waiting on line, going through the process legally? So unfair.\\n\\nHillary Clinton has pledged to keep both of these illegal amnesty programs, including the 2014 amnesty which has been blocked by the United States Supreme Court. Great.\\n\\nClinton has also pledged to add a third executive amnesty. And by the way, folks, she will be a disaster for our country, a disaster in so many other ways.\\n\\nAnd don't forget the Supreme Court of the United States. Don't forget that when you go to vote on November 8. And don't forget your Second Amendment. And don't forget the repeal and replacement of Obamacare.\\n\\nAnd don't forget building up our depleted military. And don't forget taking care of our vets. Don't forget our vets. They have been forgotten.\\n\\nClinton's plan would trigger a constitutional crisis unlike almost anything we have ever seen before. In effect, she would be abolishing the lawmaking powers of Congress in order to write her own laws from the Oval Office. And you see what bad judgment she has. She has seriously bad judgment.\\n\\nCan you imagine? In a Trump administration all immigration laws will be enforced, will be enforced. As with any law enforcement activity, we will set priorities. But unlike this administration, no one will be immune or exempt from enforcement. And ICE and Border Patrol officers will be allowed to do their jobs the way their jobs are supposed to be done.\\n\\nAnyone who has entered the United States illegally is subject to deportation. That is what it means to have laws and to have a country. Otherwise we don't have a country.\\n\\nOur enforcement priorities will include removing criminals, gang members, security threats, visa overstays, public charges. That is those relying on public welfare or straining the safety net along with millions of recent illegal arrivals and overstays who've come here under this current corrupt administration.\\n\\n\\nNumber six, we are going to suspend the issuance of visas to any place where adequate screening cannot occur.\\n\\nAccording to data provided by the Senate Subcommittee on Immigration, and the national interest between 9/11 and the end of 2014, at least 380 foreign born individuals were convicted in terror cases inside the United States. And even right now the largest number of people are under investigation for exactly this that we've ever had in the history of our country.\\n\\nOur country is a mess. We don't even know what to look for anymore, folks. Our country has to straighten out. And we have to straighten out fast.\\n\\nThe number is likely higher. But the administration refuses to provide this information, even to Congress. As soon as I enter office I am going to ask the Department of State, which has been brutalized by Hillary Clinton, brutalized.\\n\\nHomeland Security and the Department of Justice to begin a comprehensive review of these cases in order to develop a list of regions and countries from which immigration must be suspended until proven and effective vetting mechanisms can be put in place.\\n\\nI call it extreme vetting right? Extreme vetting. I want extreme. It's going to be so tough, and if somebody comes in that's fine but they're going to be good. It's extreme.\\n\\nAnd if people don't like it, we've got have a country folks. Got to have a country. Countries in which immigration will be suspended would include places like Syria and Libya. And we are going to stop the tens of thousands of people coming in from Syria. We have no idea who they are, where they come from. There's no documentation. There's no paperwork. It's going to end badly folks. It's going to end very, very badly.\\n\\nFor the price of resettling, one refugee in the United States, 12 could be resettled in a safe zone in their home region. Which I agree with 100 percent. We have to build safe zones and we'll get the money from Gulf states. We don't want to put up the money. We owe almost $20 trillion. Doubled since Obama took office, our national debt.\\n\\nBut we will get the money from Gulf states and others. We'll supervise it. We'll build safe zones which is something that I think all of us want to see.\\n\\nAnother reform, involves new screening tests for all applicants that include, and this is so important, especially if you get the right people. And we will get the right people. An ideological certification to make sure that those we are admitting to our country share our values and love our people.\\n\\nThank you. We're very proud of our country. Aren't we? Really? With all it's going through, we're very proud of our country. For instance, in the last five years, we've admitted nearly 100,000 immigrants from Iraq and Afghanistan. And these two countries according to Pew Research, a majority of residents say that the barbaric practice of honor killings against women are often or sometimes justified. That's what they say.\\n\\nThat's what they say. They're justified. Right? And we're admitting them to our country. Applicants will be asked their views about honor killings, about respect for women and gays and minorities. Attitudes on radical Islam, which our President refuses to say and many other topics as part of this vetting procedure. And if we have the right people doing it, believe me, very, very few will slip through the cracks. Hopefully, none.\\n\\n\\nNumber seven, we will insure that other countries take their people back when they order them deported.\\n\\nThere are at least 23 countries that refuse to take their people back after they've been ordered to leave the United States. Including large numbers of violent criminals, they won't take them back. So we say, OK, we'll keep them. Not going to happen with me, not going to happen with me.\\n\\nDue to a Supreme Court decision, if these violent offenders cannot be sent home, our law enforcement officers have to release them into your communities.\\n\\nAnd by the way, the results are horrific, horrific. There are often terrible consequences, such as Casey Chadwick's tragic death in Connecticut just last year. Yet despite the existence of a law that commands the Secretary of State to stop issuing visas to these countries.\\n\\nSecretary Hillary Clinton ignored this law and refused to use this powerful tool to bring nations into compliance. And, they would comply if we would act properly.\\n\\nIn other words, if we had leaders that knew what they were doing, which we don't.\\n\\nThe result of her misconduct was the release of thousands and thousands of dangerous criminal aliens who should have been sent home to their countries. Instead we have them all over the place. Probably a couple in this room as a matter of fact, but I hope not.\\n\\nAccording to a report for the Boston Globe from the year 2008 to 2014 nearly 13,000 criminal aliens were released back into U.S. communities because their home countries would not, under any circumstances, take them back. Hard to believe with the power we have. Hard to believe.\\n\\nWe're like the big bully that keeps getting beat up. You ever see that? The big bully that keeps getting beat up.\\n\\nThese 13,000 release occurred on Hillary Clinton's watch. She had the power and the duty to stop it cold, and she decided she would not do it.\\n\\nAnd, Arizona knows better than most exactly what I'm talking about.\\n\\n\\nThose released include individuals convicted of killings, sexual assaults, and some of the most heinous crimes imaginable.\\n\\nThe Boston Globe writes that a Globe review of 323 criminals released in New England from 2008 to 2012 found that as many as 30 percent committed new offenses, including rape, attempted murder, and child molestation. We take them, we take them.\\n\\nNumber eight, we will finally complete the biometric entry-exit visa tracking system which we need desperately. For years Congress has required biometric entry-exit visa tracking systems, but it has never been completed. The politicians are all talk, no action, never happens. Never happens.\\n\\nHillary Clinton, all talk. Unfortunately when there is action it's always the wrong decision. You ever notice? In my administration we will ensure that this system is in place. And, I will tell you, it will be on land, it will be on sea, it will be in air. We will have a proper tracking system.\\n\\nApproximately half of new illegal immigrants came on temporary visas and then never, ever left. Why should the? Nobody's telling them to leave. Stay as long as you want, we'll take care of you.\\n\\nBeyond violating our laws, visa overstays, pose -- and they really are a big problem, pose a substantial threat to national security. The 9/11 Commission said that this tracking system would be a high priority and would have assisted law enforcement and intelligence officials in august and September in 2001 in conducting a search for two of the 9/11 hijackers that were in the United States expired visas.\\n\\nAnd, you know what that would have meant, what that could have meant. Wouldn't that have been wonderful, right? What that could have meant?\\n\\nLast year alone nearly half a million individuals overstayed their temporary visas. Removing these overstays will be a top priority of my administration.\\n\\nIf people around the world believe they can just come on a temporary visa and never, ever leave, the Obama-Clinton policy, that's what it is, then we have a completely open border, and we no longer have a country.\\n\\nWe must send a message that visa expiration dates will be strongly enforced.\\n\\nNumber nine, we will turn off the jobs and benefits magnet.\\n\\nWe will ensure that E-Verify is used to the fullest extent possible under existing law, and we will work with Congress to strengthen and expand its use across the country.\\n\\nImmigration law doesn't exist for the purpose of keeping criminals out. It exists to protect all aspects of American life. The work site, the welfare office, the education system, and everything else.\\n\\nThat is why immigration limits are established in the first place. If we only enforced the laws against crime, then we have an open border to the entire world. We will enforce all of our immigration laws.\\n\\nAnd the same goes for government benefits. The Center for Immigration Studies estimates that 62 percent of households headed by illegal immigrants use some form of cash or non-cash welfare programs like food stamps or housing assistance.\\n\\nTremendous costs, by the way, to our country. Tremendous costs. This directly violates the federal public charge law designed to protect the United States Treasury. Those who abuse our welfare system will be priorities for immediate removal.\\n\\nNumber 10, we will reform legal immigration to serve the best interests of America and its workers, the forgotten people. Workers. We're going to take care of our workers.\\n\\nAnd by the way, and by the way, we're going to make great trade deals. We're going to renegotiate trade deals. We're going to bring our jobs back home. We're going to bring our jobs back home.\\n\\nWe have the most incompetently worked trade deals ever negotiated probably in the history of the world, and that starts with NAFTA. And now they want to go TPP, one of the great disasters.\\n\\nWe're going to bring our jobs back home. And if companies want to leave Arizona and if they want to leave other states, there's going to be a lot of trouble for them. It's not going to be so easy. There will be consequence. Remember that. There will be consequence. They're not going to be leaving, go to another country, make the product, sell it into the United States, and all we end up with is no taxes and total unemployment. It's not going to happen. There will be consequences.\\n\\nWe've admitted 59 million immigrants to the United States between 1965 and 2015. Many of these arrivals have greatly enriched our country. So true. But we now have an obligation to them and to their children to control future immigration as we are following, if you think, previous immigration waves.\\n\\nWe've had some big waves. And tremendously positive things have happened. Incredible things have happened. To ensure assimilation we want to ensure that it works. Assimilation, an important word. Integration and upward mobility.\\n\\nWithin just a few years immigration as a share of national population is set to break all historical records. The time has come for a new immigration commission to develop a new set of reforms to our legal immigration system in order to achieve the following goals.\\n\\nTo keep immigration levels measured by population share within historical norms. To select immigrants based on their likelihood of success in U.S. society and their ability to be financially self- sufficient.\\n\\nWe take anybody. Come on in, anybody. Just come on in. Not anymore.\\n\\nYou know, folks, it's called a two-way street. It is a two-way street, right? We need a system that serves our needs, not the needs of others. Remember, under a Trump administration it's called America first. Remember that.\\n\\nTo choose immigrants based on merit. Merit, skill, and proficiency. Doesn't that sound nice? And to establish new immigration controls to boost wages and to ensure that open jobs are offered to American workers first. And that in particular African- American and Latino workers who are being shut out in this process so unfairly.\\n\\nAnd Hillary Clinton is going to do nothing for the African- American worker, the Latino worker. She's going to do nothing. Give me your vote, she says, on November eighth. And then she'll say, so long, see you in four years. That's what it is.\\n\\nShe is going to do nothing. And just look at the past. She's done nothing. She's been there for 35 years. She's done nothing. And I say what do you have to lose? Choose me. Watch how good we're going to do together. Watch.\\n\\nYou watch. We want people to come into our country, but they have to come into our country legally and properly vetted, and in a manner that serves the national interest. We've been living under outdated immigration rules from decades ago. They're decades and decades old.\\n\\nTo avoid this happening in the future, I believe we should sunset our visa laws so that Congress is forced to periodically revise and revisit them to bring them up to date. They're archaic. They're ancient. We wouldn't put our entire federal budget on auto pilot for decades, so why should we do the same for the very, very complex subject of immigration?\\n\\nSo let's now talk about the big picture. These 10 steps, if rigorously followed and enforced, will accomplish more in a matter of months than our politicians have accomplished on this issue in the last 50 years. It's going to happen, folks. Because I am proudly not a politician, because I am not behold to any special interest, I've spent a lot of money on my campaign, I'll tell you. I write those checks. Nobody owns Trump.\\n\\nI will get this done for you and for your family. We'll do it right. You'll be proud of our country again. We'll do it right. We will accomplish all of the steps outlined above. And, when we do, peace and law and justice and prosperity will prevail. Crime will go down. Border crossings will plummet. Gangs will disappear.\\n\\nAnd the gangs are all over the place. And welfare use will decrease. We will have a peace dividend to spend on rebuilding America, beginning with our American inner cities. We're going to rebuild them, for once and for all.\\n\\nFor those here illegally today, who are seeking legal status, they will have one route and one route only. To return home and apply for reentry like everybody else, under the rules of the new legal immigration system that I have outlined above. Those who have left to seek entry --\\n\\nThank you. Thank you. Those who have left to seek entry under this new system -- and it will be an efficient system -- will not be awarded surplus visas, but will have to apply for entry under the immigration caps or limits that will be established in the future.\\n\\nWe will break the cycle of amnesty and illegal immigration. We will break the cycle. There will be no amnesty.\\n\\nOur message to the world will be this. You cannot obtain legal status or become a citizen of the United States by illegally entering our country. Can't do it.\\n\\nThis declaration alone will help stop the crisis of illegal crossings and illegal overstays, very importantly. People will know that you can't just smuggle in, hunker down and wait to be legalized. It's not going to work that way. Those days are over.\\n\\nImportantly, in several years when we have accomplished all of our enforcement and deportation goals and truly ended illegal immigration for good, including the construction of a great wall, which we will have built in record time. And at a reasonable cost, which you never hear from the government.\\n\\nAnd the establishment of our new lawful immigration system then and only then will we be in a position to consider the appropriate disposition of those individuals who remain.\\n\\nThat discussion can take place only in an atmosphere in which illegal immigration is a memory of the past, no longer with us, allowing us to weigh the different options available based on the new circumstances at the time.\\n\\nRight now, however, we're in the middle of a jobs crisis, a border crisis and a terrorism crisis like never before. All energies of the federal government and the legislative process must now be focused on immigration security. That is the only conversation we should be having at this time, immigration security. Cut it off.\\n\\nWhether it's dangerous materials being smuggled across the border, terrorists entering on visas or Americans losing their jobs to foreign workers, these are the problems we must now focus on fixing. And the media needs to begin demanding to hear Hillary Clinton's answer on how her policies will affect Americans and their security.\\n\\nThese are matters of life and death for our country and its people, and we deserve answers from Hillary Clinton. And do you notice, she doesn't answer.\\n\\nShe didn't go to Louisiana. She didn't go to Mexico. She was invited.\\n\\nShe doesn't have the strength or the stamina to make America great again. Believe me.\\n\\nWhat we do know, despite the lack of media curiosity, is that Hillary Clinton promises a radical amnesty combined with a radical reduction in immigration enforcement. Just ask the Border Patrol about Hillary Clinton. You won't like what you're hearing.\\n\\nThe result will be millions more illegal immigrants; thousands of more violent, horrible crimes; and total chaos and lawlessness. That's what's going to happen, as sure as you're standing there.\\n\\nThis election, and I believe this, is our last chance to secure the border, stop illegal immigration and reform our laws to make your life better. I really believe this is it. This is our last time. November 8. November 8. You got to get out and vote on November 8.\\n\\nIt's our last chance. It's our last chance. And that includes Supreme Court justices and Second Amendment. Remember that.\\n\\nSo I want to remind everyone what we're fighting for and who we are fighting for.\\n\\nI am going to ask -- these are really special people that I've gotten to know. Iâ€™m going to ask all the Angel Moms to come join me on the stage right now. These are amazing women.\\n\\nThese are amazing people.\\n\\n\\n \\n\\nThank you.\\n\\nThese are amazing people, and I am not asking for their endorsement, believe me that. I just think I've gotten to know so many of them, and many more, from our group. But they are incredible people and what they're going through is incredible, and there's just no reason for it. Let's give them a really tremendous hand.\\n\\nThat's tough stuff, I will tell you. That is tough stuff. Incredible people.\\n\\nSo, now is the time for these voices to be heard. Now is the time for the media to begin asking questions on their behalf. Now is the time for all of us as one country, Democrat, Republican, liberal, conservative to band together to deliver justice, and safety, and security for all Americans.\\n\\nLet's fix this horrible, horrible, problem. It can be fixed quickly. Let's our secure our border.\\n\\nLet's stop the drugs and the crime from pouring into our country. Let's protect our social security and Medicare. Let's get unemployed Americans off the welfare and back to work in their own country.\\n\\nThis has been an incredible evening. We're going to remember this evening. November 8, we have to get everybody. This is such an important state. November 8 we have to get everybody to go out and vote.\\n\\nWe're going to bring -- thank you, thank you. We're going to take our country back, folks. This is a movement. We're going to take our country back.\\n\\nThank you.\\n\\nThank you.\\n\\nThis is an incredible movement. The world is talking about it. The world is talking about it and by the way, if you haven't been looking to what's been happening at the polls over the last three or four days I think you should start looking. You should start looking.\\n\\nTogether we can save American lives, American jobs, and American futures. Together we can save America itself. Join me in this mission, we're going to make America great again.\\n\\nThank you. I love you. God bless you, everybody. God bless you. God bless you, thank you.\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn=bm25.get_top_n(tokenized_query, corpus, n=2)\n",
    "tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr. Speaker, Mr. Vice President, Members of Congress, the First Lady of the United States, and Citizens of America:\\nTonight, as we mark the conclusion of our celebration of Black History Month, we are reminded of our Nation\\'s path toward civil rights and the work that still remains. Recent threats targeting Jewish Community Centers and vandalism of Jewish cemeteries, as well as last week\\'s shooting in Kansas City, remind us that while we may be a Nation divided on policies, we are a country that stands united in condemning hate and evil in all its forms.\\nEach American generation passes the torch of truth, liberty and justice --- in an unbroken chain all the way down to the present.\\nThat torch is now in our hands. And we will use it to light up the world. I am here tonight to deliver a message of unity and strength, and it is a message deeply delivered from my heart.\\nA new chapter of American Greatness is now beginning.\\nA new national pride is sweeping across our Nation.\\nAnd a new surge of optimism is placing impossible dreams firmly within our grasp.\\nWhat we are witnessing today is the Renewal of the American Spirit.\\nOur allies will find that America is once again ready to lead.\\nAll the nations of the world -- friend or foe -- will find that America is strong, America is proud, and America is free.\\nIn 9 years, the United States will celebrate the 250th anniversary of our founding -- 250 years since the day we declared our Independence.\\nIt will be one of the great milestones in the history of the world.\\nBut what will America look like as we reach our 250th year? What kind of country will we leave for our children?\\nI will not allow the mistakes of recent decades past to define the course of our future.\\nFor too long, we\\'ve watched our middle class shrink as we\\'ve exported our jobs and wealth to foreign countries.\\nWe\\'ve financed and built one global project after another, but ignored the fates of our children in the inner cities of Chicago, Baltimore, Detroit -- and so many other places throughout our land.\\nWe\\'ve defended the borders of other nations, while leaving our own borders wide open, for anyone to cross -- and for drugs to pour in at a now unprecedented rate.\\nAnd we\\'ve spent trillions of dollars overseas, while our infrastructure at home has so badly crumbled.\\nThen, in 2016, the earth shifted beneath our feet. The rebellion started as a quiet protest, spoken by families of all colors and creeds --- families who just wanted a fair shot for their children, and a fair hearing for their concerns.\\nBut then the quiet voices became a loud chorus -- as thousands of citizens now spoke out together, from cities small and large, all across our country.\\nFinally, the chorus became an earthquake -- and the people turned out by the tens of millions, and they were all united by one very simple, but crucial demand, that America must put its own citizens first ... because only then, can we truly MAKE AMERICA GREAT AGAIN.\\nDying industries will come roaring back to life. Heroic veterans will get the care they so desperately need.\\nOur military will be given the resources its brave warriors so richly deserve.\\nCrumbling infrastructure will be replaced with new roads, bridges, tunnels, airports and railways gleaming across our beautiful land.\\nOur terrible drug epidemic will slow down and ultimately, stop.\\nAnd our neglected inner cities will see a rebirth of hope, safety, and opportunity.\\nAbove all else, we will keep our promises to the American people.\\nIt\\'s been a little over a month since my inauguration, and I want to take this moment to update the Nation on the progress I\\'ve made in keeping those promises.\\nSince my election, Ford, Fiat-Chrysler, General Motors, Sprint, Softbank, Lockheed, Intel, Walmart, and many others, have announced that they will invest billions of dollars in the United States and will create tens of thousands of new American jobs.\\nThe stock market has gained almost three trillion dollars in value since the election on November 8th, a record. We\\'ve saved taxpayers hundreds of millions of dollars by bringing down the price of the fantastic new F-35 jet fighter, and will be saving billions more dollars on contracts all across our Government. We have placed a hiring freeze on non-military and non-essential Federal workers.\\nWe have begun to drain the swamp of government corruption by imposing a 5 year ban on lobbying by executive branch officials --- and a lifetime ban on becoming lobbyists for a foreign government.\\nWe have undertaken a historic effort to massively reduce jobâ€‘crushing regulations, creating a deregulation task force inside of every Government agency; imposing a new rule which mandates that for every 1 new regulation, 2 old regulations must be eliminated; and stopping a regulation that threatens the future and livelihoods of our great coal miners.\\nWe have cleared the way for the construction of the Keystone and Dakota Access Pipelines -- thereby creating tens of thousands of jobs -- and I\\'ve issued a new directive that new American pipelines be made with American steel.\\nWe have withdrawn the United States from the job-killing Trans-Pacific Partnership.\\nWith the help of Prime Minister Justin Trudeau, we have formed a Council with our neighbors in Canada to help ensure that women entrepreneurs have access to the networks, markets and capital they need to start a business and live out their financial dreams.\\nTo protect our citizens, I have directed the Department of Justice to form a Task Force on Reducing Violent Crime.\\nI have further ordered the Departments of Homeland Security and Justice, along with the Department of State and the Director of National Intelligence, to coordinate an aggressive strategy to dismantle the criminal cartels that have spread across our Nation.\\nWe will stop the drugs from pouring into our country and poisoning our youth -- and we will expand treatment for those who have become so badly addicted.\\nAt the same time, my Administration has answered the pleas of the American people for immigration enforcement and border security. By finally enforcing our immigration laws, we will raise wages, help the unemployed, save billions of dollars, and make our communities safer for everyone. We want all Americans to succeed --- but that can\\'t happen in an environment of lawless chaos. We must restore integrity and the rule of law to our borders.\\nFor that reason, we will soon begin the construction of a great wall along our southern border. It will be started ahead of schedule and, when finished, it will be a very effective weapon against drugs and crime.\\nAs we speak, we are removing gang members, drug dealers and criminals that threaten our communities and prey on our citizens. Bad ones are going out as I speak tonight and as I have promised.\\nTo any in Congress who do not believe we should enforce our laws, I would ask you this question: what would you say to the American family that loses their jobs, their income, or a loved one, because America refused to uphold its laws and defend its borders?\\nOur obligation is to serve, protect, and defend the citizens of the United States. We are also taking strong measures to protect our Nation from Radical Islamic Terrorism.\\nAccording to data provided by the Department of Justice, the vast majority of individuals convicted for terrorism-related offenses since 9/11 came here from outside of our country. We have seen the attacks at home --- from Boston to San Bernardino to the Pentagon and yes, even the World Trade Center.\\nWe have seen the attacks in France, in Belgium, in Germany and all over the world.\\nIt is not compassionate, but reckless, to allow uncontrolled entry from places where proper vetting cannot occur. Those given the high honor of admission to the United States should support this country and love its people and its values.\\nWe cannot allow a beachhead of terrorism to form inside America -- we cannot allow our Nation to become a sanctuary for extremists.\\nThat is why my Administration has been working on improved vetting procedures, and we will shortly take new steps to keep our Nation safe -- and to keep out those who would do us harm.\\nAs promised, I directed the Department of Defense to develop a plan to demolish and destroy ISIS -- a network of lawless savages that have slaughtered Muslims and Christians, and men, women, and children of all faiths and beliefs. We will work with our allies, including our friends and allies in the Muslim world, to extinguish this vile enemy from our planet.\\nI have also imposed new sanctions on entities and individuals who support Iran\\'s ballistic missile program, and reaffirmed our unbreakable alliance with the State of Israel.\\nFinally, I have kept my promise to appoint a Justice to the United States Supreme Court -- from my list of 20 judges -- who will defend our Constitution. I am honored to have Maureen Scalia with us in the gallery tonight. Her late, great husband, Antonin Scalia, will forever be a symbol of American justice. To fill his seat, we have chosen Judge Neil Gorsuch, a man of incredible skill, and deep devotion to the law. He was confirmed unanimously to the Court of Appeals, and I am asking the Senate to swiftly approve his nomination.\\nTonight, as I outline the next steps we must take as a country, we must honestly acknowledge the circumstances we inherited.\\nNinety-four million Americans are out of the labor force.\\nOver 43 million people are now living in poverty, and over 43 million Americans are on food stamps.\\nMore than 1 in 5 people in their prime working years are not working.\\nWe have the worst financial recovery in 65 years.\\nIn the last 8 years, the past Administration has put on more new debt than nearly all other Presidents combined.\\nWe\\'ve lost more than one-fourth of our manufacturing jobs since NAFTA was approved, and we\\'ve lost 60,000 factories since China joined the World Trade Organization in 2001.\\nOur trade deficit in goods with the world last year was nearly $800 billion dollars.\\nAnd overseas, we have inherited a series of tragic foreign policy disasters.\\nSolving these, and so many other pressing problems, will require us to work past the differences of party. It will require us to tap into the American spirit that has overcome every challenge throughout our long and storied history.\\nBut to accomplish our goals at home and abroad, we must restart the engine of the American economy -- making it easier for companies to do business in the United States, and much harder for companies to leave.\\nRight now, American companies are taxed at one of the highest rates anywhere in the world.\\nMy economic team is developing historic tax reform that will reduce the tax rate on our companies so they can compete and thrive anywhere and with anyone. At the same time, we will provide massive tax relief for the middle class.\\nWe must create a level playing field for American companies and workers.\\nCurrently, when we ship products out of America, many other countries make us pay very high tariffs and taxes -- but when foreign companies ship their products into America, we charge them almost nothing.\\nI just met with officials and workers from a great American company, Harley-Davidson. In fact, they proudly displayed five of their magnificent motorcycles, made in the USA, on the front lawn of the White House.\\nAt our meeting, I asked them, how are you doing, how is business? They said that it\\'s good. I asked them further how they are doing with other countries, mainly international sales. They told me -- without even complaining because they have been mistreated for so long that they have become used to it -- that it is very hard to do business with other countries because they tax our goods at such a high rate. They said that in one case another country taxed their motorcycles at 100 percent.\\nThey weren\\'t even asking for change. But I am.\\nI believe strongly in free trade but it also has to be FAIR TRADE.\\nThe first Republican President, Abraham Lincoln, warned that the \"abandonment of the protective policy by the American Government [will] produce want and ruin among our people.\"\\nLincoln was right -- and it is time we heeded his words. I am not going to let America and its great companies and workers, be taken advantage of anymore.\\nI am going to bring back millions of jobs. Protecting our workers also means reforming our system of legal immigration. The current, outdated system depresses wages for our poorest workers, and puts great pressure on taxpayers.\\nNations around the world, like Canada, Australia and many others --- have a merit-based immigration system. It is a basic principle that those seeking to enter a country ought to be able to support themselves financially. Yet, in America, we do not enforce this rule, straining the very public resources that our poorest citizens rely upon. According to the National Academy of Sciences, our current immigration system costs America\\'s taxpayers many billions of dollars a year.\\nSwitching away from this current system of lower-skilled immigration, and instead adopting a merit-based system, will have many benefits: it will save countless dollars, raise workers\\' wages, and help struggling families --- including immigrant families --- enter the middle class.\\nI believe that real and positive immigration reform is possible, as long as we focus on the following goals: to improve jobs and wages for Americans, to strengthen our nation\\'s security, and to restore respect for our laws.\\nIf we are guided by the well-being of American citizens then I believe Republicans and Democrats can work together to achieve an outcome that has eluded our country for decades.\\nAnother Republican President, Dwight D. Eisenhower, initiated the last truly great national infrastructure program --- the building of the interstate highway system. The time has come for a new program of national rebuilding.\\nAmerica has spent approximately six trillion dollars in the Middle East, all this while our infrastructure at home is crumbling. With this six trillion dollars we could have rebuilt our country --- twice. And maybe even three times if we had people who had the ability to negotiate.\\nTo launch our national rebuilding, I will be asking the Congress to approve legislation that produces a $1 trillion investment in the infrastructure of the United States -- financed through both public and private capital --- creating millions of new jobs.\\nThis effort will be guided by two core principles: Buy American, and Hire American.\\nTonight, I am also calling on this Congress to repeal and replace Obamacare with reforms that expand choice, increase access, lower costs, and at the same time, provide better Healthcare.\\nMandating every American to buy government-approved health insurance was never the right solution for America. The way to make health insurance available to everyone is to lower the cost of health insurance, and that is what we will do.\\nObamacare premiums nationwide have increased by double and triple digits. As an example, Arizona went up 116 percent last year alone. Governor Matt Bevin of Kentucky just said Obamacare is failing in his State -- it is unsustainable and collapsing.\\nOne third of counties have only one insurer on the exchanges --- leaving many Americans with no choice at all.\\nRemember when you were told that you could keep your doctor, and keep your plan?\\nWe now know that all of those promises have been broken.\\nObamacare is collapsing --- and we must act decisively to protect all Americans. Action is not a choice --- it is a necessity.\\nSo I am calling on all Democrats and Republicans in the Congress to work with us to save Americans from this imploding Obamacare disaster.\\nHere are the principles that should guide the Congress as we move to create a better healthcare system for all Americans:\\nFirst, we should ensure that Americans with pre-existing conditions have access to coverage, and that we have a stable transition for Americans currently enrolled in the healthcare exchanges.\\nSecondly, we should help Americans purchase their own coverage, through the use of tax credits and expanded Health Savings Accounts --- but it must be the plan they want, not the plan forced on them by the Government.\\nThirdly, we should give our great State Governors the resources and flexibility they need with Medicaid to make sure no one is left out.\\nFourthly, we should implement legal reforms that protect patients and doctors from unnecessary costs that drive up the price of insurance -- and work to bring down the artificially high price of drugs and bring them down immediately.\\nFinally, the time has come to give Americans the freedom to purchase health insurance across State lines --- creating a truly competitive national marketplace that will bring cost way down and provide far better care.\\nEverything that is broken in our country can be fixed. Every problem can be solved. And every hurting family can find healing, and hope.\\nOur citizens deserve this, and so much more --- so why not join forces to finally get it done? On this and so many other things, Democrats and Republicans should get together and unite for the good of our country, and for the good of the American people.\\nMy administration wants to work with members in both parties to make childcare accessible and affordable, to help ensure new parents have paid family leave, to invest in women\\'s health, and to promote clean air and clear water, and to rebuild our military and our infrastructure.\\nTrue love for our people requires us to find common ground, to advance the common good, and to cooperate on behalf of every American child who deserves a brighter future.\\nAn incredible young woman is with us this evening who should serve as an inspiration to us all.\\nToday is Rare Disease day, and joining us in the gallery is a Rare Disease Survivor, Megan Crowley. Megan was diagnosed with Pompe Disease, a rare and serious illness, when she was 15 months old. She was not expected to live past 5.\\nOn receiving this news, Megan\\'s dad, John, fought with everything he had to save the life of his precious child. He founded a company to look for a cure, and helped develop the drug that saved Megan\\'s life. Today she is 20 years old -- and a sophomore at Notre Dame.\\nMegan\\'s story is about the unbounded power of a father\\'s love for a daughter.\\nBut our slow and burdensome approval process at the Food and Drug Administration keeps too many advances, like the one that saved Megan\\'s life, from reaching those in need.\\nIf we slash the restraints, not just at the FDA but across our Government, then we will be blessed with far more miracles like Megan.\\nIn fact, our children will grow up in a Nation of miracles.\\nBut to achieve this future, we must enrich the mind --- and the souls --- of every American child.\\nEducation is the civil rights issue of our time.\\nI am calling upon Members of both parties to pass an education bill that funds school choice for disadvantaged youth, including millions of African-American and Latino children. These families should be free to choose the public, private, charter, magnet, religious or home school that is right for them.\\nJoining us tonight in the gallery is a remarkable woman, Denisha Merriweather. As a young girl, Denisha struggled in school and failed third grade twice. But then she was able to enroll in a private center for learning, with the help of a tax credit scholarship program. Today, she is the first in her family to graduate, not just from high school, but from college. Later this year she will get her masters degree in social work.\\nWe want all children to be able to break the cycle of poverty just like Denisha.\\nBut to break the cycle of poverty, we must also break the cycle of violence.\\nThe murder rate in 2015 experienced its largest single-year increase in nearly half a century.\\nIn Chicago, more than 4,000 people were shot last year alone --- and the murder rate so far this year has been even higher.\\nThis is not acceptable in our society.\\nEvery American child should be able to grow up in a safe community, to attend a great school, and to have access to a high-paying job.\\nBut to create this future, we must work with --- not against --- the men and women of law enforcement.\\nWe must build bridges of cooperation and trust --- not drive the wedge of disunity and division.\\nPolice and sheriffs are members of our community. They are friends and neighbors, they are mothers and fathers, sons and daughters -- and they leave behind loved ones every day who worry whether or not they\\'ll come home safe and sound.\\nWe must support the incredible men and women of law enforcement.\\nAnd we must support the victims of crime.\\nI have ordered the Department of Homeland Security to create an office to serve American Victims. The office is called VOICE --- Victims Of Immigration Crime Engagement. We are providing a voice to those who have been ignored by our media, and silenced by special interests.\\nJoining us in the audience tonight are four very brave Americans whose government failed them.\\nTheir names are Jamiel Shaw, Susan Oliver, Jenna Oliver, and Jessica Davis.\\nJamiel\\'s 17-year-old son was viciously murdered by an illegal immigrant gang member, who had just been released from prison. Jamiel Shaw Jr. was an incredible young man, with unlimited potential who was getting ready to go to college where he would have excelled as a great quarterback. But he never got the chance. His father, who is in the audience tonight, has become a good friend of mine.\\nAlso with us are Susan Oliver and Jessica Davis. Their husbands --- Deputy Sheriff Danny Oliver and Detective Michael Davis --- were slain in the line of duty in California. They were pillars of their community. These brave men were viciously gunned down by an illegal immigrant with a criminal record and two prior deportations.\\nSitting with Susan is her daughter, Jenna. Jenna: I want you to know that your father was a hero, and that tonight you have the love of an entire country supporting you and praying for you.\\nTo Jamiel, Jenna, Susan and Jessica: I want you to know --- we will never stop fighting for justice. Your loved ones will never be forgotten, we will always honor their memory.\\nFinally, to keep America Safe we must provide the men and women of the United States military with the tools they need to prevent war and --- if they must --- to fight and to win.\\nI am sending the Congress a budget that rebuilds the military, eliminates the Defense sequester, and calls for one of the largest increases in national defense spending in American history.\\nMy budget will also increase funding for our veterans.\\nOur veterans have delivered for this Nation --- and now we must deliver for them.\\nThe challenges we face as a Nation are great. But our people are even greater.\\nAnd none are greater or braver than those who fight for America in uniform.\\nWe are blessed to be joined tonight by Carryn Owens, the widow of a U.S. Navy Special Operator, Senior Chief William \"Ryan\" Owens. Ryan died as he lived: a warrior, and a hero --- battling against terrorism and securing our Nation.\\nI just spoke to General Mattis, who reconfirmed that, and I quote, \"Ryan was a part of a highly successful raid that generated large amounts of vital intelligence that will lead to many more victories in the future against our enemies.\" Ryan\\'s legacy is etched into eternity. For as the Bible teaches us, there is no greater act of love than to lay down one\\'s life for one\\'s friends. Ryan laid down his life for his friends, for his country, and for our freedom --- we will never forget him.\\nTo those allies who wonder what kind of friend America will be, look no further than the heroes who wear our uniform.\\nOur foreign policy calls for a direct, robust and meaningful engagement with the world. It is American leadership based on vital security interests that we share with our allies across the globe.\\nWe strongly support NATO, an alliance forged through the bonds of two World Wars that dethroned fascism, and a Cold War that defeated communism.\\nBut our partners must meet their financial obligations.\\nAnd now, based on our very strong and frank discussions, they are beginning to do just that.\\nWe expect our partners, whether in NATO, in the Middle East, or the Pacific --- to take a direct and meaningful role in both strategic and military operations, and pay their fair share of the cost.\\nWe will respect historic institutions, but we will also respect the sovereign rights of nations.\\nFree nations are the best vehicle for expressing the will of the people --- and America respects the right of all nations to chart their own path. My job is not to represent the world. My job is to represent the United States of America. But we know that America is better off, when there is less conflict -- not more.\\nWe must learn from the mistakes of the past --- we have seen the war and destruction that have raged across our world.\\nThe only long-term solution for these humanitarian disasters is to create the conditions where displaced persons can safely return home and begin the long process of rebuilding.\\nAmerica is willing to find new friends, and to forge new partnerships, where shared interests align. We want harmony and stability, not war and conflict.\\nWe want peace, wherever peace can be found. America is friends today with former enemies. Some of our closest allies, decades ago, fought on the opposite side of these World Wars. This history should give us all faith in the possibilities for a better world.\\nHopefully, the 250th year for America will see a world that is more peaceful, more just and more free.\\nOn our 100th anniversary, in 1876, citizens from across our Nation came to Philadelphia to celebrate America\\'s centennial. At that celebration, the country\\'s builders and artists and inventors showed off their creations.\\nAlexander Graham Bell displayed his telephone for the first time.\\nRemington unveiled the first typewriter. An early attempt was made at electric light.\\nThomas Edison showed an automatic telegraph and an electric pen.\\nImagine the wonders our country could know in America\\'s 250th year.\\nThink of the marvels we can achieve if we simply set free the dreams of our people.\\nCures to illnesses that have always plagued us are not too much to hope.\\nAmerican footprints on distant worlds are not too big a dream.\\nMillions lifted from welfare to work is not too much to expect.\\nAnd streets where mothers are safe from fear -- schools where children learn in peace -- and jobs where Americans prosper and grow -- are not too much to ask.\\nWhen we have all of this, we will have made America greater than ever before. For all Americans.\\nThis is our vision. This is our mission.\\nBut we can only get there together.\\nWe are one people, with one destiny.\\nWe all bleed the same blood.\\nWe all salute the same flag.\\nAnd we are all made by the same God.\\nAnd when we fulfill this vision; when we celebrate our 250 years of glorious freedom, we will look back on tonight as when this new chapter of American Greatness began.\\nThe time for small thinking is over. The time for trivial fights is behind us.\\nWe just need the courage to share the dreams that fill our hearts.\\nThe bravery to express the hopes that stir our souls.\\nAnd the confidence to turn those hopes and dreams to action.\\nFrom now on, America will be empowered by our aspirations, not burdened by our fears ---\\ninspired by the future, not bound by the failures of the past ---\\nand guided by our vision, not blinded by our doubts.\\nI am asking all citizens to embrace this Renewal of the American Spirit. I am asking all members of Congress to join me in dreaming big, and bold and daring things for our country. And I am asking everyone watching tonight to seize this moment and --\\nBelieve in yourselves.\\nBelieve in your future.\\nAnd believe, once more, in America.\\nThank you, God bless you, and God Bless these United States.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr. Speaker, Mr. Vice President, Members of Congress, the First Lady of the United States, and Citizens of America:\\nTonight, as we mark the conclusion of our celebration of Black History Month, we are reminded of our Nation\\'s path toward civil rights and the work that still remains. Recent threats targeting Jewish Community Centers and vandalism of Jewish cemeteries, as well as last week\\'s shooting in Kansas City, remind us that while we may be a Nation divided on policies, we are a country that stands united in condemning hate and evil in all its forms.\\nEach American generation passes the torch of truth, liberty and justice --- in an unbroken chain all the way down to the present.\\nThat torch is now in our hands. And we will use it to light up the world. I am here tonight to deliver a message of unity and strength, and it is a message deeply delivered from my heart.\\nA new chapter of American Greatness is now beginning.\\nA new national pride is sweeping across our Nation.\\nAnd a new surge of optimism is placing impossible dreams firmly within our grasp.\\nWhat we are witnessing today is the Renewal of the American Spirit.\\nOur allies will find that America is once again ready to lead.\\nAll the nations of the world -- friend or foe -- will find that America is strong, America is proud, and America is free.\\nIn 9 years, the United States will celebrate the 250th anniversary of our founding -- 250 years since the day we declared our Independence.\\nIt will be one of the great milestones in the history of the world.\\nBut what will America look like as we reach our 250th year? What kind of country will we leave for our children?\\nI will not allow the mistakes of recent decades past to define the course of our future.\\nFor too long, we\\'ve watched our middle class shrink as we\\'ve exported our jobs and wealth to foreign countries.\\nWe\\'ve financed and built one global project after another, but ignored the fates of our children in the inner cities of Chicago, Baltimore, Detroit -- and so many other places throughout our land.\\nWe\\'ve defended the borders of other nations, while leaving our own borders wide open, for anyone to cross -- and for drugs to pour in at a now unprecedented rate.\\nAnd we\\'ve spent trillions of dollars overseas, while our infrastructure at home has so badly crumbled.\\nThen, in 2016, the earth shifted beneath our feet. The rebellion started as a quiet protest, spoken by families of all colors and creeds --- families who just wanted a fair shot for their children, and a fair hearing for their concerns.\\nBut then the quiet voices became a loud chorus -- as thousands of citizens now spoke out together, from cities small and large, all across our country.\\nFinally, the chorus became an earthquake -- and the people turned out by the tens of millions, and they were all united by one very simple, but crucial demand, that America must put its own citizens first ... because only then, can we truly MAKE AMERICA GREAT AGAIN.\\nDying industries will come roaring back to life. Heroic veterans will get the care they so desperately need.\\nOur military will be given the resources its brave warriors so richly deserve.\\nCrumbling infrastructure will be replaced with new roads, bridges, tunnels, airports and railways gleaming across our beautiful land.\\nOur terrible drug epidemic will slow down and ultimately, stop.\\nAnd our neglected inner cities will see a rebirth of hope, safety, and opportunity.\\nAbove all else, we will keep our promises to the American people.\\nIt\\'s been a little over a month since my inauguration, and I want to take this moment to update the Nation on the progress I\\'ve made in keeping those promises.\\nSince my election, Ford, Fiat-Chrysler, General Motors, Sprint, Softbank, Lockheed, Intel, Walmart, and many others, have announced that they will invest billions of dollars in the United States and will create tens of thousands of new American jobs.\\nThe stock market has gained almost three trillion dollars in value since the election on November 8th, a record. We\\'ve saved taxpayers hundreds of millions of dollars by bringing down the price of the fantastic new F-35 jet fighter, and will be saving billions more dollars on contracts all across our Government. We have placed a hiring freeze on non-military and non-essential Federal workers.\\nWe have begun to drain the swamp of government corruption by imposing a 5 year ban on lobbying by executive branch officials --- and a lifetime ban on becoming lobbyists for a foreign government.\\nWe have undertaken a historic effort to massively reduce jobâ€‘crushing regulations, creating a deregulation task force inside of every Government agency; imposing a new rule which mandates that for every 1 new regulation, 2 old regulations must be eliminated; and stopping a regulation that threatens the future and livelihoods of our great coal miners.\\nWe have cleared the way for the construction of the Keystone and Dakota Access Pipelines -- thereby creating tens of thousands of jobs -- and I\\'ve issued a new directive that new American pipelines be made with American steel.\\nWe have withdrawn the United States from the job-killing Trans-Pacific Partnership.\\nWith the help of Prime Minister Justin Trudeau, we have formed a Council with our neighbors in Canada to help ensure that women entrepreneurs have access to the networks, markets and capital they need to start a business and live out their financial dreams.\\nTo protect our citizens, I have directed the Department of Justice to form a Task Force on Reducing Violent Crime.\\nI have further ordered the Departments of Homeland Security and Justice, along with the Department of State and the Director of National Intelligence, to coordinate an aggressive strategy to dismantle the criminal cartels that have spread across our Nation.\\nWe will stop the drugs from pouring into our country and poisoning our youth -- and we will expand treatment for those who have become so badly addicted.\\nAt the same time, my Administration has answered the pleas of the American people for immigration enforcement and border security. By finally enforcing our immigration laws, we will raise wages, help the unemployed, save billions of dollars, and make our communities safer for everyone. We want all Americans to succeed --- but that can\\'t happen in an environment of lawless chaos. We must restore integrity and the rule of law to our borders.\\nFor that reason, we will soon begin the construction of a great wall along our southern border. It will be started ahead of schedule and, when finished, it will be a very effective weapon against drugs and crime.\\nAs we speak, we are removing gang members, drug dealers and criminals that threaten our communities and prey on our citizens. Bad ones are going out as I speak tonight and as I have promised.\\nTo any in Congress who do not believe we should enforce our laws, I would ask you this question: what would you say to the American family that loses their jobs, their income, or a loved one, because America refused to uphold its laws and defend its borders?\\nOur obligation is to serve, protect, and defend the citizens of the United States. We are also taking strong measures to protect our Nation from Radical Islamic Terrorism.\\nAccording to data provided by the Department of Justice, the vast majority of individuals convicted for terrorism-related offenses since 9/11 came here from outside of our country. We have seen the attacks at home --- from Boston to San Bernardino to the Pentagon and yes, even the World Trade Center.\\nWe have seen the attacks in France, in Belgium, in Germany and all over the world.\\nIt is not compassionate, but reckless, to allow uncontrolled entry from places where proper vetting cannot occur. Those given the high honor of admission to the United States should support this country and love its people and its values.\\nWe cannot allow a beachhead of terrorism to form inside America -- we cannot allow our Nation to become a sanctuary for extremists.\\nThat is why my Administration has been working on improved vetting procedures, and we will shortly take new steps to keep our Nation safe -- and to keep out those who would do us harm.\\nAs promised, I directed the Department of Defense to develop a plan to demolish and destroy ISIS -- a network of lawless savages that have slaughtered Muslims and Christians, and men, women, and children of all faiths and beliefs. We will work with our allies, including our friends and allies in the Muslim world, to extinguish this vile enemy from our planet.\\nI have also imposed new sanctions on entities and individuals who support Iran\\'s ballistic missile program, and reaffirmed our unbreakable alliance with the State of Israel.\\nFinally, I have kept my promise to appoint a Justice to the United States Supreme Court -- from my list of 20 judges -- who will defend our Constitution. I am honored to have Maureen Scalia with us in the gallery tonight. Her late, great husband, Antonin Scalia, will forever be a symbol of American justice. To fill his seat, we have chosen Judge Neil Gorsuch, a man of incredible skill, and deep devotion to the law. He was confirmed unanimously to the Court of Appeals, and I am asking the Senate to swiftly approve his nomination.\\nTonight, as I outline the next steps we must take as a country, we must honestly acknowledge the circumstances we inherited.\\nNinety-four million Americans are out of the labor force.\\nOver 43 million people are now living in poverty, and over 43 million Americans are on food stamps.\\nMore than 1 in 5 people in their prime working years are not working.\\nWe have the worst financial recovery in 65 years.\\nIn the last 8 years, the past Administration has put on more new debt than nearly all other Presidents combined.\\nWe\\'ve lost more than one-fourth of our manufacturing jobs since NAFTA was approved, and we\\'ve lost 60,000 factories since China joined the World Trade Organization in 2001.\\nOur trade deficit in goods with the world last year was nearly $800 billion dollars.\\nAnd overseas, we have inherited a series of tragic foreign policy disasters.\\nSolving these, and so many other pressing problems, will require us to work past the differences of party. It will require us to tap into the American spirit that has overcome every challenge throughout our long and storied history.\\nBut to accomplish our goals at home and abroad, we must restart the engine of the American economy -- making it easier for companies to do business in the United States, and much harder for companies to leave.\\nRight now, American companies are taxed at one of the highest rates anywhere in the world.\\nMy economic team is developing historic tax reform that will reduce the tax rate on our companies so they can compete and thrive anywhere and with anyone. At the same time, we will provide massive tax relief for the middle class.\\nWe must create a level playing field for American companies and workers.\\nCurrently, when we ship products out of America, many other countries make us pay very high tariffs and taxes -- but when foreign companies ship their products into America, we charge them almost nothing.\\nI just met with officials and workers from a great American company, Harley-Davidson. In fact, they proudly displayed five of their magnificent motorcycles, made in the USA, on the front lawn of the White House.\\nAt our meeting, I asked them, how are you doing, how is business? They said that it\\'s good. I asked them further how they are doing with other countries, mainly international sales. They told me -- without even complaining because they have been mistreated for so long that they have become used to it -- that it is very hard to do business with other countries because they tax our goods at such a high rate. They said that in one case another country taxed their motorcycles at 100 percent.\\nThey weren\\'t even asking for change. But I am.\\nI believe strongly in free trade but it also has to be FAIR TRADE.\\nThe first Republican President, Abraham Lincoln, warned that the \"abandonment of the protective policy by the American Government [will] produce want and ruin among our people.\"\\nLincoln was right -- and it is time we heeded his words. I am not going to let America and its great companies and workers, be taken advantage of anymore.\\nI am going to bring back millions of jobs. Protecting our workers also means reforming our system of legal immigration. The current, outdated system depresses wages for our poorest workers, and puts great pressure on taxpayers.\\nNations around the world, like Canada, Australia and many others --- have a merit-based immigration system. It is a basic principle that those seeking to enter a country ought to be able to support themselves financially. Yet, in America, we do not enforce this rule, straining the very public resources that our poorest citizens rely upon. According to the National Academy of Sciences, our current immigration system costs America\\'s taxpayers many billions of dollars a year.\\nSwitching away from this current system of lower-skilled immigration, and instead adopting a merit-based system, will have many benefits: it will save countless dollars, raise workers\\' wages, and help struggling families --- including immigrant families --- enter the middle class.\\nI believe that real and positive immigration reform is possible, as long as we focus on the following goals: to improve jobs and wages for Americans, to strengthen our nation\\'s security, and to restore respect for our laws.\\nIf we are guided by the well-being of American citizens then I believe Republicans and Democrats can work together to achieve an outcome that has eluded our country for decades.\\nAnother Republican President, Dwight D. Eisenhower, initiated the last truly great national infrastructure program --- the building of the interstate highway system. The time has come for a new program of national rebuilding.\\nAmerica has spent approximately six trillion dollars in the Middle East, all this while our infrastructure at home is crumbling. With this six trillion dollars we could have rebuilt our country --- twice. And maybe even three times if we had people who had the ability to negotiate.\\nTo launch our national rebuilding, I will be asking the Congress to approve legislation that produces a $1 trillion investment in the infrastructure of the United States -- financed through both public and private capital --- creating millions of new jobs.\\nThis effort will be guided by two core principles: Buy American, and Hire American.\\nTonight, I am also calling on this Congress to repeal and replace Obamacare with reforms that expand choice, increase access, lower costs, and at the same time, provide better Healthcare.\\nMandating every American to buy government-approved health insurance was never the right solution for America. The way to make health insurance available to everyone is to lower the cost of health insurance, and that is what we will do.\\nObamacare premiums nationwide have increased by double and triple digits. As an example, Arizona went up 116 percent last year alone. Governor Matt Bevin of Kentucky just said Obamacare is failing in his State -- it is unsustainable and collapsing.\\nOne third of counties have only one insurer on the exchanges --- leaving many Americans with no choice at all.\\nRemember when you were told that you could keep your doctor, and keep your plan?\\nWe now know that all of those promises have been broken.\\nObamacare is collapsing --- and we must act decisively to protect all Americans. Action is not a choice --- it is a necessity.\\nSo I am calling on all Democrats and Republicans in the Congress to work with us to save Americans from this imploding Obamacare disaster.\\nHere are the principles that should guide the Congress as we move to create a better healthcare system for all Americans:\\nFirst, we should ensure that Americans with pre-existing conditions have access to coverage, and that we have a stable transition for Americans currently enrolled in the healthcare exchanges.\\nSecondly, we should help Americans purchase their own coverage, through the use of tax credits and expanded Health Savings Accounts --- but it must be the plan they want, not the plan forced on them by the Government.\\nThirdly, we should give our great State Governors the resources and flexibility they need with Medicaid to make sure no one is left out.\\nFourthly, we should implement legal reforms that protect patients and doctors from unnecessary costs that drive up the price of insurance -- and work to bring down the artificially high price of drugs and bring them down immediately.\\nFinally, the time has come to give Americans the freedom to purchase health insurance across State lines --- creating a truly competitive national marketplace that will bring cost way down and provide far better care.\\nEverything that is broken in our country can be fixed. Every problem can be solved. And every hurting family can find healing, and hope.\\nOur citizens deserve this, and so much more --- so why not join forces to finally get it done? On this and so many other things, Democrats and Republicans should get together and unite for the good of our country, and for the good of the American people.\\nMy administration wants to work with members in both parties to make childcare accessible and affordable, to help ensure new parents have paid family leave, to invest in women\\'s health, and to promote clean air and clear water, and to rebuild our military and our infrastructure.\\nTrue love for our people requires us to find common ground, to advance the common good, and to cooperate on behalf of every American child who deserves a brighter future.\\nAn incredible young woman is with us this evening who should serve as an inspiration to us all.\\nToday is Rare Disease day, and joining us in the gallery is a Rare Disease Survivor, Megan Crowley. Megan was diagnosed with Pompe Disease, a rare and serious illness, when she was 15 months old. She was not expected to live past 5.\\nOn receiving this news, Megan\\'s dad, John, fought with everything he had to save the life of his precious child. He founded a company to look for a cure, and helped develop the drug that saved Megan\\'s life. Today she is 20 years old -- and a sophomore at Notre Dame.\\nMegan\\'s story is about the unbounded power of a father\\'s love for a daughter.\\nBut our slow and burdensome approval process at the Food and Drug Administration keeps too many advances, like the one that saved Megan\\'s life, from reaching those in need.\\nIf we slash the restraints, not just at the FDA but across our Government, then we will be blessed with far more miracles like Megan.\\nIn fact, our children will grow up in a Nation of miracles.\\nBut to achieve this future, we must enrich the mind --- and the souls --- of every American child.\\nEducation is the civil rights issue of our time.\\nI am calling upon Members of both parties to pass an education bill that funds school choice for disadvantaged youth, including millions of African-American and Latino children. These families should be free to choose the public, private, charter, magnet, religious or home school that is right for them.\\nJoining us tonight in the gallery is a remarkable woman, Denisha Merriweather. As a young girl, Denisha struggled in school and failed third grade twice. But then she was able to enroll in a private center for learning, with the help of a tax credit scholarship program. Today, she is the first in her family to graduate, not just from high school, but from college. Later this year she will get her masters degree in social work.\\nWe want all children to be able to break the cycle of poverty just like Denisha.\\nBut to break the cycle of poverty, we must also break the cycle of violence.\\nThe murder rate in 2015 experienced its largest single-year increase in nearly half a century.\\nIn Chicago, more than 4,000 people were shot last year alone --- and the murder rate so far this year has been even higher.\\nThis is not acceptable in our society.\\nEvery American child should be able to grow up in a safe community, to attend a great school, and to have access to a high-paying job.\\nBut to create this future, we must work with --- not against --- the men and women of law enforcement.\\nWe must build bridges of cooperation and trust --- not drive the wedge of disunity and division.\\nPolice and sheriffs are members of our community. They are friends and neighbors, they are mothers and fathers, sons and daughters -- and they leave behind loved ones every day who worry whether or not they\\'ll come home safe and sound.\\nWe must support the incredible men and women of law enforcement.\\nAnd we must support the victims of crime.\\nI have ordered the Department of Homeland Security to create an office to serve American Victims. The office is called VOICE --- Victims Of Immigration Crime Engagement. We are providing a voice to those who have been ignored by our media, and silenced by special interests.\\nJoining us in the audience tonight are four very brave Americans whose government failed them.\\nTheir names are Jamiel Shaw, Susan Oliver, Jenna Oliver, and Jessica Davis.\\nJamiel\\'s 17-year-old son was viciously murdered by an illegal immigrant gang member, who had just been released from prison. Jamiel Shaw Jr. was an incredible young man, with unlimited potential who was getting ready to go to college where he would have excelled as a great quarterback. But he never got the chance. His father, who is in the audience tonight, has become a good friend of mine.\\nAlso with us are Susan Oliver and Jessica Davis. Their husbands --- Deputy Sheriff Danny Oliver and Detective Michael Davis --- were slain in the line of duty in California. They were pillars of their community. These brave men were viciously gunned down by an illegal immigrant with a criminal record and two prior deportations.\\nSitting with Susan is her daughter, Jenna. Jenna: I want you to know that your father was a hero, and that tonight you have the love of an entire country supporting you and praying for you.\\nTo Jamiel, Jenna, Susan and Jessica: I want you to know --- we will never stop fighting for justice. Your loved ones will never be forgotten, we will always honor their memory.\\nFinally, to keep America Safe we must provide the men and women of the United States military with the tools they need to prevent war and --- if they must --- to fight and to win.\\nI am sending the Congress a budget that rebuilds the military, eliminates the Defense sequester, and calls for one of the largest increases in national defense spending in American history.\\nMy budget will also increase funding for our veterans.\\nOur veterans have delivered for this Nation --- and now we must deliver for them.\\nThe challenges we face as a Nation are great. But our people are even greater.\\nAnd none are greater or braver than those who fight for America in uniform.\\nWe are blessed to be joined tonight by Carryn Owens, the widow of a U.S. Navy Special Operator, Senior Chief William \"Ryan\" Owens. Ryan died as he lived: a warrior, and a hero --- battling against terrorism and securing our Nation.\\nI just spoke to General Mattis, who reconfirmed that, and I quote, \"Ryan was a part of a highly successful raid that generated large amounts of vital intelligence that will lead to many more victories in the future against our enemies.\" Ryan\\'s legacy is etched into eternity. For as the Bible teaches us, there is no greater act of love than to lay down one\\'s life for one\\'s friends. Ryan laid down his life for his friends, for his country, and for our freedom --- we will never forget him.\\nTo those allies who wonder what kind of friend America will be, look no further than the heroes who wear our uniform.\\nOur foreign policy calls for a direct, robust and meaningful engagement with the world. It is American leadership based on vital security interests that we share with our allies across the globe.\\nWe strongly support NATO, an alliance forged through the bonds of two World Wars that dethroned fascism, and a Cold War that defeated communism.\\nBut our partners must meet their financial obligations.\\nAnd now, based on our very strong and frank discussions, they are beginning to do just that.\\nWe expect our partners, whether in NATO, in the Middle East, or the Pacific --- to take a direct and meaningful role in both strategic and military operations, and pay their fair share of the cost.\\nWe will respect historic institutions, but we will also respect the sovereign rights of nations.\\nFree nations are the best vehicle for expressing the will of the people --- and America respects the right of all nations to chart their own path. My job is not to represent the world. My job is to represent the United States of America. But we know that America is better off, when there is less conflict -- not more.\\nWe must learn from the mistakes of the past --- we have seen the war and destruction that have raged across our world.\\nThe only long-term solution for these humanitarian disasters is to create the conditions where displaced persons can safely return home and begin the long process of rebuilding.\\nAmerica is willing to find new friends, and to forge new partnerships, where shared interests align. We want harmony and stability, not war and conflict.\\nWe want peace, wherever peace can be found. America is friends today with former enemies. Some of our closest allies, decades ago, fought on the opposite side of these World Wars. This history should give us all faith in the possibilities for a better world.\\nHopefully, the 250th year for America will see a world that is more peaceful, more just and more free.\\nOn our 100th anniversary, in 1876, citizens from across our Nation came to Philadelphia to celebrate America\\'s centennial. At that celebration, the country\\'s builders and artists and inventors showed off their creations.\\nAlexander Graham Bell displayed his telephone for the first time.\\nRemington unveiled the first typewriter. An early attempt was made at electric light.\\nThomas Edison showed an automatic telegraph and an electric pen.\\nImagine the wonders our country could know in America\\'s 250th year.\\nThink of the marvels we can achieve if we simply set free the dreams of our people.\\nCures to illnesses that have always plagued us are not too much to hope.\\nAmerican footprints on distant worlds are not too big a dream.\\nMillions lifted from welfare to work is not too much to expect.\\nAnd streets where mothers are safe from fear -- schools where children learn in peace -- and jobs where Americans prosper and grow -- are not too much to ask.\\nWhen we have all of this, we will have made America greater than ever before. For all Americans.\\nThis is our vision. This is our mission.\\nBut we can only get there together.\\nWe are one people, with one destiny.\\nWe all bleed the same blood.\\nWe all salute the same flag.\\nAnd we are all made by the same God.\\nAnd when we fulfill this vision; when we celebrate our 250 years of glorious freedom, we will look back on tonight as when this new chapter of American Greatness began.\\nThe time for small thinking is over. The time for trivial fights is behind us.\\nWe just need the courage to share the dreams that fill our hearts.\\nThe bravery to express the hopes that stir our souls.\\nAnd the confidence to turn those hopes and dreams to action.\\nFrom now on, America will be empowered by our aspirations, not burdened by our fears ---\\ninspired by the future, not bound by the failures of the past ---\\nand guided by our vision, not blinded by our doubts.\\nI am asking all citizens to embrace this Renewal of the American Spirit. I am asking all members of Congress to join me in dreaming big, and bold and daring things for our country. And I am asking everyone watching tonight to seize this moment and --\\nBelieve in yourselves.\\nBelieve in your future.\\nAnd believe, once more, in America.\\nThank you, God bless you, and God Bless these United States.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, 4, 7,8,9 exclude\n",
    "fileS=['Trump Black History Month Speech.txt', 'Trump Congressional Address.txt', 'Trump CPAC Speech.txt',  'Trump Immigration Speech 8-31-16.txt',\n",
    " 'Trump Inauguration Speech.txt',  'Trump Response to Healthcare Bill Failure.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=[]\n",
    "for i in range(len(tn)):\n",
    "    for j in range(len(corpus)):\n",
    "        if tn[i]==corpus[j]:\n",
    "            index.append(j)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump Black History Month Speech.txt\n",
      "Trump Congressional Address.txt\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(index)):\n",
    "    print(fileS[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly try for BM25L, BM25Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Gensim package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from gensim) (1.22.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package gensim:\n",
      "\n",
      "NAME\n",
      "    gensim\n",
      "\n",
      "DESCRIPTION\n",
      "    This package contains functionality to transform documents (strings) into vectors, and calculate\n",
      "    similarities between documents.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _matutils\n",
      "    corpora (package)\n",
      "    downloader\n",
      "    interfaces\n",
      "    matutils\n",
      "    models (package)\n",
      "    nosy\n",
      "    parsing (package)\n",
      "    scripts (package)\n",
      "    similarities (package)\n",
      "    test (package)\n",
      "    topic_coherence (package)\n",
      "    utils\n",
      "\n",
      "DATA\n",
      "    logger = <Logger gensim (WARNING)>\n",
      "\n",
      "VERSION\n",
      "    4.2.0\n",
      "\n",
      "FILE\n",
      "    c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages\\gensim\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module gensim.similarities.termsim in gensim.similarities:\n",
      "\n",
      "NAME\n",
      "    gensim.similarities.termsim - This module provides classes that deal with term similarities.\n",
      "\n",
      "CLASSES\n",
      "    gensim.utils.SaveLoad(builtins.object)\n",
      "        SparseTermSimilarityMatrix\n",
      "        TermSimilarityIndex\n",
      "            UniformTermSimilarityIndex\n",
      "            WordEmbeddingSimilarityIndex\n",
      "    \n",
      "    class SparseTermSimilarityMatrix(gensim.utils.SaveLoad)\n",
      "     |  SparseTermSimilarityMatrix(source, dictionary=None, tfidf=None, symmetric=True, dominant=False, nonzero_limit=100, dtype=<class 'numpy.float32'>)\n",
      "     |  \n",
      "     |  Builds a sparse term similarity matrix using a term similarity index.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from gensim.test.utils import common_texts as corpus, datapath\n",
      "     |  >>> from gensim.corpora import Dictionary\n",
      "     |  >>> from gensim.models import Word2Vec\n",
      "     |  >>> from gensim.similarities import SoftCosineSimilarity, SparseTermSimilarityMatrix, WordEmbeddingSimilarityIndex\n",
      "     |  >>> from gensim.similarities.index import AnnoyIndexer\n",
      "     |  >>>\n",
      "     |  >>> model_corpus_file = datapath('lee_background.cor')\n",
      "     |  >>> model = Word2Vec(corpus_file=model_corpus_file, vector_size=20, min_count=1)  # train word-vectors\n",
      "     |  >>>\n",
      "     |  >>> dictionary = Dictionary(corpus)\n",
      "     |  >>> tfidf = TfidfModel(dictionary=dictionary)\n",
      "     |  >>> words = [word for word, count in dictionary.most_common()]\n",
      "     |  >>> word_vectors = model.wv.vectors_for_all(words, allow_inference=False)  # produce vectors for words in corpus\n",
      "     |  >>>\n",
      "     |  >>> indexer = AnnoyIndexer(word_vectors, num_trees=2)  # use Annoy for faster word similarity lookups\n",
      "     |  >>> termsim_index = WordEmbeddingSimilarityIndex(word_vectors, kwargs={'indexer': indexer})\n",
      "     |  >>> similarity_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary, tfidf)  # compute word similarities\n",
      "     |  >>>\n",
      "     |  >>> tfidf_corpus = tfidf[[dictionary.doc2bow(document) for document in common_texts]]\n",
      "     |  >>> docsim_index = SoftCosineSimilarity(tfidf_corpus, similarity_matrix, num_best=10)  # index tfidf_corpus\n",
      "     |  >>>\n",
      "     |  >>> query = 'graph trees computer'.split()  # make a query\n",
      "     |  >>> sims = docsim_index[dictionary.doc2bow(query)]  # find the ten closest documents from tfidf_corpus\n",
      "     |  \n",
      "     |  Check out `the Gallery <https://radimrehurek.com/gensim/auto_examples/tutorials/run_scm.html>`_\n",
      "     |  for more examples.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  source : :class:`~gensim.similarities.termsim.TermSimilarityIndex` or :class:`scipy.sparse.spmatrix`\n",
      "     |      The source of the term similarity. Either a term similarity index that will be used for\n",
      "     |      building the term similarity matrix, or an existing sparse term similarity matrix that will\n",
      "     |      be encapsulated and stored in the matrix attribute. When a matrix is specified as the\n",
      "     |      source, any other parameters will be ignored.\n",
      "     |  dictionary : :class:`~gensim.corpora.dictionary.Dictionary` or None, optional\n",
      "     |      A dictionary that specifies a mapping between terms and the indices of rows and columns\n",
      "     |      of the resulting term similarity matrix. The dictionary may only be None when source is\n",
      "     |      a :class:`scipy.sparse.spmatrix`.\n",
      "     |  tfidf : :class:`gensim.models.tfidfmodel.TfidfModel` or None, optional\n",
      "     |      A model that specifies the relative importance of the terms in the dictionary. The columns\n",
      "     |      of the term similarity matrix will be build in a decreasing order of importance of\n",
      "     |      terms, or in the order of term identifiers if None.\n",
      "     |  symmetric : bool, optional\n",
      "     |      Whether the symmetry of the term similarity matrix will be enforced. Symmetry is a necessary\n",
      "     |      precondition for positive definiteness, which is necessary if you later wish to derive a\n",
      "     |      unique change-of-basis matrix from the term similarity matrix using Cholesky factorization.\n",
      "     |      Setting symmetric to False will significantly reduce memory usage during matrix construction.\n",
      "     |  dominant: bool, optional\n",
      "     |      Whether the strict column diagonal dominance of the term similarity matrix will be enforced.\n",
      "     |      Strict diagonal dominance and symmetry are sufficient preconditions for positive\n",
      "     |      definiteness, which is necessary if you later wish to derive a change-of-basis matrix from\n",
      "     |      the term similarity matrix using Cholesky factorization.\n",
      "     |  nonzero_limit : int or None, optional\n",
      "     |      The maximum number of non-zero elements outside the diagonal in a single column of the\n",
      "     |      sparse term similarity matrix. If None, then no limit will be imposed.\n",
      "     |  dtype : numpy.dtype, optional\n",
      "     |      The data type of the sparse term similarity matrix.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  matrix : :class:`scipy.sparse.csc_matrix`\n",
      "     |      The encapsulated sparse term similarity matrix.\n",
      "     |  \n",
      "     |  Raises\n",
      "     |  ------\n",
      "     |  ValueError\n",
      "     |      If `dictionary` is empty.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  :class:`~gensim.similarities.docsim.SoftCosineSimilarity`\n",
      "     |      A document similarity index using the soft cosine similarity over the term similarity matrix.\n",
      "     |  :class:`~gensim.similarities.termsim.LevenshteinSimilarityIndex`\n",
      "     |      A term similarity index that computes Levenshtein similarities between terms.\n",
      "     |  :class:`~gensim.similarities.termsim.WordEmbeddingSimilarityIndex`\n",
      "     |      A term similarity index that computes cosine similarities between word embeddings.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SparseTermSimilarityMatrix\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, source, dictionary=None, tfidf=None, symmetric=True, dominant=False, nonzero_limit=100, dtype=<class 'numpy.float32'>)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  inner_product(self, X, Y, normalized=(False, False))\n",
      "     |      Get the inner product(s) between real vectors / corpora X and Y.\n",
      "     |      \n",
      "     |      Return the inner product(s) between real vectors / corpora vec1 and vec2 expressed in a\n",
      "     |      non-orthogonal normalized basis, where the dot product between the basis vectors is given by\n",
      "     |      the sparse term similarity matrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      vec1 : list of (int, float) or iterable of list of (int, float)\n",
      "     |          A query vector / corpus in the sparse bag-of-words format.\n",
      "     |      vec2 : list of (int, float) or iterable of list of (int, float)\n",
      "     |          A document vector / corpus in the sparse bag-of-words format.\n",
      "     |      normalized : tuple of {True, False, 'maintain'}, optional\n",
      "     |          First/second value specifies whether the query/document vectors in the inner product\n",
      "     |          will be L2-normalized (True; corresponds to the soft cosine measure), maintain their\n",
      "     |          L2-norm during change of basis ('maintain'; corresponds to query expansion with partial\n",
      "     |          membership), or kept as-is (False; corresponds to query expansion; default).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      `self.matrix.dtype`,  `scipy.sparse.csr_matrix`, or :class:`numpy.matrix`\n",
      "     |          The inner product(s) between `X` and `Y`.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      The soft cosine measure was perhaps first described by [sidorovetal14]_.\n",
      "     |      Further notes on the efficient implementation of the soft cosine measure are described by\n",
      "     |      [novotny18]_.\n",
      "     |      \n",
      "     |      .. [sidorovetal14] Grigori Sidorov et al., \"Soft Similarity and Soft Cosine Measure: Similarity\n",
      "     |         of Features in Vector Space Model\", 2014, http://www.cys.cic.ipn.mx/ojs/index.php/CyS/article/view/2043/1921.\n",
      "     |      \n",
      "     |      .. [novotny18] Vít Novotný, \"Implementation Notes for the Soft Cosine Measure\", 2018,\n",
      "     |         http://dx.doi.org/10.1145/3269206.3269317.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      "     |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      "     |      optionally log the event at `log_level`.\n",
      "     |      \n",
      "     |      Events are important moments during the object's life, such as \"model created\",\n",
      "     |      \"model saved\", \"model loaded\", etc.\n",
      "     |      \n",
      "     |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      "     |      but is useful during debugging and support.\n",
      "     |      \n",
      "     |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      "     |      will not record events into `self.lifecycle_events` then.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      event_name : str\n",
      "     |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      "     |      event : dict\n",
      "     |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      "     |          Can be empty.\n",
      "     |      \n",
      "     |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      "     |      \n",
      "     |          - `datetime`: the current date & time\n",
      "     |          - `gensim`: the current Gensim version\n",
      "     |          - `python`: the current Python version\n",
      "     |          - `platform`: the current platform\n",
      "     |          - `event`: the name of this event\n",
      "     |      log_level : int\n",
      "     |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=4)\n",
      "     |      Save the object to a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname_or_handle : str or file-like\n",
      "     |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      "     |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      "     |      separately : list of str or None, optional\n",
      "     |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      "     |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      "     |          loading and sharing the large arrays in RAM between multiple processes.\n",
      "     |      \n",
      "     |          If list of str: store these attributes into separate files. The automated size check\n",
      "     |          is not performed in this case.\n",
      "     |      sep_limit : int, optional\n",
      "     |          Don't store arrays smaller than this separately. In bytes.\n",
      "     |      ignore : frozenset of str, optional\n",
      "     |          Attributes that shouldn't be stored at all.\n",
      "     |      pickle_protocol : int, optional\n",
      "     |          Protocol number for pickle.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.load`\n",
      "     |          Load object from file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(fname, mmap=None) from builtins.type\n",
      "     |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to file that contains needed object.\n",
      "     |      mmap : str, optional\n",
      "     |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      "     |          via mmap (shared memory) using `mmap='r'.\n",
      "     |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |          Save object to file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      object\n",
      "     |          Object loaded from `fname`.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          When called on an object instance instead of class (this is a class method).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TermSimilarityIndex(gensim.utils.SaveLoad)\n",
      "     |  Base class = common interface for retrieving the most similar terms for a given term.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  :class:`~gensim.similarities.termsim.SparseTermSimilarityMatrix`\n",
      "     |      A sparse term similarity matrix built using a term similarity index.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TermSimilarityIndex\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  most_similar(self, term, topn=10)\n",
      "     |      Get most similar terms for a given term.\n",
      "     |      \n",
      "     |      Return the most similar terms for a given term along with their similarities.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      term : str\n",
      "     |          The term for which we are retrieving `topn` most similar terms.\n",
      "     |      topn : int, optional\n",
      "     |          The maximum number of most similar terms to `term` that will be retrieved.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      iterable of (str, float)\n",
      "     |          Most similar terms along with their similarities to `term`. Only terms distinct from\n",
      "     |          `term` must be returned.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      "     |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      "     |      optionally log the event at `log_level`.\n",
      "     |      \n",
      "     |      Events are important moments during the object's life, such as \"model created\",\n",
      "     |      \"model saved\", \"model loaded\", etc.\n",
      "     |      \n",
      "     |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      "     |      but is useful during debugging and support.\n",
      "     |      \n",
      "     |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      "     |      will not record events into `self.lifecycle_events` then.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      event_name : str\n",
      "     |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      "     |      event : dict\n",
      "     |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      "     |          Can be empty.\n",
      "     |      \n",
      "     |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      "     |      \n",
      "     |          - `datetime`: the current date & time\n",
      "     |          - `gensim`: the current Gensim version\n",
      "     |          - `python`: the current Python version\n",
      "     |          - `platform`: the current platform\n",
      "     |          - `event`: the name of this event\n",
      "     |      log_level : int\n",
      "     |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=4)\n",
      "     |      Save the object to a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname_or_handle : str or file-like\n",
      "     |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      "     |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      "     |      separately : list of str or None, optional\n",
      "     |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      "     |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      "     |          loading and sharing the large arrays in RAM between multiple processes.\n",
      "     |      \n",
      "     |          If list of str: store these attributes into separate files. The automated size check\n",
      "     |          is not performed in this case.\n",
      "     |      sep_limit : int, optional\n",
      "     |          Don't store arrays smaller than this separately. In bytes.\n",
      "     |      ignore : frozenset of str, optional\n",
      "     |          Attributes that shouldn't be stored at all.\n",
      "     |      pickle_protocol : int, optional\n",
      "     |          Protocol number for pickle.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.load`\n",
      "     |          Load object from file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(fname, mmap=None) from builtins.type\n",
      "     |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to file that contains needed object.\n",
      "     |      mmap : str, optional\n",
      "     |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      "     |          via mmap (shared memory) using `mmap='r'.\n",
      "     |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |          Save object to file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      object\n",
      "     |          Object loaded from `fname`.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          When called on an object instance instead of class (this is a class method).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class UniformTermSimilarityIndex(TermSimilarityIndex)\n",
      "     |  UniformTermSimilarityIndex(dictionary, term_similarity=0.5)\n",
      "     |  \n",
      "     |  Retrieves most similar terms for a given term under the hypothesis that the similarities between\n",
      "     |  distinct terms are uniform.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\n",
      "     |      A dictionary that specifies the considered terms.\n",
      "     |  term_similarity : float, optional\n",
      "     |      The uniform similarity between distinct terms.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  :class:`~gensim.similarities.termsim.SparseTermSimilarityMatrix`\n",
      "     |      A sparse term similarity matrix built using a term similarity index.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  This class is mainly intended for testing SparseTermSimilarityMatrix and other classes that\n",
      "     |  depend on the TermSimilarityIndex.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      UniformTermSimilarityIndex\n",
      "     |      TermSimilarityIndex\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dictionary, term_similarity=0.5)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  most_similar(self, t1, topn=10)\n",
      "     |      Get most similar terms for a given term.\n",
      "     |      \n",
      "     |      Return the most similar terms for a given term along with their similarities.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      term : str\n",
      "     |          The term for which we are retrieving `topn` most similar terms.\n",
      "     |      topn : int, optional\n",
      "     |          The maximum number of most similar terms to `term` that will be retrieved.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      iterable of (str, float)\n",
      "     |          Most similar terms along with their similarities to `term`. Only terms distinct from\n",
      "     |          `term` must be returned.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from TermSimilarityIndex:\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      "     |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      "     |      optionally log the event at `log_level`.\n",
      "     |      \n",
      "     |      Events are important moments during the object's life, such as \"model created\",\n",
      "     |      \"model saved\", \"model loaded\", etc.\n",
      "     |      \n",
      "     |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      "     |      but is useful during debugging and support.\n",
      "     |      \n",
      "     |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      "     |      will not record events into `self.lifecycle_events` then.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      event_name : str\n",
      "     |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      "     |      event : dict\n",
      "     |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      "     |          Can be empty.\n",
      "     |      \n",
      "     |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      "     |      \n",
      "     |          - `datetime`: the current date & time\n",
      "     |          - `gensim`: the current Gensim version\n",
      "     |          - `python`: the current Python version\n",
      "     |          - `platform`: the current platform\n",
      "     |          - `event`: the name of this event\n",
      "     |      log_level : int\n",
      "     |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=4)\n",
      "     |      Save the object to a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname_or_handle : str or file-like\n",
      "     |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      "     |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      "     |      separately : list of str or None, optional\n",
      "     |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      "     |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      "     |          loading and sharing the large arrays in RAM between multiple processes.\n",
      "     |      \n",
      "     |          If list of str: store these attributes into separate files. The automated size check\n",
      "     |          is not performed in this case.\n",
      "     |      sep_limit : int, optional\n",
      "     |          Don't store arrays smaller than this separately. In bytes.\n",
      "     |      ignore : frozenset of str, optional\n",
      "     |          Attributes that shouldn't be stored at all.\n",
      "     |      pickle_protocol : int, optional\n",
      "     |          Protocol number for pickle.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.load`\n",
      "     |          Load object from file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(fname, mmap=None) from builtins.type\n",
      "     |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to file that contains needed object.\n",
      "     |      mmap : str, optional\n",
      "     |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      "     |          via mmap (shared memory) using `mmap='r'.\n",
      "     |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |          Save object to file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      object\n",
      "     |          Object loaded from `fname`.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          When called on an object instance instead of class (this is a class method).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class WordEmbeddingSimilarityIndex(TermSimilarityIndex)\n",
      "     |  WordEmbeddingSimilarityIndex(keyedvectors, threshold=0.0, exponent=2.0, kwargs=None)\n",
      "     |  \n",
      "     |  Computes cosine similarities between word embeddings and retrieves most\n",
      "     |  similar terms for a given term.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  By fitting the word embeddings to a vocabulary that you will be using, you\n",
      "     |  can eliminate all out-of-vocabulary (OOV) words that you would otherwise\n",
      "     |  receive from the `most_similar` method. In subword models such as fastText,\n",
      "     |  this procedure will also infer word-vectors for words from your vocabulary\n",
      "     |  that previously had no word-vector.\n",
      "     |  \n",
      "     |  >>> from gensim.test.utils import common_texts, datapath\n",
      "     |  >>> from gensim.corpora import Dictionary\n",
      "     |  >>> from gensim.models import FastText\n",
      "     |  >>> from gensim.models.word2vec import LineSentence\n",
      "     |  >>> from gensim.similarities import WordEmbeddingSimilarityIndex\n",
      "     |  >>>\n",
      "     |  >>> model = FastText(common_texts, vector_size=20, min_count=1)  # train word-vectors on a corpus\n",
      "     |  >>> different_corpus = LineSentence(datapath('lee_background.cor'))\n",
      "     |  >>> dictionary = Dictionary(different_corpus)  # construct a vocabulary on a different corpus\n",
      "     |  >>> words = [word for word, count in dictionary.most_common()]\n",
      "     |  >>> word_vectors = model.wv.vectors_for_all(words)  # remove OOV word-vectors and infer word-vectors for new words\n",
      "     |  >>> assert len(dictionary) == len(word_vectors)  # all words from our vocabulary received their word-vectors\n",
      "     |  >>> termsim_index = WordEmbeddingSimilarityIndex(word_vectors)\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  keyedvectors : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      "     |      The word embeddings.\n",
      "     |  threshold : float, optional\n",
      "     |      Only embeddings more similar than `threshold` are considered when retrieving word embeddings\n",
      "     |      closest to a given word embedding.\n",
      "     |  exponent : float, optional\n",
      "     |      Take the word embedding similarities larger than `threshold` to the power of `exponent`.\n",
      "     |  kwargs : dict or None\n",
      "     |      A dict with keyword arguments that will be passed to the\n",
      "     |      :meth:`~gensim.models.keyedvectors.KeyedVectors.most_similar` method\n",
      "     |      when retrieving the word embeddings closest to a given word embedding.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  :class:`~gensim.similarities.levenshtein.LevenshteinSimilarityIndex`\n",
      "     |      Retrieve most similar terms for a given term using the Levenshtein distance.\n",
      "     |  :class:`~gensim.similarities.termsim.SparseTermSimilarityMatrix`\n",
      "     |      Build a term similarity matrix and compute the Soft Cosine Measure.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      WordEmbeddingSimilarityIndex\n",
      "     |      TermSimilarityIndex\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, keyedvectors, threshold=0.0, exponent=2.0, kwargs=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  most_similar(self, t1, topn=10)\n",
      "     |      Get most similar terms for a given term.\n",
      "     |      \n",
      "     |      Return the most similar terms for a given term along with their similarities.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      term : str\n",
      "     |          The term for which we are retrieving `topn` most similar terms.\n",
      "     |      topn : int, optional\n",
      "     |          The maximum number of most similar terms to `term` that will be retrieved.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      iterable of (str, float)\n",
      "     |          Most similar terms along with their similarities to `term`. Only terms distinct from\n",
      "     |          `term` must be returned.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from TermSimilarityIndex:\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      "     |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      "     |      optionally log the event at `log_level`.\n",
      "     |      \n",
      "     |      Events are important moments during the object's life, such as \"model created\",\n",
      "     |      \"model saved\", \"model loaded\", etc.\n",
      "     |      \n",
      "     |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      "     |      but is useful during debugging and support.\n",
      "     |      \n",
      "     |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      "     |      will not record events into `self.lifecycle_events` then.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      event_name : str\n",
      "     |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      "     |      event : dict\n",
      "     |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      "     |          Can be empty.\n",
      "     |      \n",
      "     |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      "     |      \n",
      "     |          - `datetime`: the current date & time\n",
      "     |          - `gensim`: the current Gensim version\n",
      "     |          - `python`: the current Python version\n",
      "     |          - `platform`: the current platform\n",
      "     |          - `event`: the name of this event\n",
      "     |      log_level : int\n",
      "     |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=4)\n",
      "     |      Save the object to a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname_or_handle : str or file-like\n",
      "     |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      "     |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      "     |      separately : list of str or None, optional\n",
      "     |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      "     |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      "     |          loading and sharing the large arrays in RAM between multiple processes.\n",
      "     |      \n",
      "     |          If list of str: store these attributes into separate files. The automated size check\n",
      "     |          is not performed in this case.\n",
      "     |      sep_limit : int, optional\n",
      "     |          Don't store arrays smaller than this separately. In bytes.\n",
      "     |      ignore : frozenset of str, optional\n",
      "     |          Attributes that shouldn't be stored at all.\n",
      "     |      pickle_protocol : int, optional\n",
      "     |          Protocol number for pickle.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.load`\n",
      "     |          Load object from file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(fname, mmap=None) from builtins.type\n",
      "     |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to file that contains needed object.\n",
      "     |      mmap : str, optional\n",
      "     |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      "     |          via mmap (shared memory) using `mmap='r'.\n",
      "     |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |          Save object to file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      object\n",
      "     |          Object loaded from `fname`.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          When called on an object instance instead of class (this is a class method).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    sqrt(x, /)\n",
      "        Return the square root of x.\n",
      "\n",
      "DATA\n",
      "    NON_NEGATIVE_NORM_ASSERTION_MESSAGE = 'sparse documents must not conta...\n",
      "    logger = <Logger gensim.similarities.termsim (WARNING)>\n",
      "\n",
      "FILE\n",
      "    c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages\\gensim\\similarities\\termsim.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gensim.similarities.termsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package gensim.topic_coherence in gensim:\n",
      "\n",
      "NAME\n",
      "    gensim.topic_coherence\n",
      "\n",
      "DESCRIPTION\n",
      "    This package contains implementation of the individual components of\n",
      "    the topic coherence pipeline.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    aggregation\n",
      "    direct_confirmation_measure\n",
      "    indirect_confirmation_measure\n",
      "    probability_estimation\n",
      "    segmentation\n",
      "    text_analysis\n",
      "\n",
      "FILE\n",
      "    c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages\\gensim\\topic_coherence\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gensim.topic_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbm25\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BM25\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "from gensim.summarization.bm25 import BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4411401303895046,\n",
       " 3.2979916804886016,\n",
       " 1.5600394753312772,\n",
       " 1.571152298809557,\n",
       " 1.5543146105226204,\n",
       " 1.053356832738657]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25gm = BM25(tokenized_corpus)\n",
    "bm25gm\n",
    "\n",
    "scores = bm25gm.get_scores(tokenized_query)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.44114013, 3.29799168, 1.56003948, 1.5711523 , 1.55431461,\n",
       "       1.05335683])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 1: Trump Congressional Address.txt\n",
      "rank 2: Trump Immigration Speech 8-31-16.txt\n",
      "rank 3: Trump CPAC Speech.txt\n"
     ]
    }
   ],
   "source": [
    "best_docs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:3]\n",
    "for i, b in enumerate(best_docs):\n",
    "    #print(f\"rank {i+1}: {corpus[b]}\")\n",
    "    print(f\"rank {i+1}: {fileS[b]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 2]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enumerate at 0x17bfa23de80>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enumerate(best_docs) ### to check?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn\n",
      "\n",
      "DESCRIPTION\n",
      "    Machine learning module for Python\n",
      "    ==================================\n",
      "    \n",
      "    sklearn is a Python module integrating classical machine\n",
      "    learning algorithms in the tightly-knit world of scientific Python\n",
      "    packages (numpy, scipy, matplotlib).\n",
      "    \n",
      "    It aims to provide simple and efficient solutions to learning problems\n",
      "    that are accessible to everybody and reusable in various contexts:\n",
      "    machine-learning as a versatile tool for science and engineering.\n",
      "    \n",
      "    See http://scikit-learn.org for complete documentation.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __check_build (package)\n",
      "    _build_utils (package)\n",
      "    _config\n",
      "    _distributor_init\n",
      "    _isotonic\n",
      "    _loss (package)\n",
      "    base\n",
      "    calibration\n",
      "    cluster (package)\n",
      "    compose (package)\n",
      "    conftest\n",
      "    covariance (package)\n",
      "    cross_decomposition (package)\n",
      "    datasets (package)\n",
      "    decomposition (package)\n",
      "    discriminant_analysis\n",
      "    dummy\n",
      "    ensemble (package)\n",
      "    exceptions\n",
      "    experimental (package)\n",
      "    externals (package)\n",
      "    feature_extraction (package)\n",
      "    feature_selection (package)\n",
      "    gaussian_process (package)\n",
      "    impute (package)\n",
      "    inspection (package)\n",
      "    isotonic\n",
      "    kernel_approximation\n",
      "    kernel_ridge\n",
      "    linear_model (package)\n",
      "    manifold (package)\n",
      "    metrics (package)\n",
      "    mixture (package)\n",
      "    model_selection (package)\n",
      "    multiclass\n",
      "    multioutput\n",
      "    naive_bayes\n",
      "    neighbors (package)\n",
      "    neural_network (package)\n",
      "    pipeline\n",
      "    preprocessing (package)\n",
      "    random_projection\n",
      "    semi_supervised (package)\n",
      "    setup\n",
      "    svm (package)\n",
      "    tests (package)\n",
      "    tree (package)\n",
      "    utils (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    clone(estimator, *, safe=True)\n",
      "        Constructs a new estimator with the same parameters.\n",
      "        \n",
      "        Clone does a deep copy of the model in an estimator\n",
      "        without actually copying attached data. It yields a new estimator\n",
      "        with the same parameters that has not been fit on any data.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : {list, tuple, set} of estimator objects or estimator object\n",
      "            The estimator or group of estimators to be cloned.\n",
      "        \n",
      "        safe : bool, default=True\n",
      "            If safe is false, clone will fall back to a deep copy on objects\n",
      "            that are not estimators.\n",
      "    \n",
      "    config_context(**new_config)\n",
      "        Context manager for global scikit-learn configuration\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, optional\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error.  Global default: False.\n",
      "        \n",
      "        working_memory : int, optional\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. Global default: 1024.\n",
      "        \n",
      "        print_changed_only : bool, optional\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()', but would print\n",
      "            'SVC(C=1.0, cache_size=200, ...)' with all the non-changed parameters\n",
      "            when False. Default is True.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Default changed from False to True.\n",
      "        \n",
      "        display : {'text', 'diagram'}, optional\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. Default is 'text'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        All settings, not just those presently modified, will be returned to\n",
      "        their previous values when the context manager is exited. This is not\n",
      "        thread-safe.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import sklearn\n",
      "        >>> from sklearn.utils.validation import assert_all_finite\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     assert_all_finite([float('nan')])\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     with sklearn.config_context(assume_finite=False):\n",
      "        ...         assert_all_finite([float('nan')])\n",
      "        Traceback (most recent call last):\n",
      "        ...\n",
      "        ValueError: Input contains NaN, ...\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        set_config: Set global scikit-learn configuration\n",
      "        get_config: Retrieve current values of the global configuration\n",
      "    \n",
      "    get_config()\n",
      "        Retrieve current values for configuration set by :func:`set_config`\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        config : dict\n",
      "            Keys are parameter names that can be passed to :func:`set_config`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context: Context manager for global scikit-learn configuration\n",
      "        set_config: Set global scikit-learn configuration\n",
      "    \n",
      "    set_config(assume_finite=None, working_memory=None, print_changed_only=None, display=None)\n",
      "        Set global scikit-learn configuration\n",
      "        \n",
      "        .. versionadded:: 0.19\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, optional\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error.  Global default: False.\n",
      "        \n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "        working_memory : int, optional\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. Global default: 1024.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        print_changed_only : bool, optional\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()' while the default\n",
      "            behaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\n",
      "            all the non-changed parameters.\n",
      "        \n",
      "            .. versionadded:: 0.21\n",
      "        \n",
      "        display : {'text', 'diagram'}, optional\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. Default is 'text'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context: Context manager for global scikit-learn configuration\n",
      "        get_config: Retrieve current values of the global configuration\n",
      "    \n",
      "    show_versions()\n",
      "        Print useful debugging information\"\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "\n",
      "DATA\n",
      "    __SKLEARN_SETUP__ = False\n",
      "    __all__ = ['calibration', 'cluster', 'covariance', 'cross_decompositio...\n",
      "\n",
      "VERSION\n",
      "    0.23.2\n",
      "\n",
      "FILE\n",
      "    c:\\anaconda\\envs\\textanalytics\\lib\\site-packages\\sklearn\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn' has no attribute 'metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-26d494d15b77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhelp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sklearn' has no attribute 'metrics'"
     ]
    }
   ],
   "source": [
    "help(sklearn.metrics.pairwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-596153aea2bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhelp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "help(metrics.pairwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module sklearn.metrics.pairwise in sklearn.metrics:\n",
      "\n",
      "NAME\n",
      "    sklearn.metrics.pairwise - # -*- coding: utf-8 -*-\n",
      "\n",
      "FUNCTIONS\n",
      "    additive_chi2_kernel(X, Y=None)\n",
      "        Computes the additive chi-squared kernel between observations in X and Y\n",
      "        \n",
      "        The chi-squared kernel is computed between each pair of rows in X and Y.  X\n",
      "        and Y have to be non-negative. This kernel is most commonly applied to\n",
      "        histograms.\n",
      "        \n",
      "        The chi-squared kernel is given by::\n",
      "        \n",
      "            k(x, y) = -Sum [(x - y)^2 / (x + y)]\n",
      "        \n",
      "        It can be interpreted as a weighted difference per entry.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <chi2_kernel>`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        As the negative of a distance, this kernel is only conditionally positive\n",
      "        definite.\n",
      "        \n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_X, n_features)\n",
      "        \n",
      "        Y : array of shape (n_samples_Y, n_features)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kernel_matrix : array of shape (n_samples_X, n_samples_Y)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C.\n",
      "          Local features and kernels for classification of texture and object\n",
      "          categories: A comprehensive study\n",
      "          International Journal of Computer Vision 2007\n",
      "          https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf\n",
      "        \n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        chi2_kernel : The exponentiated version of the kernel, which is usually\n",
      "            preferable.\n",
      "        \n",
      "        sklearn.kernel_approximation.AdditiveChi2Sampler : A Fourier approximation\n",
      "            to this kernel.\n",
      "    \n",
      "    check_paired_arrays(X, Y)\n",
      "        Set X and Y appropriately and checks inputs for paired distances\n",
      "        \n",
      "        All paired distance metrics should use this function first to assert that\n",
      "        the given parameters are correct and safe to use.\n",
      "        \n",
      "        Specifically, this function first ensures that both X and Y are arrays,\n",
      "        then checks that they are at least two dimensional while ensuring that\n",
      "        their elements are floats. Finally, the function checks that the size\n",
      "        of the dimensions of the two arrays are equal.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape (n_samples_a, n_features)\n",
      "        \n",
      "        Y : {array-like, sparse matrix}, shape (n_samples_b, n_features)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        safe_X : {array-like, sparse matrix}, shape (n_samples_a, n_features)\n",
      "            An array equal to X, guaranteed to be a numpy array.\n",
      "        \n",
      "        safe_Y : {array-like, sparse matrix}, shape (n_samples_b, n_features)\n",
      "            An array equal to Y if Y was not None, guaranteed to be a numpy array.\n",
      "            If Y was None, safe_Y will be a pointer to X.\n",
      "    \n",
      "    check_pairwise_arrays(X, Y, *, precomputed=False, dtype=None, accept_sparse='csr', force_all_finite=True, copy=False)\n",
      "        Set X and Y appropriately and checks inputs\n",
      "        \n",
      "        If Y is None, it is set as a pointer to X (i.e. not a copy).\n",
      "        If Y is given, this does not happen.\n",
      "        All distance metrics should use this function first to assert that the\n",
      "        given parameters are correct and safe to use.\n",
      "        \n",
      "        Specifically, this function first ensures that both X and Y are arrays,\n",
      "        then checks that they are at least two dimensional while ensuring that\n",
      "        their elements are floats (or dtype if provided). Finally, the function\n",
      "        checks that the size of the second dimension of the two arrays is equal, or\n",
      "        the equivalent check for a precomputed distance matrix.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape (n_samples_a, n_features)\n",
      "        \n",
      "        Y : {array-like, sparse matrix}, shape (n_samples_b, n_features)\n",
      "        \n",
      "        precomputed : bool\n",
      "            True if X is to be treated as precomputed distances to the samples in\n",
      "            Y.\n",
      "        \n",
      "        dtype : string, type, list of types or None (default=None)\n",
      "            Data type required for X and Y. If None, the dtype will be an\n",
      "            appropriate float type selected by _return_float_dtype.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        accept_sparse : string, boolean or list/tuple of strings\n",
      "            String[s] representing allowed sparse matrix formats, such as 'csc',\n",
      "            'csr', etc. If the input is sparse but not in the allowed format,\n",
      "            it will be converted to the first listed format. True allows the input\n",
      "            to be any format. False means that a sparse matrix input will\n",
      "            raise an error.\n",
      "        \n",
      "        force_all_finite : boolean or 'allow-nan', (default=True)\n",
      "            Whether to raise an error on np.inf, np.nan, pd.NA in array. The\n",
      "            possibilities are:\n",
      "        \n",
      "            - True: Force all values of array to be finite.\n",
      "            - False: accepts np.inf, np.nan, pd.NA in array.\n",
      "            - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n",
      "              cannot be infinite.\n",
      "        \n",
      "            .. versionadded:: 0.22\n",
      "               ``force_all_finite`` accepts the string ``'allow-nan'``.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Accepts `pd.NA` and converts it into `np.nan`\n",
      "        \n",
      "        copy : bool\n",
      "            Whether a forced copy will be triggered. If copy=False, a copy might\n",
      "            be triggered by a conversion.\n",
      "        \n",
      "            .. versionadded:: 0.22\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        safe_X : {array-like, sparse matrix}, shape (n_samples_a, n_features)\n",
      "            An array equal to X, guaranteed to be a numpy array.\n",
      "        \n",
      "        safe_Y : {array-like, sparse matrix}, shape (n_samples_b, n_features)\n",
      "            An array equal to Y if Y was not None, guaranteed to be a numpy array.\n",
      "            If Y was None, safe_Y will be a pointer to X.\n",
      "    \n",
      "    chi2_kernel(X, Y=None, gamma=1.0)\n",
      "        Computes the exponential chi-squared kernel X and Y.\n",
      "        \n",
      "        The chi-squared kernel is computed between each pair of rows in X and Y.  X\n",
      "        and Y have to be non-negative. This kernel is most commonly applied to\n",
      "        histograms.\n",
      "        \n",
      "        The chi-squared kernel is given by::\n",
      "        \n",
      "            k(x, y) = exp(-gamma Sum [(x - y)^2 / (x + y)])\n",
      "        \n",
      "        It can be interpreted as a weighted difference per entry.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <chi2_kernel>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_X, n_features)\n",
      "        \n",
      "        Y : array of shape (n_samples_Y, n_features)\n",
      "        \n",
      "        gamma : float, default=1.\n",
      "            Scaling parameter of the chi2 kernel.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kernel_matrix : array of shape (n_samples_X, n_samples_Y)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C.\n",
      "          Local features and kernels for classification of texture and object\n",
      "          categories: A comprehensive study\n",
      "          International Journal of Computer Vision 2007\n",
      "          https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        additive_chi2_kernel : The additive version of this kernel\n",
      "        \n",
      "        sklearn.kernel_approximation.AdditiveChi2Sampler : A Fourier approximation\n",
      "            to the additive version of this kernel.\n",
      "    \n",
      "    cosine_distances(X, Y=None)\n",
      "        Compute cosine distance between samples in X and Y.\n",
      "        \n",
      "        Cosine distance is defined as 1.0 minus the cosine similarity.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array_like, sparse matrix\n",
      "            with shape (n_samples_X, n_features).\n",
      "        \n",
      "        Y : array_like, sparse matrix (optional)\n",
      "            with shape (n_samples_Y, n_features).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distance matrix : array\n",
      "            An array with shape (n_samples_X, n_samples_Y).\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        sklearn.metrics.pairwise.cosine_similarity\n",
      "        scipy.spatial.distance.cosine : dense matrices only\n",
      "    \n",
      "    cosine_similarity(X, Y=None, dense_output=True)\n",
      "        Compute cosine similarity between samples in X and Y.\n",
      "        \n",
      "        Cosine similarity, or the cosine kernel, computes similarity as the\n",
      "        normalized dot product of X and Y:\n",
      "        \n",
      "            K(X, Y) = <X, Y> / (||X||*||Y||)\n",
      "        \n",
      "        On L2-normalized data, this function is equivalent to linear_kernel.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cosine_similarity>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray or sparse array, shape: (n_samples_X, n_features)\n",
      "            Input data.\n",
      "        \n",
      "        Y : ndarray or sparse array, shape: (n_samples_Y, n_features)\n",
      "            Input data. If ``None``, the output will be the pairwise\n",
      "            similarities between all samples in ``X``.\n",
      "        \n",
      "        dense_output : boolean (optional), default True\n",
      "            Whether to return dense output even when the input is sparse. If\n",
      "            ``False``, the output is sparse if both input arrays are sparse.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               parameter ``dense_output`` for dense output.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kernel matrix : array\n",
      "            An array with shape (n_samples_X, n_samples_Y).\n",
      "    \n",
      "    distance_metrics()\n",
      "        Valid metrics for pairwise_distances.\n",
      "        \n",
      "        This function simply returns the valid pairwise distance metrics.\n",
      "        It exists to allow for a description of the mapping for\n",
      "        each of the valid strings.\n",
      "        \n",
      "        The valid distance metrics, and the function they map to, are:\n",
      "        \n",
      "        =============== ========================================\n",
      "        metric          Function\n",
      "        =============== ========================================\n",
      "        'cityblock'     metrics.pairwise.manhattan_distances\n",
      "        'cosine'        metrics.pairwise.cosine_distances\n",
      "        'euclidean'     metrics.pairwise.euclidean_distances\n",
      "        'haversine'     metrics.pairwise.haversine_distances\n",
      "        'l1'            metrics.pairwise.manhattan_distances\n",
      "        'l2'            metrics.pairwise.euclidean_distances\n",
      "        'manhattan'     metrics.pairwise.manhattan_distances\n",
      "        'nan_euclidean' metrics.pairwise.nan_euclidean_distances\n",
      "        =============== ========================================\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "    \n",
      "    euclidean_distances(X, Y=None, *, Y_norm_squared=None, squared=False, X_norm_squared=None)\n",
      "        Considering the rows of X (and Y=X) as vectors, compute the\n",
      "        distance matrix between each pair of vectors.\n",
      "        \n",
      "        For efficiency reasons, the euclidean distance between a pair of row\n",
      "        vector x and y is computed as::\n",
      "        \n",
      "            dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))\n",
      "        \n",
      "        This formulation has two advantages over other ways of computing distances.\n",
      "        First, it is computationally efficient when dealing with sparse data.\n",
      "        Second, if one argument varies but the other remains unchanged, then\n",
      "        `dot(x, x)` and/or `dot(y, y)` can be pre-computed.\n",
      "        \n",
      "        However, this is not the most precise way of doing this computation, and\n",
      "        the distance matrix returned by this function may not be exactly\n",
      "        symmetric as required by, e.g., ``scipy.spatial.distance`` functions.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape (n_samples_1, n_features)\n",
      "        \n",
      "        Y : {array-like, sparse matrix}, shape (n_samples_2, n_features)\n",
      "        \n",
      "        Y_norm_squared : array-like, shape (n_samples_2, ), optional\n",
      "            Pre-computed dot-products of vectors in Y (e.g.,\n",
      "            ``(Y**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        squared : boolean, optional\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        X_norm_squared : array-like of shape (n_samples,), optional\n",
      "            Pre-computed dot-products of vectors in X (e.g.,\n",
      "            ``(X**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        To achieve better accuracy, `X_norm_squared` and `Y_norm_squared` may be\n",
      "        unused if they are passed as ``float32``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : array, shape (n_samples_1, n_samples_2)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import euclidean_distances\n",
      "        >>> X = [[0, 1], [1, 1]]\n",
      "        >>> # distance between rows of X\n",
      "        >>> euclidean_distances(X, X)\n",
      "        array([[0., 1.],\n",
      "               [1., 0.]])\n",
      "        >>> # get distance to origin\n",
      "        >>> euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        paired_distances : distances betweens pairs of elements of X and Y.\n",
      "    \n",
      "    haversine_distances(X, Y=None)\n",
      "        Compute the Haversine distance between samples in X and Y\n",
      "        \n",
      "        The Haversine (or great circle) distance is the angular distance between\n",
      "        two points on the surface of a sphere. The first distance of each point is\n",
      "        assumed to be the latitude, the second is the longitude, given in radians.\n",
      "        The dimension of the data must be 2.\n",
      "        \n",
      "        .. math::\n",
      "           D(x, y) = 2\\arcsin[\\sqrt{\\sin^2((x1 - y1) / 2)\n",
      "                                    + \\cos(x1)\\cos(y1)\\sin^2((x2 - y2) / 2)}]\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array_like, shape (n_samples_1, 2)\n",
      "        \n",
      "        Y : array_like, shape (n_samples_2, 2), optional\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distance : {array}, shape (n_samples_1, n_samples_2)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        As the Earth is nearly spherical, the haversine formula provides a good\n",
      "        approximation of the distance between two points of the Earth surface, with\n",
      "        a less than 1% error on average.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We want to calculate the distance between the Ezeiza Airport\n",
      "        (Buenos Aires, Argentina) and the Charles de Gaulle Airport (Paris, France)\n",
      "        \n",
      "        >>> from sklearn.metrics.pairwise import haversine_distances\n",
      "        >>> from math import radians\n",
      "        >>> bsas = [-34.83333, -58.5166646]\n",
      "        >>> paris = [49.0083899664, 2.53844117956]\n",
      "        >>> bsas_in_radians = [radians(_) for _ in bsas]\n",
      "        >>> paris_in_radians = [radians(_) for _ in paris]\n",
      "        >>> result = haversine_distances([bsas_in_radians, paris_in_radians])\n",
      "        >>> result * 6371000/1000  # multiply by Earth radius to get kilometers\n",
      "        array([[    0.        , 11099.54035582],\n",
      "               [11099.54035582,     0.        ]])\n",
      "    \n",
      "    kernel_metrics()\n",
      "        Valid metrics for pairwise_kernels\n",
      "        \n",
      "        This function simply returns the valid pairwise distance metrics.\n",
      "        It exists, however, to allow for a verbose description of the mapping for\n",
      "        each of the valid strings.\n",
      "        \n",
      "        The valid distance metrics, and the function they map to, are:\n",
      "          ===============   ========================================\n",
      "          metric            Function\n",
      "          ===============   ========================================\n",
      "          'additive_chi2'   sklearn.pairwise.additive_chi2_kernel\n",
      "          'chi2'            sklearn.pairwise.chi2_kernel\n",
      "          'linear'          sklearn.pairwise.linear_kernel\n",
      "          'poly'            sklearn.pairwise.polynomial_kernel\n",
      "          'polynomial'      sklearn.pairwise.polynomial_kernel\n",
      "          'rbf'             sklearn.pairwise.rbf_kernel\n",
      "          'laplacian'       sklearn.pairwise.laplacian_kernel\n",
      "          'sigmoid'         sklearn.pairwise.sigmoid_kernel\n",
      "          'cosine'          sklearn.pairwise.cosine_similarity\n",
      "          ===============   ========================================\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "    \n",
      "    laplacian_kernel(X, Y=None, gamma=None)\n",
      "        Compute the laplacian kernel between X and Y.\n",
      "        \n",
      "        The laplacian kernel is defined as::\n",
      "        \n",
      "            K(x, y) = exp(-gamma ||x-y||_1)\n",
      "        \n",
      "        for each pair of rows x in X and y in Y.\n",
      "        Read more in the :ref:`User Guide <laplacian_kernel>`.\n",
      "        \n",
      "        .. versionadded:: 0.17\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array of shape (n_samples_X, n_features)\n",
      "        \n",
      "        Y : array of shape (n_samples_Y, n_features)\n",
      "        \n",
      "        gamma : float, default None\n",
      "            If None, defaults to 1.0 / n_features\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kernel_matrix : array of shape (n_samples_X, n_samples_Y)\n",
      "    \n",
      "    linear_kernel(X, Y=None, dense_output=True)\n",
      "        Compute the linear kernel between X and Y.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <linear_kernel>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array of shape (n_samples_1, n_features)\n",
      "        \n",
      "        Y : array of shape (n_samples_2, n_features)\n",
      "        \n",
      "        dense_output : boolean (optional), default True\n",
      "            Whether to return dense output even when the input is sparse. If\n",
      "            ``False``, the output is sparse if both input arrays are sparse.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Gram matrix : array of shape (n_samples_1, n_samples_2)\n",
      "    \n",
      "    manhattan_distances(X, Y=None, *, sum_over_features=True)\n",
      "        Compute the L1 distances between the vectors in X and Y.\n",
      "        \n",
      "        With sum_over_features equal to False it returns the componentwise\n",
      "        distances.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array_like\n",
      "            An array with shape (n_samples_X, n_features).\n",
      "        \n",
      "        Y : array_like, optional\n",
      "            An array with shape (n_samples_Y, n_features).\n",
      "        \n",
      "        sum_over_features : bool, default=True\n",
      "            If True the function returns the pairwise distance matrix\n",
      "            else it returns the componentwise L1 pairwise-distances.\n",
      "            Not supported for sparse matrix inputs.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        D : array\n",
      "            If sum_over_features is False shape is\n",
      "            (n_samples_X * n_samples_Y, n_features) and D contains the\n",
      "            componentwise L1 pairwise-distances (ie. absolute difference),\n",
      "            else shape is (n_samples_X, n_samples_Y) and D contains\n",
      "            the pairwise L1 distances.\n",
      "        \n",
      "        Notes\n",
      "        --------\n",
      "        When X and/or Y are CSR sparse matrices and they are not already\n",
      "        in canonical format, this function modifies them in-place to\n",
      "        make them canonical.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import manhattan_distances\n",
      "        >>> manhattan_distances([[3]], [[3]])\n",
      "        array([[0.]])\n",
      "        >>> manhattan_distances([[3]], [[2]])\n",
      "        array([[1.]])\n",
      "        >>> manhattan_distances([[2]], [[3]])\n",
      "        array([[1.]])\n",
      "        >>> manhattan_distances([[1, 2], [3, 4]],         [[1, 2], [0, 3]])\n",
      "        array([[0., 2.],\n",
      "               [4., 4.]])\n",
      "        >>> import numpy as np\n",
      "        >>> X = np.ones((1, 2))\n",
      "        >>> y = np.full((2, 2), 2.)\n",
      "        >>> manhattan_distances(X, y, sum_over_features=False)\n",
      "        array([[1., 1.],\n",
      "               [1., 1.]])\n",
      "    \n",
      "    nan_euclidean_distances(X, Y=None, *, squared=False, missing_values=nan, copy=True)\n",
      "        Calculate the euclidean distances in the presence of missing values.\n",
      "        \n",
      "        Compute the euclidean distance between each pair of samples in X and Y,\n",
      "        where Y=X is assumed if Y=None. When calculating the distance between a\n",
      "        pair of samples, this formulation ignores feature coordinates with a\n",
      "        missing value in either sample and scales up the weight of the remaining\n",
      "        coordinates:\n",
      "        \n",
      "            dist(x,y) = sqrt(weight * sq. distance from present coordinates)\n",
      "            where,\n",
      "            weight = Total # of coordinates / # of present coordinates\n",
      "        \n",
      "        For example, the distance between ``[3, na, na, 6]`` and ``[1, na, 4, 5]``\n",
      "        is:\n",
      "        \n",
      "            .. math::\n",
      "                \\sqrt{\\frac{4}{2}((3-1)^2 + (6-5)^2)}\n",
      "        \n",
      "        If all the coordinates are missing or if there are no common present\n",
      "        coordinates then NaN is returned for that pair.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        .. versionadded:: 0.22\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like, shape=(n_samples_1, n_features)\n",
      "        \n",
      "        Y : array-like, shape=(n_samples_2, n_features)\n",
      "        \n",
      "        squared : bool, default=False\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        missing_values : np.nan or int, default=np.nan\n",
      "            Representation of missing value\n",
      "        \n",
      "        copy : boolean, default=True\n",
      "            Make and use a deep copy of X and Y (if Y exists)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : array, shape (n_samples_1, n_samples_2)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import nan_euclidean_distances\n",
      "        >>> nan = float(\"NaN\")\n",
      "        >>> X = [[0, 1], [1, nan]]\n",
      "        >>> nan_euclidean_distances(X, X) # distance between rows of X\n",
      "        array([[0.        , 1.41421356],\n",
      "               [1.41421356, 0.        ]])\n",
      "        \n",
      "        >>> # get distance to origin\n",
      "        >>> nan_euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * John K. Dixon, \"Pattern Recognition with Partly Missing Data\",\n",
      "          IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue:\n",
      "          10, pp. 617 - 621, Oct. 1979.\n",
      "          http://ieeexplore.ieee.org/abstract/document/4310090/\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        paired_distances : distances between pairs of elements of X and Y.\n",
      "    \n",
      "    paired_cosine_distances(X, Y)\n",
      "        Computes the paired cosine distances between X and Y\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like, shape (n_samples, n_features)\n",
      "        \n",
      "        Y : array-like, shape (n_samples, n_features)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray, shape (n_samples, )\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The cosine distance is equivalent to the half the squared\n",
      "        euclidean distance if each sample is normalized to unit norm\n",
      "    \n",
      "    paired_distances(X, Y, *, metric='euclidean', **kwds)\n",
      "        Computes the paired distances between X and Y.\n",
      "        \n",
      "        Computes the distances between (X[0], Y[0]), (X[1], Y[1]), etc...\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray (n_samples, n_features)\n",
      "            Array 1 for distance computation.\n",
      "        \n",
      "        Y : ndarray (n_samples, n_features)\n",
      "            Array 2 for distance computation.\n",
      "        \n",
      "        metric : string or callable\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            specified in PAIRED_DISTANCES, including \"euclidean\",\n",
      "            \"manhattan\", or \"cosine\".\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray (n_samples, )\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import paired_distances\n",
      "        >>> X = [[0, 1], [1, 1]]\n",
      "        >>> Y = [[0, 1], [2, 1]]\n",
      "        >>> paired_distances(X, Y)\n",
      "        array([0., 1.])\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        pairwise_distances : Computes the distance between every pair of samples\n",
      "    \n",
      "    paired_euclidean_distances(X, Y)\n",
      "        Computes the paired euclidean distances between X and Y\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like, shape (n_samples, n_features)\n",
      "        \n",
      "        Y : array-like, shape (n_samples, n_features)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray (n_samples, )\n",
      "    \n",
      "    paired_manhattan_distances(X, Y)\n",
      "        Compute the L1 distances between the vectors in X and Y.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like, shape (n_samples, n_features)\n",
      "        \n",
      "        Y : array-like, shape (n_samples, n_features)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray (n_samples, )\n",
      "    \n",
      "    pairwise_distances(X, Y=None, metric='euclidean', *, n_jobs=None, force_all_finite=True, **kwds)\n",
      "        Compute the distance matrix from a vector array X and optional Y.\n",
      "        \n",
      "        This method takes either a vector array or a distance matrix, and returns\n",
      "        a distance matrix. If the input is a vector array, the distances are\n",
      "        computed. If the input is a distances matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a distance matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        distance between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "        \n",
      "        - From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "          'manhattan']. These metrics support sparse matrix\n",
      "          inputs.\n",
      "          ['nan_euclidean'] but it does not yet support sparse matrices.\n",
      "        \n",
      "        - From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',\n",
      "          'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n",
      "          'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n",
      "          See the documentation for scipy.spatial.distance for details on these\n",
      "          metrics. These metrics do not support sparse matrix inputs.\n",
      "        \n",
      "        Note that in the case of 'cityblock', 'cosine' and 'euclidean' (which are\n",
      "        valid scipy.spatial.distance metrics), the scikit-learn implementation\n",
      "        will be used, which is faster and has support for sparse matrices (except\n",
      "        for 'cityblock'). For a verbose description of the metrics from\n",
      "        scikit-learn, see the __doc__ of the sklearn.pairwise.distance_metrics\n",
      "        function.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,              [n_samples_a, n_features] otherwise\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        Y : array [n_samples_b, n_features], optional\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      "            a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int or None, optional (default=None)\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        force_all_finite : boolean or 'allow-nan', (default=True)\n",
      "            Whether to raise an error on np.inf, np.nan, pd.NA in array. The\n",
      "            possibilities are:\n",
      "        \n",
      "            - True: Force all values of array to be finite.\n",
      "            - False: accepts np.inf, np.nan, pd.NA in array.\n",
      "            - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n",
      "              cannot be infinite.\n",
      "        \n",
      "            .. versionadded:: 0.22\n",
      "               ``force_all_finite`` accepts the string ``'allow-nan'``.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Accepts `pd.NA` and converts it into `np.nan`\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        D : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b]\n",
      "            A distance matrix D such that D_{i, j} is the distance between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then D_{i, j} is the distance between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        pairwise_distances_chunked : performs the same calculation as this\n",
      "            function, but returns a generator of chunks of the distance matrix, in\n",
      "            order to limit memory usage.\n",
      "        paired_distances : Computes the distances between corresponding\n",
      "                           elements of two arrays\n",
      "    \n",
      "    pairwise_distances_argmin(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance).\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        This function works with dense 2D arrays only.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like\n",
      "            Arrays containing points. Respective shapes (n_samples1, n_features)\n",
      "            and (n_samples2, n_features)\n",
      "        \n",
      "        Y : array-like\n",
      "            Arrays containing points. Respective shapes (n_samples1, n_features)\n",
      "            and (n_samples2, n_features)\n",
      "        \n",
      "        axis : int, optional, default 1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : string or callable\n",
      "            metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        metric_kwargs : dict\n",
      "            keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : numpy.ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        sklearn.metrics.pairwise_distances\n",
      "        sklearn.metrics.pairwise_distances_argmin_min\n",
      "    \n",
      "    pairwise_distances_argmin_min(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance). The minimal distances are\n",
      "        also returned.\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            (pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),\n",
      "             pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape (n_samples1, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : {array-like, sparse matrix}, shape (n_samples2, n_features)\n",
      "            Arrays containing points.\n",
      "        \n",
      "        axis : int, optional, default 1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : string or callable, default 'euclidean'\n",
      "            metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        metric_kwargs : dict, optional\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : numpy.ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        distances : numpy.ndarray\n",
      "            distances[i] is the distance between the i-th row in X and the\n",
      "            argmin[i]-th row in Y.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        sklearn.metrics.pairwise_distances\n",
      "        sklearn.metrics.pairwise_distances_argmin\n",
      "    \n",
      "    pairwise_distances_chunked(X, Y=None, *, reduce_func=None, metric='euclidean', n_jobs=None, working_memory=None, **kwds)\n",
      "        Generate a distance matrix chunk by chunk with optional reduction\n",
      "        \n",
      "        In cases where not all of a pairwise distance matrix needs to be stored at\n",
      "        once, this is used to calculate pairwise distances in\n",
      "        ``working_memory``-sized chunks.  If ``reduce_func`` is given, it is run\n",
      "        on each chunk and its return values are concatenated into lists, arrays\n",
      "        or sparse matrices.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,\n",
      "            [n_samples_a, n_features] otherwise\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        Y : array [n_samples_b, n_features], optional\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        reduce_func : callable, optional\n",
      "            The function which is applied on each chunk of the distance matrix,\n",
      "            reducing it to needed values.  ``reduce_func(D_chunk, start)``\n",
      "            is called repeatedly, where ``D_chunk`` is a contiguous vertical\n",
      "            slice of the pairwise distance matrix, starting at row ``start``.\n",
      "            It should return one of: None; an array, a list, or a sparse matrix\n",
      "            of length ``D_chunk.shape[0]``; or a tuple of such objects. Returning\n",
      "            None is useful for in-place operations, rather than reductions.\n",
      "        \n",
      "            If None, pairwise_distances_chunked returns a generator of vertical\n",
      "            chunks of the distance matrix.\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      "            a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int or None, optional (default=None)\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        working_memory : int, optional\n",
      "            The sought maximum memory for temporary distance matrix chunks.\n",
      "            When None (default), the value of\n",
      "            ``sklearn.get_config()['working_memory']`` is used.\n",
      "        \n",
      "        `**kwds` : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Yields\n",
      "        ------\n",
      "        D_chunk : array or sparse matrix\n",
      "            A contiguous slice of distance matrix, optionally processed by\n",
      "            ``reduce_func``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Without reduce_func:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import pairwise_distances_chunked\n",
      "        >>> X = np.random.RandomState(0).rand(5, 3)\n",
      "        >>> D_chunk = next(pairwise_distances_chunked(X))\n",
      "        >>> D_chunk\n",
      "        array([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],\n",
      "               [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],\n",
      "               [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],\n",
      "               [0.19..., 0.41..., 0.44..., 0.  ..., 0.51...],\n",
      "               [0.57..., 0.76..., 0.90..., 0.51..., 0.  ...]])\n",
      "        \n",
      "        Retrieve all neighbors and average distance within radius r:\n",
      "        \n",
      "        >>> r = .2\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r) for d in D_chunk]\n",
      "        ...     avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n",
      "        ...     return neigh, avg_dist\n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n",
      "        >>> neigh, avg_dist = next(gen)\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]\n",
      "        >>> avg_dist\n",
      "        array([0.039..., 0.        , 0.        , 0.039..., 0.        ])\n",
      "        \n",
      "        Where r is defined per sample, we need to make use of ``start``:\n",
      "        \n",
      "        >>> r = [.2, .4, .4, .3, .1]\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r[i])\n",
      "        ...              for i, d in enumerate(D_chunk, start)]\n",
      "        ...     return neigh\n",
      "        >>> neigh = next(pairwise_distances_chunked(X, reduce_func=reduce_func))\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]\n",
      "        \n",
      "        Force row-by-row generation by reducing ``working_memory``:\n",
      "        \n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func,\n",
      "        ...                                  working_memory=0)\n",
      "        >>> next(gen)\n",
      "        [array([0, 3])]\n",
      "        >>> next(gen)\n",
      "        [array([0, 1])]\n",
      "    \n",
      "    pairwise_kernels(X, Y=None, metric='linear', *, filter_params=False, n_jobs=None, **kwds)\n",
      "        Compute the kernel between arrays X and optional array Y.\n",
      "        \n",
      "        This method takes either a vector array or a kernel matrix, and returns\n",
      "        a kernel matrix. If the input is a vector array, the kernels are\n",
      "        computed. If the input is a kernel matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a kernel matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        kernel between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "            ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',\n",
      "            'laplacian', 'sigmoid', 'cosine']\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,              [n_samples_a, n_features] otherwise\n",
      "            Array of pairwise kernels between samples, or a feature array.\n",
      "        \n",
      "        Y : array [n_samples_b, n_features]\n",
      "            A second feature array only if X has shape [n_samples_a, n_features].\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating kernel between instances in a\n",
      "            feature array. If metric is a string, it must be one of the metrics\n",
      "            in pairwise.PAIRWISE_KERNEL_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a kernel matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two rows from X as input and return the corresponding\n",
      "            kernel value as a single number. This means that callables from\n",
      "            :mod:`sklearn.metrics.pairwise` are not allowed, as they operate on\n",
      "            matrices, not single samples. Use the string identifying the kernel\n",
      "            instead.\n",
      "        \n",
      "        filter_params : boolean\n",
      "            Whether to filter invalid parameters or not.\n",
      "        \n",
      "        n_jobs : int or None, optional (default=None)\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the kernel function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b]\n",
      "            A kernel matrix K such that K_{i, j} is the kernel between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then K_{i, j} is the kernel between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If metric is 'precomputed', Y is ignored and X is returned.\n",
      "    \n",
      "    polynomial_kernel(X, Y=None, degree=3, gamma=None, coef0=1)\n",
      "        Compute the polynomial kernel between X and Y::\n",
      "        \n",
      "            K(X, Y) = (gamma <X, Y> + coef0)^degree\n",
      "        \n",
      "        Read more in the :ref:`User Guide <polynomial_kernel>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_1, n_features)\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_2, n_features)\n",
      "        \n",
      "        degree : int, default 3\n",
      "        \n",
      "        gamma : float, default None\n",
      "            if None, defaults to 1.0 / n_features\n",
      "        \n",
      "        coef0 : float, default 1\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Gram matrix : array of shape (n_samples_1, n_samples_2)\n",
      "    \n",
      "    rbf_kernel(X, Y=None, gamma=None)\n",
      "        Compute the rbf (gaussian) kernel between X and Y::\n",
      "        \n",
      "            K(x, y) = exp(-gamma ||x-y||^2)\n",
      "        \n",
      "        for each pair of rows x in X and y in Y.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <rbf_kernel>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array of shape (n_samples_X, n_features)\n",
      "        \n",
      "        Y : array of shape (n_samples_Y, n_features)\n",
      "        \n",
      "        gamma : float, default None\n",
      "            If None, defaults to 1.0 / n_features\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kernel_matrix : array of shape (n_samples_X, n_samples_Y)\n",
      "    \n",
      "    sigmoid_kernel(X, Y=None, gamma=None, coef0=1)\n",
      "        Compute the sigmoid kernel between X and Y::\n",
      "        \n",
      "            K(X, Y) = tanh(gamma <X, Y> + coef0)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sigmoid_kernel>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_1, n_features)\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_2, n_features)\n",
      "        \n",
      "        gamma : float, default None\n",
      "            If None, defaults to 1.0 / n_features\n",
      "        \n",
      "        coef0 : float, default 1\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Gram matrix : array of shape (n_samples_1, n_samples_2)\n",
      "\n",
      "DATA\n",
      "    KERNEL_PARAMS = {'additive_chi2': (), 'chi2': frozenset({'gamma'}), 'c...\n",
      "    PAIRED_DISTANCES = {'cityblock': <function paired_manhattan_distances>...\n",
      "    PAIRWISE_BOOLEAN_FUNCTIONS = ['dice', 'jaccard', 'kulsinski', 'matchin...\n",
      "    PAIRWISE_DISTANCE_FUNCTIONS = {'cityblock': <function manhattan_distan...\n",
      "    PAIRWISE_KERNEL_FUNCTIONS = {'additive_chi2': <function additive_chi2_...\n",
      "    sp_version = <Version('1.5.2')>\n",
      "\n",
      "FILE\n",
      "    c:\\anaconda\\envs\\textanalytics\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.metrics.pairwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance\n",
    "\n",
    "['braycurtis', 'canberra', 'chebyshev',\n",
    "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
    "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
    "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
    "              'yule']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "more\n",
    "TextDistance\n",
    "https://pypi.org/project/textdistance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textdistance\n",
      "  Downloading textdistance-4.2.1-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: textdistance\n",
      "Successfully installed textdistance-4.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package textdistance:\n",
      "\n",
      "NAME\n",
      "    textdistance\n",
      "\n",
      "DESCRIPTION\n",
      "    TextDistance.\n",
      "    Compute distance between sequences.\n",
      "    30+ algorithms, pure python implementation, common interface.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    algorithms (package)\n",
      "    benchmark\n",
      "    libraries\n",
      "    utils\n",
      "\n",
      "SUBMODULES\n",
      "    base\n",
      "    compression_based\n",
      "    edit_based\n",
      "    phonetic\n",
      "    sequence_based\n",
      "    simple\n",
      "    token_based\n",
      "\n",
      "DATA\n",
      "    VERSION = '4.2.1'\n",
      "    __license__ = 'MIT'\n",
      "    __title__ = 'TextDistance'\n",
      "    arith_ncd = ArithNCD({'base': 2, 'terminator': None, 'qval': 1})\n",
      "    bag = Bag({'qval': 1, 'external': True})\n",
      "    bwtrle_ncd = BWTRLENCD({'terminator': '\\x00'})\n",
      "    bz2_ncd = BZ2NCD({})\n",
      "    cosine = Cosine({'qval': 1, 'as_set': False, 'external': True})\n",
      "    damerau_levenshtein = DamerauLevenshtein({'qval': 1, 'test_func': <fun...\n",
      "    editex = Editex({'match_cost': 0, 'group_cost': 1, 'misma..., 'X', 'Y'...\n",
      "    entropy_ncd = EntropyNCD({'qval': 1, 'coef': 1, 'base': 2})\n",
      "    gotoh = Gotoh({'qval': 1, 'gap_open': 1, 'gap_ext': 0.4,..._ident at 0...\n",
      "    hamming = Hamming({'qval': 1, 'test_func': <function Base....BF8D83E50...\n",
      "    identity = Identity({'qval': 1, 'external': True})\n",
      "    jaccard = Jaccard({'qval': 1, 'as_set': False, 'external': True})\n",
      "    jaro = Jaro({'qval': 1, 'long_tolerance': False, 'winklerize': False, ...\n",
      "    jaro_winkler = JaroWinkler({'qval': 1, 'long_tolerance': False, 'winkl...\n",
      "    lcsseq = LCSSeq({'qval': 1, 'test_func': <function Base._ident at 0x00...\n",
      "    lcsstr = LCSStr({'qval': 1, 'external': True})\n",
      "    length = Length({'qval': 1, 'external': True})\n",
      "    levenshtein = Levenshtein({'qval': 1, 'test_func': <function B..._iden...\n",
      "    lzma_ncd = LZMANCD({})\n",
      "    matrix = Matrix({'mat': None, 'mismatch_cost': 0, 'match_cost': 1, 'sy...\n",
      "    mlipns = MLIPNS({'qval': 1, 'threshold': 0.25, 'maxmismatches': 2, 'ex...\n",
      "    monge_elkan = MongeElkan({'algorithm': DamerauLevenshtein({'qv...'symm...\n",
      "    mra = MRA({'qval': 1, 'external': True})\n",
      "    needleman_wunsch = NeedlemanWunsch({'qval': 1, 'gap_cost': 1.0, 'si......\n",
      "    overlap = Overlap({'qval': 1, 'as_set': False, 'external': True})\n",
      "    postfix = Postfix({'qval': 1, 'sim_test': <function Base._ident at 0x0...\n",
      "    prefix = Prefix({'qval': 1, 'sim_test': <function Base._ident at 0x000...\n",
      "    ratcliff_obershelp = RatcliffObershelp({'qval': 1, 'external': True})\n",
      "    rle_ncd = RLENCD({'qval': 1})\n",
      "    smith_waterman = SmithWaterman({'qval': 1, 'gap_cost': 1.0, 'sim_..._i...\n",
      "    sorensen = Sorensen({'qval': 1, 'as_set': False, 'external': True})\n",
      "    sorensen_dice = Sorensen({'qval': 1, 'as_set': False, 'external': True...\n",
      "    sqrt_ncd = SqrtNCD({'qval': 1})\n",
      "    strcmp95 = StrCmp95({'long_strings': False, 'external': True})\n",
      "    tanimoto = Tanimoto({'qval': 1, 'as_set': False, 'external': True})\n",
      "    tversky = Tversky({'qval': 1, 'ks': repeat(1), 'bias': None, 'as_set':...\n",
      "    zlib_ncd = ZLIBNCD({})\n",
      "\n",
      "VERSION\n",
      "    4.2.1\n",
      "\n",
      "AUTHOR\n",
      "    Gram (@orsinium)\n",
      "\n",
      "FILE\n",
      "    c:\\anaconda\\envs\\textanalytics\\lib\\site-packages\\textdistance\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(textdistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-Levenshtein\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "from sklearn.cluster import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.cluster in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.cluster\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.cluster` module gathers popular unsupervised clustering\n",
      "    algorithms.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _affinity_propagation\n",
      "    _agglomerative\n",
      "    _bicluster\n",
      "    _birch\n",
      "    _dbscan\n",
      "    _dbscan_inner\n",
      "    _feature_agglomeration\n",
      "    _hierarchical_fast\n",
      "    _k_means_elkan\n",
      "    _k_means_fast\n",
      "    _k_means_lloyd\n",
      "    _kmeans\n",
      "    _mean_shift\n",
      "    _optics\n",
      "    _spectral\n",
      "    affinity_propagation_\n",
      "    bicluster\n",
      "    birch\n",
      "    dbscan_\n",
      "    hierarchical\n",
      "    k_means_\n",
      "    mean_shift_\n",
      "    optics_\n",
      "    setup\n",
      "    spectral\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        sklearn.cluster._affinity_propagation.AffinityPropagation(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._agglomerative.AgglomerativeClustering(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.cluster._agglomerative.FeatureAgglomeration(sklearn.cluster._agglomerative.AgglomerativeClustering, sklearn.cluster._feature_agglomeration.AgglomerationTransform)\n",
      "        sklearn.cluster._birch.Birch(sklearn.base.ClusterMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._dbscan.DBSCAN(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._kmeans.KMeans(sklearn.base.TransformerMixin, sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.cluster._kmeans.MiniBatchKMeans\n",
      "        sklearn.cluster._mean_shift.MeanShift(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._optics.OPTICS(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._spectral.SpectralClustering(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.ClusterMixin(builtins.object)\n",
      "        sklearn.cluster._affinity_propagation.AffinityPropagation(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._agglomerative.AgglomerativeClustering(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.cluster._agglomerative.FeatureAgglomeration(sklearn.cluster._agglomerative.AgglomerativeClustering, sklearn.cluster._feature_agglomeration.AgglomerationTransform)\n",
      "        sklearn.cluster._birch.Birch(sklearn.base.ClusterMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._dbscan.DBSCAN(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._kmeans.KMeans(sklearn.base.TransformerMixin, sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.cluster._kmeans.MiniBatchKMeans\n",
      "        sklearn.cluster._mean_shift.MeanShift(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._optics.OPTICS(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._spectral.SpectralClustering(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.TransformerMixin(builtins.object)\n",
      "        sklearn.cluster._birch.Birch(sklearn.base.ClusterMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._kmeans.KMeans(sklearn.base.TransformerMixin, sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.cluster._kmeans.MiniBatchKMeans\n",
      "    sklearn.cluster._bicluster.BaseSpectral(sklearn.base.BiclusterMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.cluster._bicluster.SpectralBiclustering\n",
      "        sklearn.cluster._bicluster.SpectralCoclustering\n",
      "    \n",
      "    class AffinityPropagation(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "     |  AffinityPropagation(*, damping=0.5, max_iter=200, convergence_iter=15, copy=True, preference=None, affinity='euclidean', verbose=False, random_state='warn')\n",
      "     |  \n",
      "     |  Perform Affinity Propagation Clustering of data.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <affinity_propagation>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  damping : float, default=0.5\n",
      "     |      Damping factor (between 0.5 and 1) is the extent to\n",
      "     |      which the current value is maintained relative to\n",
      "     |      incoming values (weighted 1 - damping). This in order\n",
      "     |      to avoid numerical oscillations when updating these\n",
      "     |      values (messages).\n",
      "     |  \n",
      "     |  max_iter : int, default=200\n",
      "     |      Maximum number of iterations.\n",
      "     |  \n",
      "     |  convergence_iter : int, default=15\n",
      "     |      Number of iterations with no change in the number\n",
      "     |      of estimated clusters that stops the convergence.\n",
      "     |  \n",
      "     |  copy : bool, default=True\n",
      "     |      Make a copy of input data.\n",
      "     |  \n",
      "     |  preference : array-like of shape (n_samples,) or float, default=None\n",
      "     |      Preferences for each point - points with larger values of\n",
      "     |      preferences are more likely to be chosen as exemplars. The number\n",
      "     |      of exemplars, ie of clusters, is influenced by the input\n",
      "     |      preferences value. If the preferences are not passed as arguments,\n",
      "     |      they will be set to the median of the input similarities.\n",
      "     |  \n",
      "     |  affinity : {'euclidean', 'precomputed'}, default='euclidean'\n",
      "     |      Which affinity to use. At the moment 'precomputed' and\n",
      "     |      ``euclidean`` are supported. 'euclidean' uses the\n",
      "     |      negative squared euclidean distance between points.\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Whether to be verbose.\n",
      "     |  \n",
      "     |  random_state : int or np.random.RandomStateInstance, default: 0\n",
      "     |      Pseudo-random number generator to control the starting state.\n",
      "     |      Use an int for reproducible results across function calls.\n",
      "     |      See the :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |          this parameter was previously hardcoded as 0.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cluster_centers_indices_ : ndarray of shape (n_clusters,)\n",
      "     |      Indices of cluster centers\n",
      "     |  \n",
      "     |  cluster_centers_ : ndarray of shape (n_clusters, n_features)\n",
      "     |      Cluster centers (if affinity != ``precomputed``).\n",
      "     |  \n",
      "     |  labels_ : ndarray of shape (n_samples,)\n",
      "     |      Labels of each point\n",
      "     |  \n",
      "     |  affinity_matrix_ : ndarray of shape (n_samples, n_samples)\n",
      "     |      Stores the affinity matrix used in ``fit``.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations taken to converge.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n",
      "     |  <sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n",
      "     |  \n",
      "     |  The algorithmic complexity of affinity propagation is quadratic\n",
      "     |  in the number of points.\n",
      "     |  \n",
      "     |  When ``fit`` does not converge, ``cluster_centers_`` becomes an empty\n",
      "     |  array and all training samples will be labelled as ``-1``. In addition,\n",
      "     |  ``predict`` will then label every sample as ``-1``.\n",
      "     |  \n",
      "     |  When all training samples have equal similarities and equal preferences,\n",
      "     |  the assignment of cluster centers and labels depends on the preference.\n",
      "     |  If the preference is smaller than the similarities, ``fit`` will result in\n",
      "     |  a single cluster center and label ``0`` for every sample. Otherwise, every\n",
      "     |  training sample becomes its own cluster center and is assigned a unique\n",
      "     |  label.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n",
      "     |  Between Data Points\", Science Feb. 2007\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import AffinityPropagation\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
      "     |  ...               [4, 2], [4, 4], [4, 0]])\n",
      "     |  >>> clustering = AffinityPropagation(random_state=5).fit(X)\n",
      "     |  >>> clustering\n",
      "     |  AffinityPropagation(random_state=5)\n",
      "     |  >>> clustering.labels_\n",
      "     |  array([0, 0, 0, 1, 1, 1])\n",
      "     |  >>> clustering.predict([[0, 0], [4, 4]])\n",
      "     |  array([0, 1])\n",
      "     |  >>> clustering.cluster_centers_\n",
      "     |  array([[1, 2],\n",
      "     |         [4, 2]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AffinityPropagation\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, damping=0.5, max_iter=200, convergence_iter=15, copy=True, preference=None, affinity='euclidean', verbose=False, random_state='warn')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit the clustering from features, or affinity matrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features), or             array-like, shape (n_samples, n_samples)\n",
      "     |          Training instances to cluster, or similarities / affinities between\n",
      "     |          instances if ``affinity='precomputed'``. If a sparse feature matrix\n",
      "     |          is provided, it will be converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None)\n",
      "     |      Fit the clustering from features or affinity matrix, and return\n",
      "     |      cluster labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features), or             array-like, shape (n_samples, n_samples)\n",
      "     |          Training instances to cluster, or similarities / affinities between\n",
      "     |          instances if ``affinity='precomputed'``. If a sparse feature matrix\n",
      "     |          is provided, it will be converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray, shape (n_samples,)\n",
      "     |          Cluster labels.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict the closest cluster each sample in X belongs to.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          New data to predict. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray, shape (n_samples,)\n",
      "     |          Cluster labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class AgglomerativeClustering(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "     |  AgglomerativeClustering(n_clusters=2, *, affinity='euclidean', memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', distance_threshold=None)\n",
      "     |  \n",
      "     |  Agglomerative Clustering\n",
      "     |  \n",
      "     |  Recursively merges the pair of clusters that minimally increases\n",
      "     |  a given linkage distance.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <hierarchical_clustering>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_clusters : int or None, default=2\n",
      "     |      The number of clusters to find. It must be ``None`` if\n",
      "     |      ``distance_threshold`` is not ``None``.\n",
      "     |  \n",
      "     |  affinity : str or callable, default='euclidean'\n",
      "     |      Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n",
      "     |      \"manhattan\", \"cosine\", or \"precomputed\".\n",
      "     |      If linkage is \"ward\", only \"euclidean\" is accepted.\n",
      "     |      If \"precomputed\", a distance matrix (instead of a similarity matrix)\n",
      "     |      is needed as input for the fit method.\n",
      "     |  \n",
      "     |  memory : str or object with the joblib.Memory interface, default=None\n",
      "     |      Used to cache the output of the computation of the tree.\n",
      "     |      By default, no caching is done. If a string is given, it is the\n",
      "     |      path to the caching directory.\n",
      "     |  \n",
      "     |  connectivity : array-like or callable, default=None\n",
      "     |      Connectivity matrix. Defines for each sample the neighboring\n",
      "     |      samples following a given structure of the data.\n",
      "     |      This can be a connectivity matrix itself or a callable that transforms\n",
      "     |      the data into a connectivity matrix, such as derived from\n",
      "     |      kneighbors_graph. Default is None, i.e, the\n",
      "     |      hierarchical clustering algorithm is unstructured.\n",
      "     |  \n",
      "     |  compute_full_tree : 'auto' or bool, default='auto'\n",
      "     |      Stop early the construction of the tree at n_clusters. This is useful\n",
      "     |      to decrease computation time if the number of clusters is not small\n",
      "     |      compared to the number of samples. This option is useful only when\n",
      "     |      specifying a connectivity matrix. Note also that when varying the\n",
      "     |      number of clusters and using caching, it may be advantageous to compute\n",
      "     |      the full tree. It must be ``True`` if ``distance_threshold`` is not\n",
      "     |      ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n",
      "     |      to `True` when `distance_threshold` is not `None` or that `n_clusters`\n",
      "     |      is inferior to the maximum between 100 or `0.02 * n_samples`.\n",
      "     |      Otherwise, \"auto\" is equivalent to `False`.\n",
      "     |  \n",
      "     |  linkage : {\"ward\", \"complete\", \"average\", \"single\"}, default=\"ward\"\n",
      "     |      Which linkage criterion to use. The linkage criterion determines which\n",
      "     |      distance to use between sets of observation. The algorithm will merge\n",
      "     |      the pairs of cluster that minimize this criterion.\n",
      "     |  \n",
      "     |      - ward minimizes the variance of the clusters being merged.\n",
      "     |      - average uses the average of the distances of each observation of\n",
      "     |        the two sets.\n",
      "     |      - complete or maximum linkage uses the maximum distances between\n",
      "     |        all observations of the two sets.\n",
      "     |      - single uses the minimum of the distances between all observations\n",
      "     |        of the two sets.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added the 'single' option\n",
      "     |  \n",
      "     |  distance_threshold : float, default=None\n",
      "     |      The linkage distance threshold above which, clusters will not be\n",
      "     |      merged. If not ``None``, ``n_clusters`` must be ``None`` and\n",
      "     |      ``compute_full_tree`` must be ``True``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.21\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_clusters_ : int\n",
      "     |      The number of clusters found by the algorithm. If\n",
      "     |      ``distance_threshold=None``, it will be equal to the given\n",
      "     |      ``n_clusters``.\n",
      "     |  \n",
      "     |  labels_ : ndarray of shape (n_samples)\n",
      "     |      cluster labels for each point\n",
      "     |  \n",
      "     |  n_leaves_ : int\n",
      "     |      Number of leaves in the hierarchical tree.\n",
      "     |  \n",
      "     |  n_connected_components_ : int\n",
      "     |      The estimated number of connected components in the graph.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.21\n",
      "     |          ``n_connected_components_`` was added to replace ``n_components_``.\n",
      "     |  \n",
      "     |  children_ : array-like of shape (n_samples-1, 2)\n",
      "     |      The children of each non-leaf node. Values less than `n_samples`\n",
      "     |      correspond to leaves of the tree which are the original samples.\n",
      "     |      A node `i` greater than or equal to `n_samples` is a non-leaf\n",
      "     |      node and has children `children_[i - n_samples]`. Alternatively\n",
      "     |      at the i-th iteration, children[i][0] and children[i][1]\n",
      "     |      are merged to form node `n_samples + i`\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import AgglomerativeClustering\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
      "     |  ...               [4, 2], [4, 4], [4, 0]])\n",
      "     |  >>> clustering = AgglomerativeClustering().fit(X)\n",
      "     |  >>> clustering\n",
      "     |  AgglomerativeClustering()\n",
      "     |  >>> clustering.labels_\n",
      "     |  array([1, 1, 1, 0, 0, 0])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AgglomerativeClustering\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_clusters=2, *, affinity='euclidean', memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', distance_threshold=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit the hierarchical clustering from features, or distance matrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features) or (n_samples, n_samples)\n",
      "     |          Training instances to cluster, or distances between instances if\n",
      "     |          ``affinity='precomputed'``.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None)\n",
      "     |      Fit the hierarchical clustering from features or distance matrix,\n",
      "     |      and return cluster labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features) or (n_samples, n_samples)\n",
      "     |          Training instances to cluster, or distances between instances if\n",
      "     |          ``affinity='precomputed'``.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray, shape (n_samples,)\n",
      "     |          Cluster labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Birch(sklearn.base.ClusterMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  Birch(*, threshold=0.5, branching_factor=50, n_clusters=3, compute_labels=True, copy=True)\n",
      "     |  \n",
      "     |  Implements the Birch clustering algorithm.\n",
      "     |  \n",
      "     |  It is a memory-efficient, online-learning algorithm provided as an\n",
      "     |  alternative to :class:`MiniBatchKMeans`. It constructs a tree\n",
      "     |  data structure with the cluster centroids being read off the leaf.\n",
      "     |  These can be either the final cluster centroids or can be provided as input\n",
      "     |  to another clustering algorithm such as :class:`AgglomerativeClustering`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <birch>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.16\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  threshold : float, default=0.5\n",
      "     |      The radius of the subcluster obtained by merging a new sample and the\n",
      "     |      closest subcluster should be lesser than the threshold. Otherwise a new\n",
      "     |      subcluster is started. Setting this value to be very low promotes\n",
      "     |      splitting and vice-versa.\n",
      "     |  \n",
      "     |  branching_factor : int, default=50\n",
      "     |      Maximum number of CF subclusters in each node. If a new samples enters\n",
      "     |      such that the number of subclusters exceed the branching_factor then\n",
      "     |      that node is split into two nodes with the subclusters redistributed\n",
      "     |      in each. The parent subcluster of that node is removed and two new\n",
      "     |      subclusters are added as parents of the 2 split nodes.\n",
      "     |  \n",
      "     |  n_clusters : int, instance of sklearn.cluster model, default=3\n",
      "     |      Number of clusters after the final clustering step, which treats the\n",
      "     |      subclusters from the leaves as new samples.\n",
      "     |  \n",
      "     |      - `None` : the final clustering step is not performed and the\n",
      "     |        subclusters are returned as they are.\n",
      "     |  \n",
      "     |      - :mod:`sklearn.cluster` Estimator : If a model is provided, the model\n",
      "     |        is fit treating the subclusters as new samples and the initial data\n",
      "     |        is mapped to the label of the closest subcluster.\n",
      "     |  \n",
      "     |      - `int` : the model fit is :class:`AgglomerativeClustering` with\n",
      "     |        `n_clusters` set to be equal to the int.\n",
      "     |  \n",
      "     |  compute_labels : bool, default=True\n",
      "     |      Whether or not to compute labels for each fit.\n",
      "     |  \n",
      "     |  copy : bool, default=True\n",
      "     |      Whether or not to make a copy of the given data. If set to False,\n",
      "     |      the initial data will be overwritten.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  root_ : _CFNode\n",
      "     |      Root of the CFTree.\n",
      "     |  \n",
      "     |  dummy_leaf_ : _CFNode\n",
      "     |      Start pointer to all the leaves.\n",
      "     |  \n",
      "     |  subcluster_centers_ : ndarray\n",
      "     |      Centroids of all subclusters read directly from the leaves.\n",
      "     |  \n",
      "     |  subcluster_labels_ : ndarray\n",
      "     |      Labels assigned to the centroids of the subclusters after\n",
      "     |      they are clustered globally.\n",
      "     |  \n",
      "     |  labels_ : ndarray of shape (n_samples,)\n",
      "     |      Array of labels assigned to the input data.\n",
      "     |      if partial_fit is used instead of fit, they are assigned to the\n",
      "     |      last batch of data.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  \n",
      "     |  MiniBatchKMeans\n",
      "     |      Alternative  implementation that does incremental updates\n",
      "     |      of the centers' positions using mini-batches.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The tree data structure consists of nodes with each node consisting of\n",
      "     |  a number of subclusters. The maximum number of subclusters in a node\n",
      "     |  is determined by the branching factor. Each subcluster maintains a\n",
      "     |  linear sum, squared sum and the number of samples in that subcluster.\n",
      "     |  In addition, each subcluster can also have a node as its child, if the\n",
      "     |  subcluster is not a member of a leaf node.\n",
      "     |  \n",
      "     |  For a new point entering the root, it is merged with the subcluster closest\n",
      "     |  to it and the linear sum, squared sum and the number of samples of that\n",
      "     |  subcluster are updated. This is done recursively till the properties of\n",
      "     |  the leaf node are updated.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  * Tian Zhang, Raghu Ramakrishnan, Maron Livny\n",
      "     |    BIRCH: An efficient data clustering method for large databases.\n",
      "     |    https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf\n",
      "     |  \n",
      "     |  * Roberto Perdisci\n",
      "     |    JBirch - Java implementation of BIRCH clustering algorithm\n",
      "     |    https://code.google.com/archive/p/jbirch\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import Birch\n",
      "     |  >>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]\n",
      "     |  >>> brc = Birch(n_clusters=None)\n",
      "     |  >>> brc.fit(X)\n",
      "     |  Birch(n_clusters=None)\n",
      "     |  >>> brc.predict(X)\n",
      "     |  array([0, 0, 0, 1, 1, 1])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Birch\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, threshold=0.5, branching_factor=50, n_clusters=3, compute_labels=True, copy=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Build a CF Tree for the input data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  partial_fit(self, X=None, y=None)\n",
      "     |      Online learning. Prevents rebuilding of CFTree from scratch.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features),             default=None\n",
      "     |          Input data. If X is not provided, only the global clustering\n",
      "     |          step is done.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict data using the ``centroids_`` of subclusters.\n",
      "     |      \n",
      "     |      Avoid computation of the row norms of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape(n_samples,)\n",
      "     |          Labelled data.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform X into subcluster centroids dimension.\n",
      "     |      \n",
      "     |      Each dimension represents the distance from the sample point to each\n",
      "     |      cluster centroid.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_trans : {array-like, sparse matrix} of shape (n_samples, n_clusters)\n",
      "     |          Transformed data.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None)\n",
      "     |      Perform clustering on X and returns cluster labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,)\n",
      "     |          Cluster labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,), default=None\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class DBSCAN(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "     |  DBSCAN(eps=0.5, *, min_samples=5, metric='euclidean', metric_params=None, algorithm='auto', leaf_size=30, p=None, n_jobs=None)\n",
      "     |  \n",
      "     |  Perform DBSCAN clustering from vector array or distance matrix.\n",
      "     |  \n",
      "     |  DBSCAN - Density-Based Spatial Clustering of Applications with Noise.\n",
      "     |  Finds core samples of high density and expands clusters from them.\n",
      "     |  Good for data which contains clusters of similar density.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <dbscan>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  eps : float, default=0.5\n",
      "     |      The maximum distance between two samples for one to be considered\n",
      "     |      as in the neighborhood of the other. This is not a maximum bound\n",
      "     |      on the distances of points within a cluster. This is the most\n",
      "     |      important DBSCAN parameter to choose appropriately for your data set\n",
      "     |      and distance function.\n",
      "     |  \n",
      "     |  min_samples : int, default=5\n",
      "     |      The number of samples (or total weight) in a neighborhood for a point\n",
      "     |      to be considered as a core point. This includes the point itself.\n",
      "     |  \n",
      "     |  metric : string, or callable, default='euclidean'\n",
      "     |      The metric to use when calculating distance between instances in a\n",
      "     |      feature array. If metric is a string or callable, it must be one of\n",
      "     |      the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n",
      "     |      its metric parameter.\n",
      "     |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "     |      must be square. X may be a :term:`Glossary <sparse graph>`, in which\n",
      "     |      case only \"nonzero\" elements may be considered neighbors for DBSCAN.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         metric *precomputed* to accept precomputed sparse matrix.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      The algorithm to be used by the NearestNeighbors module\n",
      "     |      to compute pointwise distances and find nearest neighbors.\n",
      "     |      See NearestNeighbors module documentation for details.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to BallTree or cKDTree. This can affect the speed\n",
      "     |      of the construction and query, as well as the memory required\n",
      "     |      to store the tree. The optimal value depends\n",
      "     |      on the nature of the problem.\n",
      "     |  \n",
      "     |  p : float, default=None\n",
      "     |      The power of the Minkowski metric to be used to calculate distance\n",
      "     |      between points.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of parallel jobs to run.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  core_sample_indices_ : ndarray of shape (n_core_samples,)\n",
      "     |      Indices of core samples.\n",
      "     |  \n",
      "     |  components_ : ndarray of shape (n_core_samples, n_features)\n",
      "     |      Copy of each core sample found by training.\n",
      "     |  \n",
      "     |  labels_ : ndarray of shape (n_samples)\n",
      "     |      Cluster labels for each point in the dataset given to fit().\n",
      "     |      Noisy samples are given the label -1.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import DBSCAN\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 2], [2, 2], [2, 3],\n",
      "     |  ...               [8, 7], [8, 8], [25, 80]])\n",
      "     |  >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n",
      "     |  >>> clustering.labels_\n",
      "     |  array([ 0,  0,  0,  1,  1, -1])\n",
      "     |  >>> clustering\n",
      "     |  DBSCAN(eps=3, min_samples=2)\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  OPTICS\n",
      "     |      A similar clustering at multiple values of eps. Our implementation\n",
      "     |      is optimized for memory usage.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For an example, see :ref:`examples/cluster/plot_dbscan.py\n",
      "     |  <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n",
      "     |  \n",
      "     |  This implementation bulk-computes all neighborhood queries, which increases\n",
      "     |  the memory complexity to O(n.d) where d is the average number of neighbors,\n",
      "     |  while original DBSCAN had memory complexity O(n). It may attract a higher\n",
      "     |  memory complexity when querying these nearest neighborhoods, depending\n",
      "     |  on the ``algorithm``.\n",
      "     |  \n",
      "     |  One way to avoid the query complexity is to pre-compute sparse\n",
      "     |  neighborhoods in chunks using\n",
      "     |  :func:`NearestNeighbors.radius_neighbors_graph\n",
      "     |  <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n",
      "     |  ``mode='distance'``, then using ``metric='precomputed'`` here.\n",
      "     |  \n",
      "     |  Another way to reduce memory and computation time is to remove\n",
      "     |  (near-)duplicate points and use ``sample_weight`` instead.\n",
      "     |  \n",
      "     |  :class:`cluster.OPTICS` provides a similar clustering with lower memory\n",
      "     |  usage.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Ester, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\n",
      "     |  Algorithm for Discovering Clusters in Large Spatial Databases with Noise\".\n",
      "     |  In: Proceedings of the 2nd International Conference on Knowledge Discovery\n",
      "     |  and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n",
      "     |  \n",
      "     |  Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n",
      "     |  DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\n",
      "     |  ACM Transactions on Database Systems (TODS), 42(3), 19.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DBSCAN\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, eps=0.5, *, min_samples=5, metric='euclidean', metric_params=None, algorithm='auto', leaf_size=30, p=None, n_jobs=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Perform DBSCAN clustering from features, or distance matrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)\n",
      "     |          Training instances to cluster, or distances between instances if\n",
      "     |          ``metric='precomputed'``. If a sparse matrix is provided, it will\n",
      "     |          be converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weight of each sample, such that a sample with a weight of at least\n",
      "     |          ``min_samples`` is by itself a core sample; a sample with a\n",
      "     |          negative weight may inhibit its eps-neighbor from being core.\n",
      "     |          Note that weights are absolute, and default to 1.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None, sample_weight=None)\n",
      "     |      Perform DBSCAN clustering from features or distance matrix,\n",
      "     |      and return cluster labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)\n",
      "     |          Training instances to cluster, or distances between instances if\n",
      "     |          ``metric='precomputed'``. If a sparse matrix is provided, it will\n",
      "     |          be converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weight of each sample, such that a sample with a weight of at least\n",
      "     |          ``min_samples`` is by itself a core sample; a sample with a\n",
      "     |          negative weight may inhibit its eps-neighbor from being core.\n",
      "     |          Note that weights are absolute, and default to 1.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,)\n",
      "     |          Cluster labels. Noisy samples are given the label -1.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class FeatureAgglomeration(AgglomerativeClustering, sklearn.cluster._feature_agglomeration.AgglomerationTransform)\n",
      "     |  FeatureAgglomeration(n_clusters=2, *, affinity='euclidean', memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', pooling_func=<function mean at 0x0000017BEFD7A700>, distance_threshold=None)\n",
      "     |  \n",
      "     |  Agglomerate features.\n",
      "     |  \n",
      "     |  Similar to AgglomerativeClustering, but recursively merges features\n",
      "     |  instead of samples.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <hierarchical_clustering>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_clusters : int, default=2\n",
      "     |      The number of clusters to find. It must be ``None`` if\n",
      "     |      ``distance_threshold`` is not ``None``.\n",
      "     |  \n",
      "     |  affinity : str or callable, default='euclidean'\n",
      "     |      Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n",
      "     |      \"manhattan\", \"cosine\", or 'precomputed'.\n",
      "     |      If linkage is \"ward\", only \"euclidean\" is accepted.\n",
      "     |  \n",
      "     |  memory : str or object with the joblib.Memory interface, default=None\n",
      "     |      Used to cache the output of the computation of the tree.\n",
      "     |      By default, no caching is done. If a string is given, it is the\n",
      "     |      path to the caching directory.\n",
      "     |  \n",
      "     |  connectivity : array-like or callable, default=None\n",
      "     |      Connectivity matrix. Defines for each feature the neighboring\n",
      "     |      features following a given structure of the data.\n",
      "     |      This can be a connectivity matrix itself or a callable that transforms\n",
      "     |      the data into a connectivity matrix, such as derived from\n",
      "     |      kneighbors_graph. Default is None, i.e, the\n",
      "     |      hierarchical clustering algorithm is unstructured.\n",
      "     |  \n",
      "     |  compute_full_tree : 'auto' or bool, optional, default='auto'\n",
      "     |      Stop early the construction of the tree at n_clusters. This is useful\n",
      "     |      to decrease computation time if the number of clusters is not small\n",
      "     |      compared to the number of features. This option is useful only when\n",
      "     |      specifying a connectivity matrix. Note also that when varying the\n",
      "     |      number of clusters and using caching, it may be advantageous to compute\n",
      "     |      the full tree. It must be ``True`` if ``distance_threshold`` is not\n",
      "     |      ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n",
      "     |      to `True` when `distance_threshold` is not `None` or that `n_clusters`\n",
      "     |      is inferior to the maximum between 100 or `0.02 * n_samples`.\n",
      "     |      Otherwise, \"auto\" is equivalent to `False`.\n",
      "     |  \n",
      "     |  linkage : {'ward', 'complete', 'average', 'single'}, default='ward'\n",
      "     |      Which linkage criterion to use. The linkage criterion determines which\n",
      "     |      distance to use between sets of features. The algorithm will merge\n",
      "     |      the pairs of cluster that minimize this criterion.\n",
      "     |  \n",
      "     |      - ward minimizes the variance of the clusters being merged.\n",
      "     |      - average uses the average of the distances of each feature of\n",
      "     |        the two sets.\n",
      "     |      - complete or maximum linkage uses the maximum distances between\n",
      "     |        all features of the two sets.\n",
      "     |      - single uses the minimum of the distances between all observations\n",
      "     |        of the two sets.\n",
      "     |  \n",
      "     |  pooling_func : callable, default=np.mean\n",
      "     |      This combines the values of agglomerated features into a single\n",
      "     |      value, and should accept an array of shape [M, N] and the keyword\n",
      "     |      argument `axis=1`, and reduce it to an array of size [M].\n",
      "     |  \n",
      "     |  distance_threshold : float, default=None\n",
      "     |      The linkage distance threshold above which, clusters will not be\n",
      "     |      merged. If not ``None``, ``n_clusters`` must be ``None`` and\n",
      "     |      ``compute_full_tree`` must be ``True``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.21\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_clusters_ : int\n",
      "     |      The number of clusters found by the algorithm. If\n",
      "     |      ``distance_threshold=None``, it will be equal to the given\n",
      "     |      ``n_clusters``.\n",
      "     |  \n",
      "     |  labels_ : array-like of (n_features,)\n",
      "     |      cluster labels for each feature.\n",
      "     |  \n",
      "     |  n_leaves_ : int\n",
      "     |      Number of leaves in the hierarchical tree.\n",
      "     |  \n",
      "     |  n_connected_components_ : int\n",
      "     |      The estimated number of connected components in the graph.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.21\n",
      "     |          ``n_connected_components_`` was added to replace ``n_components_``.\n",
      "     |  \n",
      "     |  children_ : array-like of shape (n_nodes-1, 2)\n",
      "     |      The children of each non-leaf node. Values less than `n_features`\n",
      "     |      correspond to leaves of the tree which are the original samples.\n",
      "     |      A node `i` greater than or equal to `n_features` is a non-leaf\n",
      "     |      node and has children `children_[i - n_features]`. Alternatively\n",
      "     |      at the i-th iteration, children[i][0] and children[i][1]\n",
      "     |      are merged to form node `n_features + i`\n",
      "     |  \n",
      "     |  distances_ : array-like of shape (n_nodes-1,)\n",
      "     |      Distances between nodes in the corresponding place in `children_`.\n",
      "     |      Only computed if distance_threshold is not None.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn import datasets, cluster\n",
      "     |  >>> digits = datasets.load_digits()\n",
      "     |  >>> images = digits.images\n",
      "     |  >>> X = np.reshape(images, (len(images), -1))\n",
      "     |  >>> agglo = cluster.FeatureAgglomeration(n_clusters=32)\n",
      "     |  >>> agglo.fit(X)\n",
      "     |  FeatureAgglomeration(n_clusters=32)\n",
      "     |  >>> X_reduced = agglo.transform(X)\n",
      "     |  >>> X_reduced.shape\n",
      "     |  (1797, 32)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      FeatureAgglomeration\n",
      "     |      AgglomerativeClustering\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.cluster._feature_agglomeration.AgglomerationTransform\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_clusters=2, *, affinity='euclidean', memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', pooling_func=<function mean at 0x0000017BEFD7A700>, distance_threshold=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, **params)\n",
      "     |      Fit the hierarchical clustering on the data\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  fit_predict\n",
      "     |      Fit the hierarchical clustering from features or distance matrix,\n",
      "     |      and return cluster labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features) or (n_samples, n_samples)\n",
      "     |          Training instances to cluster, or distances between instances if\n",
      "     |          ``affinity='precomputed'``.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray, shape (n_samples,)\n",
      "     |          Cluster labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.cluster._feature_agglomeration.AgglomerationTransform:\n",
      "     |  \n",
      "     |  inverse_transform(self, Xred)\n",
      "     |      Inverse the transformation.\n",
      "     |      Return a vector of size nb_features with the values of Xred assigned\n",
      "     |      to each group of features\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      Xred : array-like of shape (n_samples, n_clusters) or (n_clusters,)\n",
      "     |          The values to be assigned to each cluster of samples\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array, shape=[n_samples, n_features] or [n_features]\n",
      "     |          A vector of size n_samples with the values of Xred assigned to\n",
      "     |          each of the cluster of samples.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform a new matrix using the built clustering\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features) or (n_samples,)\n",
      "     |          A M by N array of M observations in N dimensions or a length\n",
      "     |          M array of M one-dimensional observations.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Y : array, shape = [n_samples, n_clusters] or [n_clusters]\n",
      "     |          The pooled values for each feature cluster.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,), default=None\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "    \n",
      "    class KMeans(sklearn.base.TransformerMixin, sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "     |  KMeans(n_clusters=8, *, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='deprecated', verbose=0, random_state=None, copy_x=True, n_jobs='deprecated', algorithm='auto')\n",
      "     |  \n",
      "     |  K-Means clustering.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <k_means>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  n_clusters : int, default=8\n",
      "     |      The number of clusters to form as well as the number of\n",
      "     |      centroids to generate.\n",
      "     |  \n",
      "     |  init : {'k-means++', 'random', ndarray, callable}, default='k-means++'\n",
      "     |      Method for initialization:\n",
      "     |  \n",
      "     |      'k-means++' : selects initial cluster centers for k-mean\n",
      "     |      clustering in a smart way to speed up convergence. See section\n",
      "     |      Notes in k_init for more details.\n",
      "     |  \n",
      "     |      'random': choose `n_clusters` observations (rows) at random from data\n",
      "     |      for the initial centroids.\n",
      "     |  \n",
      "     |      If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
      "     |      and gives the initial centers.\n",
      "     |  \n",
      "     |      If a callable is passed, it should take arguments X, n_clusters and a\n",
      "     |      random state and return an initialization.\n",
      "     |  \n",
      "     |  n_init : int, default=10\n",
      "     |      Number of time the k-means algorithm will be run with different\n",
      "     |      centroid seeds. The final results will be the best output of\n",
      "     |      n_init consecutive runs in terms of inertia.\n",
      "     |  \n",
      "     |  max_iter : int, default=300\n",
      "     |      Maximum number of iterations of the k-means algorithm for a\n",
      "     |      single run.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Relative tolerance with regards to Frobenius norm of the difference\n",
      "     |      in the cluster centers of two consecutive iterations to declare\n",
      "     |      convergence.\n",
      "     |  \n",
      "     |  precompute_distances : {'auto', True, False}, default='auto'\n",
      "     |      Precompute distances (faster but takes more memory).\n",
      "     |  \n",
      "     |      'auto' : do not precompute distances if n_samples * n_clusters > 12\n",
      "     |      million. This corresponds to about 100MB overhead per job using\n",
      "     |      double precision.\n",
      "     |  \n",
      "     |      True : always precompute distances.\n",
      "     |  \n",
      "     |      False : never precompute distances.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.23\n",
      "     |          'precompute_distances' was deprecated in version 0.22 and will be\n",
      "     |          removed in 0.25. It has no effect.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Verbosity mode.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Determines random number generation for centroid initialization. Use\n",
      "     |      an int to make the randomness deterministic.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  copy_x : bool, default=True\n",
      "     |      When pre-computing distances it is more numerically accurate to center\n",
      "     |      the data first. If copy_x is True (default), then the original data is\n",
      "     |      not modified. If False, the original data is modified, and put back\n",
      "     |      before the function returns, but small numerical differences may be\n",
      "     |      introduced by subtracting and then adding the data mean. Note that if\n",
      "     |      the original data is not C-contiguous, a copy will be made even if\n",
      "     |      copy_x is False. If the original data is sparse, but not in CSR format,\n",
      "     |      a copy will be made even if copy_x is False.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of OpenMP threads to use for the computation. Parallelism is\n",
      "     |      sample-wise on the main cython loop which assigns each sample to its\n",
      "     |      closest center.\n",
      "     |  \n",
      "     |      ``None`` or ``-1`` means using all processors.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.23\n",
      "     |          ``n_jobs`` was deprecated in version 0.23 and will be removed in\n",
      "     |          0.25.\n",
      "     |  \n",
      "     |  algorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n",
      "     |      K-means algorithm to use. The classical EM-style algorithm is \"full\".\n",
      "     |      The \"elkan\" variation is more efficient on data with well-defined\n",
      "     |      clusters, by using the triangle inequality. However it's more memory\n",
      "     |      intensive due to the allocation of an extra array of shape\n",
      "     |      (n_samples, n_clusters).\n",
      "     |  \n",
      "     |      For now \"auto\" (kept for backward compatibiliy) chooses \"elkan\" but it\n",
      "     |      might change in the future for a better heuristic.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |          Added Elkan algorithm\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cluster_centers_ : ndarray of shape (n_clusters, n_features)\n",
      "     |      Coordinates of cluster centers. If the algorithm stops before fully\n",
      "     |      converging (see ``tol`` and ``max_iter``), these will not be\n",
      "     |      consistent with ``labels_``.\n",
      "     |  \n",
      "     |  labels_ : ndarray of shape (n_samples,)\n",
      "     |      Labels of each point\n",
      "     |  \n",
      "     |  inertia_ : float\n",
      "     |      Sum of squared distances of samples to their closest cluster center.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  \n",
      "     |  MiniBatchKMeans\n",
      "     |      Alternative online implementation that does incremental updates\n",
      "     |      of the centers positions using mini-batches.\n",
      "     |      For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n",
      "     |      probably much faster than the default batch implementation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The k-means problem is solved using either Lloyd's or Elkan's algorithm.\n",
      "     |  \n",
      "     |  The average complexity is given by O(k n T), were n is the number of\n",
      "     |  samples and T is the number of iteration.\n",
      "     |  \n",
      "     |  The worst case complexity is given by O(n^(k+2/p)) with\n",
      "     |  n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n",
      "     |  'How slow is the k-means method?' SoCG2006)\n",
      "     |  \n",
      "     |  In practice, the k-means algorithm is very fast (one of the fastest\n",
      "     |  clustering algorithms available), but it falls in local minima. That's why\n",
      "     |  it can be useful to restart it several times.\n",
      "     |  \n",
      "     |  If the algorithm stops before fully converging (because of ``tol`` or\n",
      "     |  ``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\n",
      "     |  i.e. the ``cluster_centers_`` will not be the means of the points in each\n",
      "     |  cluster. Also, the estimator will reassign ``labels_`` after the last\n",
      "     |  iteration to make ``labels_`` consistent with ``predict`` on the training\n",
      "     |  set.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  \n",
      "     |  >>> from sklearn.cluster import KMeans\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
      "     |  ...               [10, 2], [10, 4], [10, 0]])\n",
      "     |  >>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
      "     |  >>> kmeans.labels_\n",
      "     |  array([1, 1, 1, 0, 0, 0], dtype=int32)\n",
      "     |  >>> kmeans.predict([[0, 0], [12, 3]])\n",
      "     |  array([1, 0], dtype=int32)\n",
      "     |  >>> kmeans.cluster_centers_\n",
      "     |  array([[10.,  2.],\n",
      "     |         [ 1.,  2.]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KMeans\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_clusters=8, *, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='deprecated', verbose=0, random_state=None, copy_x=True, n_jobs='deprecated', algorithm='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Compute k-means clustering.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training instances to cluster. It must be noted that the data\n",
      "     |          will be converted to C ordering, which will cause a memory\n",
      "     |          copy if the given data is not C-contiguous.\n",
      "     |          If a sparse matrix is passed, a copy will be made if it's not in\n",
      "     |          CSR format.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.20\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None, sample_weight=None)\n",
      "     |      Compute cluster centers and predict cluster index for each sample.\n",
      "     |      \n",
      "     |      Convenience method; equivalent to calling fit(X) followed by\n",
      "     |      predict(X).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data to transform.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,)\n",
      "     |          Index of the cluster each sample belongs to.\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, sample_weight=None)\n",
      "     |      Compute clustering and transform X to cluster-distance space.\n",
      "     |      \n",
      "     |      Equivalent to fit(X).transform(X), but more efficiently implemented.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data to transform.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : array of shape (n_samples, n_clusters)\n",
      "     |          X transformed in the new space.\n",
      "     |  \n",
      "     |  predict(self, X, sample_weight=None)\n",
      "     |      Predict the closest cluster each sample in X belongs to.\n",
      "     |      \n",
      "     |      In the vector quantization literature, `cluster_centers_` is called\n",
      "     |      the code book and each value returned by `predict` is the index of\n",
      "     |      the closest code in the code book.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data to predict.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,)\n",
      "     |          Index of the cluster each sample belongs to.\n",
      "     |  \n",
      "     |  score(self, X, y=None, sample_weight=None)\n",
      "     |      Opposite of the value of X on the K-means objective.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Opposite of the value of X on the K-means objective.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform X to a cluster-distance space.\n",
      "     |      \n",
      "     |      In the new space, each dimension is the distance to the cluster\n",
      "     |      centers.  Note that even if X is sparse, the array returned by\n",
      "     |      `transform` will typically be dense.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data to transform.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray of shape (n_samples, n_clusters)\n",
      "     |          X transformed in the new space.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MeanShift(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "     |  MeanShift(*, bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300)\n",
      "     |  \n",
      "     |  Mean shift clustering using a flat kernel.\n",
      "     |  \n",
      "     |  Mean shift clustering aims to discover \"blobs\" in a smooth density of\n",
      "     |  samples. It is a centroid-based algorithm, which works by updating\n",
      "     |  candidates for centroids to be the mean of the points within a given\n",
      "     |  region. These candidates are then filtered in a post-processing stage to\n",
      "     |  eliminate near-duplicates to form the final set of centroids.\n",
      "     |  \n",
      "     |  Seeding is performed using a binning technique for scalability.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <mean_shift>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  bandwidth : float, default=None\n",
      "     |      Bandwidth used in the RBF kernel.\n",
      "     |  \n",
      "     |      If not given, the bandwidth is estimated using\n",
      "     |      sklearn.cluster.estimate_bandwidth; see the documentation for that\n",
      "     |      function for hints on scalability (see also the Notes, below).\n",
      "     |  \n",
      "     |  seeds : array-like of shape (n_samples, n_features), default=None\n",
      "     |      Seeds used to initialize kernels. If not set,\n",
      "     |      the seeds are calculated by clustering.get_bin_seeds\n",
      "     |      with bandwidth as the grid size and default values for\n",
      "     |      other parameters.\n",
      "     |  \n",
      "     |  bin_seeding : bool, default=False\n",
      "     |      If true, initial kernel locations are not locations of all\n",
      "     |      points, but rather the location of the discretized version of\n",
      "     |      points, where points are binned onto a grid whose coarseness\n",
      "     |      corresponds to the bandwidth. Setting this option to True will speed\n",
      "     |      up the algorithm because fewer seeds will be initialized.\n",
      "     |      The default value is False.\n",
      "     |      Ignored if seeds argument is not None.\n",
      "     |  \n",
      "     |  min_bin_freq : int, default=1\n",
      "     |     To speed up the algorithm, accept only those bins with at least\n",
      "     |     min_bin_freq points as seeds.\n",
      "     |  \n",
      "     |  cluster_all : bool, default=True\n",
      "     |      If true, then all points are clustered, even those orphans that are\n",
      "     |      not within any kernel. Orphans are assigned to the nearest kernel.\n",
      "     |      If false, then orphans are given cluster label -1.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to use for the computation. This works by computing\n",
      "     |      each of the n_init runs in parallel.\n",
      "     |  \n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  max_iter : int, default=300\n",
      "     |      Maximum number of iterations, per seed point before the clustering\n",
      "     |      operation terminates (for that seed point), if has not converged yet.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cluster_centers_ : array, [n_clusters, n_features]\n",
      "     |      Coordinates of cluster centers.\n",
      "     |  \n",
      "     |  labels_ : array of shape (n_samples,)\n",
      "     |      Labels of each point.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Maximum number of iterations performed on each seed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import MeanShift\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 1], [2, 1], [1, 0],\n",
      "     |  ...               [4, 7], [3, 5], [3, 6]])\n",
      "     |  >>> clustering = MeanShift(bandwidth=2).fit(X)\n",
      "     |  >>> clustering.labels_\n",
      "     |  array([1, 1, 1, 0, 0, 0])\n",
      "     |  >>> clustering.predict([[0, 0], [5, 5]])\n",
      "     |  array([1, 0])\n",
      "     |  >>> clustering\n",
      "     |  MeanShift(bandwidth=2)\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  \n",
      "     |  Scalability:\n",
      "     |  \n",
      "     |  Because this implementation uses a flat kernel and\n",
      "     |  a Ball Tree to look up members of each kernel, the complexity will tend\n",
      "     |  towards O(T*n*log(n)) in lower dimensions, with n the number of samples\n",
      "     |  and T the number of points. In higher dimensions the complexity will\n",
      "     |  tend towards O(T*n^2).\n",
      "     |  \n",
      "     |  Scalability can be boosted by using fewer seeds, for example by using\n",
      "     |  a higher value of min_bin_freq in the get_bin_seeds function.\n",
      "     |  \n",
      "     |  Note that the estimate_bandwidth function is much less scalable than the\n",
      "     |  mean shift algorithm and will be the bottleneck if it is used.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  Dorin Comaniciu and Peter Meer, \"Mean Shift: A robust approach toward\n",
      "     |  feature space analysis\". IEEE Transactions on Pattern Analysis and\n",
      "     |  Machine Intelligence. 2002. pp. 603-619.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MeanShift\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Perform clustering.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Samples to cluster.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict the closest cluster each sample in X belongs to.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape=[n_samples, n_features]\n",
      "     |          New data to predict.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : array, shape [n_samples,]\n",
      "     |          Index of the cluster each sample belongs to.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None)\n",
      "     |      Perform clustering on X and returns cluster labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,)\n",
      "     |          Cluster labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MiniBatchKMeans(KMeans)\n",
      "     |  MiniBatchKMeans(n_clusters=8, *, init='k-means++', max_iter=100, batch_size=100, verbose=0, compute_labels=True, random_state=None, tol=0.0, max_no_improvement=10, init_size=None, n_init=3, reassignment_ratio=0.01)\n",
      "     |  \n",
      "     |  Mini-Batch K-Means clustering.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <mini_batch_kmeans>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  n_clusters : int, default=8\n",
      "     |      The number of clusters to form as well as the number of\n",
      "     |      centroids to generate.\n",
      "     |  \n",
      "     |  init : {'k-means++', 'random'} or ndarray of shape             (n_clusters, n_features), default='k-means++'\n",
      "     |      Method for initialization\n",
      "     |  \n",
      "     |      'k-means++' : selects initial cluster centers for k-mean\n",
      "     |      clustering in a smart way to speed up convergence. See section\n",
      "     |      Notes in k_init for more details.\n",
      "     |  \n",
      "     |      'random': choose k observations (rows) at random from data for\n",
      "     |      the initial centroids.\n",
      "     |  \n",
      "     |      If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
      "     |      and gives the initial centers.\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      Maximum number of iterations over the complete dataset before\n",
      "     |      stopping independently of any early stopping criterion heuristics.\n",
      "     |  \n",
      "     |  batch_size : int, default=100\n",
      "     |      Size of the mini batches.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Verbosity mode.\n",
      "     |  \n",
      "     |  compute_labels : bool, default=True\n",
      "     |      Compute label assignment and inertia for the complete dataset\n",
      "     |      once the minibatch optimization has converged in fit.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Determines random number generation for centroid initialization and\n",
      "     |      random reassignment. Use an int to make the randomness deterministic.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  tol : float, default=0.0\n",
      "     |      Control early stopping based on the relative center changes as\n",
      "     |      measured by a smoothed, variance-normalized of the mean center\n",
      "     |      squared position changes. This early stopping heuristics is\n",
      "     |      closer to the one used for the batch variant of the algorithms\n",
      "     |      but induces a slight computational and memory overhead over the\n",
      "     |      inertia heuristic.\n",
      "     |  \n",
      "     |      To disable convergence detection based on normalized center\n",
      "     |      change, set tol to 0.0 (default).\n",
      "     |  \n",
      "     |  max_no_improvement : int, default=10\n",
      "     |      Control early stopping based on the consecutive number of mini\n",
      "     |      batches that does not yield an improvement on the smoothed inertia.\n",
      "     |  \n",
      "     |      To disable convergence detection based on inertia, set\n",
      "     |      max_no_improvement to None.\n",
      "     |  \n",
      "     |  init_size : int, default=None\n",
      "     |      Number of samples to randomly sample for speeding up the\n",
      "     |      initialization (sometimes at the expense of accuracy): the\n",
      "     |      only algorithm is initialized by running a batch KMeans on a\n",
      "     |      random subset of the data. This needs to be larger than n_clusters.\n",
      "     |  \n",
      "     |      If `None`, `init_size= 3 * batch_size`.\n",
      "     |  \n",
      "     |  n_init : int, default=3\n",
      "     |      Number of random initializations that are tried.\n",
      "     |      In contrast to KMeans, the algorithm is only run once, using the\n",
      "     |      best of the ``n_init`` initializations as measured by inertia.\n",
      "     |  \n",
      "     |  reassignment_ratio : float, default=0.01\n",
      "     |      Control the fraction of the maximum number of counts for a\n",
      "     |      center to be reassigned. A higher value means that low count\n",
      "     |      centers are more easily reassigned, which means that the\n",
      "     |      model will take longer to converge, but should converge in a\n",
      "     |      better clustering.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  cluster_centers_ : ndarray of shape (n_clusters, n_features)\n",
      "     |      Coordinates of cluster centers\n",
      "     |  \n",
      "     |  labels_ : int\n",
      "     |      Labels of each point (if compute_labels is set to True).\n",
      "     |  \n",
      "     |  inertia_ : float\n",
      "     |      The value of the inertia criterion associated with the chosen\n",
      "     |      partition (if compute_labels is set to True). The inertia is\n",
      "     |      defined as the sum of square distances of samples to their nearest\n",
      "     |      neighbor.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  KMeans\n",
      "     |      The classic implementation of the clustering method based on the\n",
      "     |      Lloyd's algorithm. It consumes the whole set of input data at each\n",
      "     |      iteration.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import MiniBatchKMeans\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
      "     |  ...               [4, 2], [4, 0], [4, 4],\n",
      "     |  ...               [4, 5], [0, 1], [2, 2],\n",
      "     |  ...               [3, 2], [5, 5], [1, -1]])\n",
      "     |  >>> # manually fit on batches\n",
      "     |  >>> kmeans = MiniBatchKMeans(n_clusters=2,\n",
      "     |  ...                          random_state=0,\n",
      "     |  ...                          batch_size=6)\n",
      "     |  >>> kmeans = kmeans.partial_fit(X[0:6,:])\n",
      "     |  >>> kmeans = kmeans.partial_fit(X[6:12,:])\n",
      "     |  >>> kmeans.cluster_centers_\n",
      "     |  array([[2. , 1. ],\n",
      "     |         [3.5, 4.5]])\n",
      "     |  >>> kmeans.predict([[0, 0], [4, 4]])\n",
      "     |  array([0, 1], dtype=int32)\n",
      "     |  >>> # fit on the whole data\n",
      "     |  >>> kmeans = MiniBatchKMeans(n_clusters=2,\n",
      "     |  ...                          random_state=0,\n",
      "     |  ...                          batch_size=6,\n",
      "     |  ...                          max_iter=10).fit(X)\n",
      "     |  >>> kmeans.cluster_centers_\n",
      "     |  array([[3.95918367, 2.40816327],\n",
      "     |         [1.12195122, 1.3902439 ]])\n",
      "     |  >>> kmeans.predict([[0, 0], [4, 4]])\n",
      "     |  array([1, 0], dtype=int32)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MiniBatchKMeans\n",
      "     |      KMeans\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_clusters=8, *, init='k-means++', max_iter=100, batch_size=100, verbose=0, compute_labels=True, random_state=None, tol=0.0, max_no_improvement=10, init_size=None, n_init=3, reassignment_ratio=0.01)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Compute the centroids on X by chunking it into mini-batches.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      "     |          Training instances to cluster. It must be noted that the data\n",
      "     |          will be converted to C ordering, which will cause a memory copy\n",
      "     |          if the given data is not C-contiguous.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight (default: None).\n",
      "     |      \n",
      "     |          .. versionadded:: 0.20\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  partial_fit(self, X, y=None, sample_weight=None)\n",
      "     |      Update k means estimate on a single mini-batch X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Coordinates of the data points to cluster. It must be noted that\n",
      "     |          X will be copied if it is not C-contiguous.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight (default: None).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  predict(self, X, sample_weight=None)\n",
      "     |      Predict the closest cluster each sample in X belongs to.\n",
      "     |      \n",
      "     |      In the vector quantization literature, `cluster_centers_` is called\n",
      "     |      the code book and each value returned by `predict` is the index of\n",
      "     |      the closest code in the code book.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data to predict.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight (default: None).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : array, shape [n_samples,]\n",
      "     |          Index of the cluster each sample belongs to.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from KMeans:\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None, sample_weight=None)\n",
      "     |      Compute cluster centers and predict cluster index for each sample.\n",
      "     |      \n",
      "     |      Convenience method; equivalent to calling fit(X) followed by\n",
      "     |      predict(X).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data to transform.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,)\n",
      "     |          Index of the cluster each sample belongs to.\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, sample_weight=None)\n",
      "     |      Compute clustering and transform X to cluster-distance space.\n",
      "     |      \n",
      "     |      Equivalent to fit(X).transform(X), but more efficiently implemented.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data to transform.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : array of shape (n_samples, n_clusters)\n",
      "     |          X transformed in the new space.\n",
      "     |  \n",
      "     |  score(self, X, y=None, sample_weight=None)\n",
      "     |      Opposite of the value of X on the K-means objective.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          The weights for each observation in X. If None, all observations\n",
      "     |          are assigned equal weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Opposite of the value of X on the K-means objective.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform X to a cluster-distance space.\n",
      "     |      \n",
      "     |      In the new space, each dimension is the distance to the cluster\n",
      "     |      centers.  Note that even if X is sparse, the array returned by\n",
      "     |      `transform` will typically be dense.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          New data to transform.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray of shape (n_samples, n_clusters)\n",
      "     |          X transformed in the new space.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class OPTICS(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "     |  OPTICS(*, min_samples=5, max_eps=inf, metric='minkowski', p=2, metric_params=None, cluster_method='xi', eps=None, xi=0.05, predecessor_correction=True, min_cluster_size=None, algorithm='auto', leaf_size=30, n_jobs=None)\n",
      "     |  \n",
      "     |  Estimate clustering structure from vector array.\n",
      "     |  \n",
      "     |  OPTICS (Ordering Points To Identify the Clustering Structure), closely\n",
      "     |  related to DBSCAN, finds core sample of high density and expands clusters\n",
      "     |  from them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable\n",
      "     |  neighborhood radius. Better suited for usage on large datasets than the\n",
      "     |  current sklearn implementation of DBSCAN.\n",
      "     |  \n",
      "     |  Clusters are then extracted using a DBSCAN-like method\n",
      "     |  (cluster_method = 'dbscan') or an automatic\n",
      "     |  technique proposed in [1]_ (cluster_method = 'xi').\n",
      "     |  \n",
      "     |  This implementation deviates from the original OPTICS by first performing\n",
      "     |  k-nearest-neighborhood searches on all points to identify core sizes, then\n",
      "     |  computing only the distances to unprocessed points when constructing the\n",
      "     |  cluster order. Note that we do not employ a heap to manage the expansion\n",
      "     |  candidates, so the time complexity will be O(n^2).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <optics>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  min_samples : int > 1 or float between 0 and 1 (default=5)\n",
      "     |      The number of samples in a neighborhood for a point to be considered as\n",
      "     |      a core point. Also, up and down steep regions can't have more then\n",
      "     |      ``min_samples`` consecutive non-steep points. Expressed as an absolute\n",
      "     |      number or a fraction of the number of samples (rounded to be at least\n",
      "     |      2).\n",
      "     |  \n",
      "     |  max_eps : float, optional (default=np.inf)\n",
      "     |      The maximum distance between two samples for one to be considered as\n",
      "     |      in the neighborhood of the other. Default value of ``np.inf`` will\n",
      "     |      identify clusters across all scales; reducing ``max_eps`` will result\n",
      "     |      in shorter run times.\n",
      "     |  \n",
      "     |  metric : str or callable, optional (default='minkowski')\n",
      "     |      Metric to use for distance computation. Any metric from scikit-learn\n",
      "     |      or scipy.spatial.distance can be used.\n",
      "     |  \n",
      "     |      If metric is a callable function, it is called on each\n",
      "     |      pair of instances (rows) and the resulting value recorded. The callable\n",
      "     |      should take two arrays as input and return one value indicating the\n",
      "     |      distance between them. This works for Scipy's metrics, but is less\n",
      "     |      efficient than passing the metric name as a string. If metric is\n",
      "     |      \"precomputed\", X is assumed to be a distance matrix and must be square.\n",
      "     |  \n",
      "     |      Valid values for metric are:\n",
      "     |  \n",
      "     |      - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "     |        'manhattan']\n",
      "     |  \n",
      "     |      - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "     |        'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "     |        'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "     |        'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "     |        'yule']\n",
      "     |  \n",
      "     |      See the documentation for scipy.spatial.distance for details on these\n",
      "     |      metrics.\n",
      "     |  \n",
      "     |  p : int, optional (default=2)\n",
      "     |      Parameter for the Minkowski metric from\n",
      "     |      :class:`sklearn.metrics.pairwise_distances`. When p = 1, this is\n",
      "     |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric_params : dict, optional (default=None)\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  cluster_method : str, optional (default='xi')\n",
      "     |      The extraction method used to extract clusters using the calculated\n",
      "     |      reachability and ordering. Possible values are \"xi\" and \"dbscan\".\n",
      "     |  \n",
      "     |  eps : float, optional (default=None)\n",
      "     |      The maximum distance between two samples for one to be considered as\n",
      "     |      in the neighborhood of the other. By default it assumes the same value\n",
      "     |      as ``max_eps``.\n",
      "     |      Used only when ``cluster_method='dbscan'``.\n",
      "     |  \n",
      "     |  xi : float, between 0 and 1, optional (default=0.05)\n",
      "     |      Determines the minimum steepness on the reachability plot that\n",
      "     |      constitutes a cluster boundary. For example, an upwards point in the\n",
      "     |      reachability plot is defined by the ratio from one point to its\n",
      "     |      successor being at most 1-xi.\n",
      "     |      Used only when ``cluster_method='xi'``.\n",
      "     |  \n",
      "     |  predecessor_correction : bool, optional (default=True)\n",
      "     |      Correct clusters according to the predecessors calculated by OPTICS\n",
      "     |      [2]_. This parameter has minimal effect on most datasets.\n",
      "     |      Used only when ``cluster_method='xi'``.\n",
      "     |  \n",
      "     |  min_cluster_size : int > 1 or float between 0 and 1 (default=None)\n",
      "     |      Minimum number of samples in an OPTICS cluster, expressed as an\n",
      "     |      absolute number or a fraction of the number of samples (rounded to be\n",
      "     |      at least 2). If ``None``, the value of ``min_samples`` is used instead.\n",
      "     |      Used only when ``cluster_method='xi'``.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method. (default)\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, optional (default=30)\n",
      "     |      Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n",
      "     |      affect the speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree. The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  n_jobs : int or None, optional (default=None)\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  labels_ : array, shape (n_samples,)\n",
      "     |      Cluster labels for each point in the dataset given to fit().\n",
      "     |      Noisy samples and points which are not included in a leaf cluster\n",
      "     |      of ``cluster_hierarchy_`` are labeled as -1.\n",
      "     |  \n",
      "     |  reachability_ : array, shape (n_samples,)\n",
      "     |      Reachability distances per sample, indexed by object order. Use\n",
      "     |      ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n",
      "     |  \n",
      "     |  ordering_ : array, shape (n_samples,)\n",
      "     |      The cluster ordered list of sample indices.\n",
      "     |  \n",
      "     |  core_distances_ : array, shape (n_samples,)\n",
      "     |      Distance at which each sample becomes a core point, indexed by object\n",
      "     |      order. Points which will never be core have a distance of inf. Use\n",
      "     |      ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n",
      "     |  \n",
      "     |  predecessor_ : array, shape (n_samples,)\n",
      "     |      Point that a sample was reached from, indexed by object order.\n",
      "     |      Seed points have a predecessor of -1.\n",
      "     |  \n",
      "     |  cluster_hierarchy_ : array, shape (n_clusters, 2)\n",
      "     |      The list of clusters in the form of ``[start, end]`` in each row, with\n",
      "     |      all indices inclusive. The clusters are ordered according to\n",
      "     |      ``(end, -start)`` (ascending) so that larger clusters encompassing\n",
      "     |      smaller clusters come after those smaller ones. Since ``labels_`` does\n",
      "     |      not reflect the hierarchy, usually\n",
      "     |      ``len(cluster_hierarchy_) > np.unique(optics.labels_)``. Please also\n",
      "     |      note that these indices are of the ``ordering_``, i.e.\n",
      "     |      ``X[ordering_][start:end + 1]`` form a cluster.\n",
      "     |      Only available when ``cluster_method='xi'``.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  DBSCAN\n",
      "     |      A similar clustering for a specified neighborhood radius (eps).\n",
      "     |      Our implementation is optimized for runtime.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n",
      "     |     and Jörg Sander. \"OPTICS: ordering points to identify the clustering\n",
      "     |     structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n",
      "     |  \n",
      "     |  .. [2] Schubert, Erich, Michael Gertz.\n",
      "     |     \"Improving the Cluster Structure Extracted from OPTICS Plots.\" Proc. of\n",
      "     |     the Conference \"Lernen, Wissen, Daten, Analysen\" (LWDA) (2018): 318-329.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import OPTICS\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 2], [2, 5], [3, 6],\n",
      "     |  ...               [8, 7], [8, 8], [7, 3]])\n",
      "     |  >>> clustering = OPTICS(min_samples=2).fit(X)\n",
      "     |  >>> clustering.labels_\n",
      "     |  array([0, 0, 0, 1, 1, 1])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OPTICS\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, min_samples=5, max_eps=inf, metric='minkowski', p=2, metric_params=None, cluster_method='xi', eps=None, xi=0.05, predecessor_correction=True, min_cluster_size=None, algorithm='auto', leaf_size=30, n_jobs=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Perform OPTICS clustering.\n",
      "     |      \n",
      "     |      Extracts an ordered list of points and reachability distances, and\n",
      "     |      performs initial clustering using ``max_eps`` distance specified at\n",
      "     |      OPTICS object instantiation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array, shape (n_samples, n_features), or (n_samples, n_samples)          if metric=’precomputed’\n",
      "     |          A feature array, or array of distances between samples if\n",
      "     |          metric='precomputed'.\n",
      "     |      \n",
      "     |      y : ignored\n",
      "     |          Ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : instance of OPTICS\n",
      "     |          The instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None)\n",
      "     |      Perform clustering on X and returns cluster labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray of shape (n_samples,)\n",
      "     |          Cluster labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SpectralBiclustering(BaseSpectral)\n",
      "     |  SpectralBiclustering(n_clusters=3, *, method='bistochastic', n_components=6, n_best=3, svd_method='randomized', n_svd_vecs=None, mini_batch=False, init='k-means++', n_init=10, n_jobs='deprecated', random_state=None)\n",
      "     |  \n",
      "     |  Spectral biclustering (Kluger, 2003).\n",
      "     |  \n",
      "     |  Partitions rows and columns under the assumption that the data has\n",
      "     |  an underlying checkerboard structure. For instance, if there are\n",
      "     |  two row partitions and three column partitions, each row will\n",
      "     |  belong to three biclusters, and each column will belong to two\n",
      "     |  biclusters. The outer product of the corresponding row and column\n",
      "     |  label vectors gives this checkerboard structure.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <spectral_biclustering>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_clusters : int or tuple (n_row_clusters, n_column_clusters), default=3\n",
      "     |      The number of row and column clusters in the checkerboard\n",
      "     |      structure.\n",
      "     |  \n",
      "     |  method : {'bistochastic', 'scale', 'log'}, default='bistochastic'\n",
      "     |      Method of normalizing and converting singular vectors into\n",
      "     |      biclusters. May be one of 'scale', 'bistochastic', or 'log'.\n",
      "     |      The authors recommend using 'log'. If the data is sparse,\n",
      "     |      however, log normalization will not work, which is why the\n",
      "     |      default is 'bistochastic'.\n",
      "     |  \n",
      "     |      .. warning::\n",
      "     |         if `method='log'`, the data must be sparse.\n",
      "     |  \n",
      "     |  n_components : int, default=6\n",
      "     |      Number of singular vectors to check.\n",
      "     |  \n",
      "     |  n_best : int, default=3\n",
      "     |      Number of best singular vectors to which to project the data\n",
      "     |      for clustering.\n",
      "     |  \n",
      "     |  svd_method : {'randomized', 'arpack'}, default='randomized'\n",
      "     |      Selects the algorithm for finding singular vectors. May be\n",
      "     |      'randomized' or 'arpack'. If 'randomized', uses\n",
      "     |      :func:`~sklearn.utils.extmath.randomized_svd`, which may be faster\n",
      "     |      for large matrices. If 'arpack', uses\n",
      "     |      `scipy.sparse.linalg.svds`, which is more accurate, but\n",
      "     |      possibly slower in some cases.\n",
      "     |  \n",
      "     |  n_svd_vecs : int, default=None\n",
      "     |      Number of vectors to use in calculating the SVD. Corresponds\n",
      "     |      to `ncv` when `svd_method=arpack` and `n_oversamples` when\n",
      "     |      `svd_method` is 'randomized`.\n",
      "     |  \n",
      "     |  mini_batch : bool, default=False\n",
      "     |      Whether to use mini-batch k-means, which is faster but may get\n",
      "     |      different results.\n",
      "     |  \n",
      "     |  init : {'k-means++', 'random'} or ndarray of (n_clusters, n_features),             default='k-means++'\n",
      "     |      Method for initialization of k-means algorithm; defaults to\n",
      "     |      'k-means++'.\n",
      "     |  \n",
      "     |  n_init : int, default=10\n",
      "     |      Number of random initializations that are tried with the\n",
      "     |      k-means algorithm.\n",
      "     |  \n",
      "     |      If mini-batch k-means is used, the best initialization is\n",
      "     |      chosen and the algorithm runs once. Otherwise, the algorithm\n",
      "     |      is run for each initialization and the best solution chosen.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to use for the computation. This works by breaking\n",
      "     |      down the pairwise matrix into n_jobs even slices and computing them in\n",
      "     |      parallel.\n",
      "     |  \n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.23\n",
      "     |          ``n_jobs`` was deprecated in version 0.23 and will be removed in\n",
      "     |          0.25.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used for randomizing the singular value decomposition and the k-means\n",
      "     |      initialization. Use an int to make the randomness deterministic.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  rows_ : array-like of shape (n_row_clusters, n_rows)\n",
      "     |      Results of the clustering. `rows[i, r]` is True if\n",
      "     |      cluster `i` contains row `r`. Available only after calling ``fit``.\n",
      "     |  \n",
      "     |  columns_ : array-like of shape (n_column_clusters, n_columns)\n",
      "     |      Results of the clustering, like `rows`.\n",
      "     |  \n",
      "     |  row_labels_ : array-like of shape (n_rows,)\n",
      "     |      Row partition labels.\n",
      "     |  \n",
      "     |  column_labels_ : array-like of shape (n_cols,)\n",
      "     |      Column partition labels.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import SpectralBiclustering\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 1], [2, 1], [1, 0],\n",
      "     |  ...               [4, 7], [3, 5], [3, 6]])\n",
      "     |  >>> clustering = SpectralBiclustering(n_clusters=2, random_state=0).fit(X)\n",
      "     |  >>> clustering.row_labels_\n",
      "     |  array([1, 1, 1, 0, 0, 0], dtype=int32)\n",
      "     |  >>> clustering.column_labels_\n",
      "     |  array([0, 1], dtype=int32)\n",
      "     |  >>> clustering\n",
      "     |  SpectralBiclustering(n_clusters=2, random_state=0)\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  * Kluger, Yuval, et. al., 2003. `Spectral biclustering of microarray\n",
      "     |    data: coclustering genes and conditions\n",
      "     |    <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608>`__.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpectralBiclustering\n",
      "     |      BaseSpectral\n",
      "     |      sklearn.base.BiclusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_clusters=3, *, method='bistochastic', n_components=6, n_best=3, svd_method='randomized', n_svd_vecs=None, mini_batch=False, init='k-means++', n_init=10, n_jobs='deprecated', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSpectral:\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Creates a biclustering for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BiclusterMixin:\n",
      "     |  \n",
      "     |  get_indices(self, i)\n",
      "     |      Row and column indices of the i'th bicluster.\n",
      "     |      \n",
      "     |      Only works if ``rows_`` and ``columns_`` attributes exist.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      i : int\n",
      "     |          The index of the cluster.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      row_ind : ndarray, dtype=np.intp\n",
      "     |          Indices of rows in the dataset that belong to the bicluster.\n",
      "     |      col_ind : ndarray, dtype=np.intp\n",
      "     |          Indices of columns in the dataset that belong to the bicluster.\n",
      "     |  \n",
      "     |  get_shape(self, i)\n",
      "     |      Shape of the i'th bicluster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      i : int\n",
      "     |          The index of the cluster.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      shape : tuple (int, int)\n",
      "     |          Number of rows and columns (resp.) in the bicluster.\n",
      "     |  \n",
      "     |  get_submatrix(self, i, data)\n",
      "     |      Return the submatrix corresponding to bicluster `i`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      i : int\n",
      "     |          The index of the cluster.\n",
      "     |      data : array-like\n",
      "     |          The data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      submatrix : ndarray\n",
      "     |          The submatrix corresponding to bicluster i.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Works with sparse matrices. Only works if ``rows_`` and\n",
      "     |      ``columns_`` attributes exist.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from sklearn.base.BiclusterMixin:\n",
      "     |  \n",
      "     |  biclusters_\n",
      "     |      Convenient way to get row and column indicators together.\n",
      "     |      \n",
      "     |      Returns the ``rows_`` and ``columns_`` members.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BiclusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SpectralClustering(sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      "     |  SpectralClustering(n_clusters=8, *, eigen_solver=None, n_components=None, random_state=None, n_init=10, gamma=1.0, affinity='rbf', n_neighbors=10, eigen_tol=0.0, assign_labels='kmeans', degree=3, coef0=1, kernel_params=None, n_jobs=None)\n",
      "     |  \n",
      "     |  Apply clustering to a projection of the normalized Laplacian.\n",
      "     |  \n",
      "     |  In practice Spectral Clustering is very useful when the structure of\n",
      "     |  the individual clusters is highly non-convex or more generally when\n",
      "     |  a measure of the center and spread of the cluster is not a suitable\n",
      "     |  description of the complete cluster. For instance when clusters are\n",
      "     |  nested circles on the 2D plane.\n",
      "     |  \n",
      "     |  If affinity is the adjacency matrix of a graph, this method can be\n",
      "     |  used to find normalized graph cuts.\n",
      "     |  \n",
      "     |  When calling ``fit``, an affinity matrix is constructed using either\n",
      "     |  kernel function such the Gaussian (aka RBF) kernel of the euclidean\n",
      "     |  distanced ``d(X, X)``::\n",
      "     |  \n",
      "     |          np.exp(-gamma * d(X,X) ** 2)\n",
      "     |  \n",
      "     |  or a k-nearest neighbors connectivity matrix.\n",
      "     |  \n",
      "     |  Alternatively, using ``precomputed``, a user-provided affinity\n",
      "     |  matrix can be used.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <spectral_clustering>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_clusters : integer, optional\n",
      "     |      The dimension of the projection subspace.\n",
      "     |  \n",
      "     |  eigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}\n",
      "     |      The eigenvalue decomposition strategy to use. AMG requires pyamg\n",
      "     |      to be installed. It can be faster on very large, sparse problems,\n",
      "     |      but may also lead to instabilities.\n",
      "     |  \n",
      "     |  n_components : integer, optional, default=n_clusters\n",
      "     |      Number of eigen vectors to use for the spectral embedding\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      A pseudo random number generator used for the initialization of the\n",
      "     |      lobpcg eigen vectors decomposition when ``eigen_solver='amg'`` and by\n",
      "     |      the K-Means initialization. Use an int to make the randomness\n",
      "     |      deterministic.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  n_init : int, optional, default: 10\n",
      "     |      Number of time the k-means algorithm will be run with different\n",
      "     |      centroid seeds. The final results will be the best output of\n",
      "     |      n_init consecutive runs in terms of inertia.\n",
      "     |  \n",
      "     |  gamma : float, default=1.0\n",
      "     |      Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.\n",
      "     |      Ignored for ``affinity='nearest_neighbors'``.\n",
      "     |  \n",
      "     |  affinity : string or callable, default 'rbf'\n",
      "     |      How to construct the affinity matrix.\n",
      "     |       - 'nearest_neighbors' : construct the affinity matrix by computing a\n",
      "     |         graph of nearest neighbors.\n",
      "     |       - 'rbf' : construct the affinity matrix using a radial basis function\n",
      "     |         (RBF) kernel.\n",
      "     |       - 'precomputed' : interpret ``X`` as a precomputed affinity matrix.\n",
      "     |       - 'precomputed_nearest_neighbors' : interpret ``X`` as a sparse graph\n",
      "     |         of precomputed nearest neighbors, and constructs the affinity matrix\n",
      "     |         by selecting the ``n_neighbors`` nearest neighbors.\n",
      "     |       - one of the kernels supported by\n",
      "     |         :func:`~sklearn.metrics.pairwise_kernels`.\n",
      "     |  \n",
      "     |      Only kernels that produce similarity scores (non-negative values that\n",
      "     |      increase with similarity) should be used. This property is not checked\n",
      "     |      by the clustering algorithm.\n",
      "     |  \n",
      "     |  n_neighbors : integer\n",
      "     |      Number of neighbors to use when constructing the affinity matrix using\n",
      "     |      the nearest neighbors method. Ignored for ``affinity='rbf'``.\n",
      "     |  \n",
      "     |  eigen_tol : float, optional, default: 0.0\n",
      "     |      Stopping criterion for eigendecomposition of the Laplacian matrix\n",
      "     |      when ``eigen_solver='arpack'``.\n",
      "     |  \n",
      "     |  assign_labels : {'kmeans', 'discretize'}, default: 'kmeans'\n",
      "     |      The strategy to use to assign labels in the embedding\n",
      "     |      space. There are two ways to assign labels after the laplacian\n",
      "     |      embedding. k-means can be applied and is a popular choice. But it can\n",
      "     |      also be sensitive to initialization. Discretization is another approach\n",
      "     |      which is less sensitive to random initialization.\n",
      "     |  \n",
      "     |  degree : float, default=3\n",
      "     |      Degree of the polynomial kernel. Ignored by other kernels.\n",
      "     |  \n",
      "     |  coef0 : float, default=1\n",
      "     |      Zero coefficient for polynomial and sigmoid kernels.\n",
      "     |      Ignored by other kernels.\n",
      "     |  \n",
      "     |  kernel_params : dictionary of string to any, optional\n",
      "     |      Parameters (keyword arguments) and values for kernel passed as\n",
      "     |      callable object. Ignored by other kernels.\n",
      "     |  \n",
      "     |  n_jobs : int or None, optional (default=None)\n",
      "     |      The number of parallel jobs to run.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  affinity_matrix_ : array-like, shape (n_samples, n_samples)\n",
      "     |      Affinity matrix used for clustering. Available only if after calling\n",
      "     |      ``fit``.\n",
      "     |  \n",
      "     |  labels_ : array, shape (n_samples,)\n",
      "     |      Labels of each point\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import SpectralClustering\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 1], [2, 1], [1, 0],\n",
      "     |  ...               [4, 7], [3, 5], [3, 6]])\n",
      "     |  >>> clustering = SpectralClustering(n_clusters=2,\n",
      "     |  ...         assign_labels=\"discretize\",\n",
      "     |  ...         random_state=0).fit(X)\n",
      "     |  >>> clustering.labels_\n",
      "     |  array([1, 1, 1, 0, 0, 0])\n",
      "     |  >>> clustering\n",
      "     |  SpectralClustering(assign_labels='discretize', n_clusters=2,\n",
      "     |      random_state=0)\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  If you have an affinity matrix, such as a distance matrix,\n",
      "     |  for which 0 means identical elements, and high values means\n",
      "     |  very dissimilar elements, it can be transformed in a\n",
      "     |  similarity matrix that is well suited for the algorithm by\n",
      "     |  applying the Gaussian (RBF, heat) kernel::\n",
      "     |  \n",
      "     |      np.exp(- dist_matrix ** 2 / (2. * delta ** 2))\n",
      "     |  \n",
      "     |  Where ``delta`` is a free parameter representing the width of the Gaussian\n",
      "     |  kernel.\n",
      "     |  \n",
      "     |  Another alternative is to take a symmetric version of the k\n",
      "     |  nearest neighbors connectivity matrix of the points.\n",
      "     |  \n",
      "     |  If the pyamg package is installed, it is used: this greatly\n",
      "     |  speeds up computation.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  - Normalized cuts and image segmentation, 2000\n",
      "     |    Jianbo Shi, Jitendra Malik\n",
      "     |    http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324\n",
      "     |  \n",
      "     |  - A Tutorial on Spectral Clustering, 2007\n",
      "     |    Ulrike von Luxburg\n",
      "     |    http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n",
      "     |  \n",
      "     |  - Multiclass spectral clustering, 2003\n",
      "     |    Stella X. Yu, Jianbo Shi\n",
      "     |    https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpectralClustering\n",
      "     |      sklearn.base.ClusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_clusters=8, *, eigen_solver=None, n_components=None, random_state=None, n_init=10, gamma=1.0, affinity='rbf', n_neighbors=10, eigen_tol=0.0, assign_labels='kmeans', degree=3, coef0=1, kernel_params=None, n_jobs=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Perform spectral clustering from features, or affinity matrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features), or             array-like, shape (n_samples, n_samples)\n",
      "     |          Training instances to cluster, or similarities / affinities between\n",
      "     |          instances if ``affinity='precomputed'``. If a sparse matrix is\n",
      "     |          provided in a format other than ``csr_matrix``, ``csc_matrix``,\n",
      "     |          or ``coo_matrix``, it will be converted into a sparse\n",
      "     |          ``csr_matrix``.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None)\n",
      "     |      Perform spectral clustering from features, or affinity matrix,\n",
      "     |      and return cluster labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features), or             array-like, shape (n_samples, n_samples)\n",
      "     |          Training instances to cluster, or similarities / affinities between\n",
      "     |          instances if ``affinity='precomputed'``. If a sparse matrix is\n",
      "     |          provided in a format other than ``csr_matrix``, ``csc_matrix``,\n",
      "     |          or ``coo_matrix``, it will be converted into a sparse\n",
      "     |          ``csr_matrix``.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      labels : ndarray, shape (n_samples,)\n",
      "     |          Cluster labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SpectralCoclustering(BaseSpectral)\n",
      "     |  SpectralCoclustering(n_clusters=3, *, svd_method='randomized', n_svd_vecs=None, mini_batch=False, init='k-means++', n_init=10, n_jobs='deprecated', random_state=None)\n",
      "     |  \n",
      "     |  Spectral Co-Clustering algorithm (Dhillon, 2001).\n",
      "     |  \n",
      "     |  Clusters rows and columns of an array `X` to solve the relaxed\n",
      "     |  normalized cut of the bipartite graph created from `X` as follows:\n",
      "     |  the edge between row vertex `i` and column vertex `j` has weight\n",
      "     |  `X[i, j]`.\n",
      "     |  \n",
      "     |  The resulting bicluster structure is block-diagonal, since each\n",
      "     |  row and each column belongs to exactly one bicluster.\n",
      "     |  \n",
      "     |  Supports sparse matrices, as long as they are nonnegative.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <spectral_coclustering>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_clusters : int, default=3\n",
      "     |      The number of biclusters to find.\n",
      "     |  \n",
      "     |  svd_method : {'randomized', 'arpack'}, default='randomized'\n",
      "     |      Selects the algorithm for finding singular vectors. May be\n",
      "     |      'randomized' or 'arpack'. If 'randomized', use\n",
      "     |      :func:`sklearn.utils.extmath.randomized_svd`, which may be faster\n",
      "     |      for large matrices. If 'arpack', use\n",
      "     |      :func:`scipy.sparse.linalg.svds`, which is more accurate, but\n",
      "     |      possibly slower in some cases.\n",
      "     |  \n",
      "     |  n_svd_vecs : int, default=None\n",
      "     |      Number of vectors to use in calculating the SVD. Corresponds\n",
      "     |      to `ncv` when `svd_method=arpack` and `n_oversamples` when\n",
      "     |      `svd_method` is 'randomized`.\n",
      "     |  \n",
      "     |  mini_batch : bool, default=False\n",
      "     |      Whether to use mini-batch k-means, which is faster but may get\n",
      "     |      different results.\n",
      "     |  \n",
      "     |  init : {'k-means++', 'random', or ndarray of shape             (n_clusters, n_features), default='k-means++'\n",
      "     |      Method for initialization of k-means algorithm; defaults to\n",
      "     |      'k-means++'.\n",
      "     |  \n",
      "     |  n_init : int, default=10\n",
      "     |      Number of random initializations that are tried with the\n",
      "     |      k-means algorithm.\n",
      "     |  \n",
      "     |      If mini-batch k-means is used, the best initialization is\n",
      "     |      chosen and the algorithm runs once. Otherwise, the algorithm\n",
      "     |      is run for each initialization and the best solution chosen.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to use for the computation. This works by breaking\n",
      "     |      down the pairwise matrix into n_jobs even slices and computing them in\n",
      "     |      parallel.\n",
      "     |  \n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.23\n",
      "     |          ``n_jobs`` was deprecated in version 0.23 and will be removed in\n",
      "     |          0.25.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used for randomizing the singular value decomposition and the k-means\n",
      "     |      initialization. Use an int to make the randomness deterministic.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  rows_ : array-like of shape (n_row_clusters, n_rows)\n",
      "     |      Results of the clustering. `rows[i, r]` is True if\n",
      "     |      cluster `i` contains row `r`. Available only after calling ``fit``.\n",
      "     |  \n",
      "     |  columns_ : array-like of shape (n_column_clusters, n_columns)\n",
      "     |      Results of the clustering, like `rows`.\n",
      "     |  \n",
      "     |  row_labels_ : array-like of shape (n_rows,)\n",
      "     |      The bicluster label of each row.\n",
      "     |  \n",
      "     |  column_labels_ : array-like of shape (n_cols,)\n",
      "     |      The bicluster label of each column.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import SpectralCoclustering\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[1, 1], [2, 1], [1, 0],\n",
      "     |  ...               [4, 7], [3, 5], [3, 6]])\n",
      "     |  >>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)\n",
      "     |  >>> clustering.row_labels_ #doctest: +SKIP\n",
      "     |  array([0, 1, 1, 0, 0, 0], dtype=int32)\n",
      "     |  >>> clustering.column_labels_ #doctest: +SKIP\n",
      "     |  array([0, 0], dtype=int32)\n",
      "     |  >>> clustering\n",
      "     |  SpectralCoclustering(n_clusters=2, random_state=0)\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  * Dhillon, Inderjit S, 2001. `Co-clustering documents and words using\n",
      "     |    bipartite spectral graph partitioning\n",
      "     |    <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.3011>`__.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpectralCoclustering\n",
      "     |      BaseSpectral\n",
      "     |      sklearn.base.BiclusterMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_clusters=3, *, svd_method='randomized', n_svd_vecs=None, mini_batch=False, init='k-means++', n_init=10, n_jobs='deprecated', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSpectral:\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Creates a biclustering for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BiclusterMixin:\n",
      "     |  \n",
      "     |  get_indices(self, i)\n",
      "     |      Row and column indices of the i'th bicluster.\n",
      "     |      \n",
      "     |      Only works if ``rows_`` and ``columns_`` attributes exist.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      i : int\n",
      "     |          The index of the cluster.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      row_ind : ndarray, dtype=np.intp\n",
      "     |          Indices of rows in the dataset that belong to the bicluster.\n",
      "     |      col_ind : ndarray, dtype=np.intp\n",
      "     |          Indices of columns in the dataset that belong to the bicluster.\n",
      "     |  \n",
      "     |  get_shape(self, i)\n",
      "     |      Shape of the i'th bicluster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      i : int\n",
      "     |          The index of the cluster.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      shape : tuple (int, int)\n",
      "     |          Number of rows and columns (resp.) in the bicluster.\n",
      "     |  \n",
      "     |  get_submatrix(self, i, data)\n",
      "     |      Return the submatrix corresponding to bicluster `i`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      i : int\n",
      "     |          The index of the cluster.\n",
      "     |      data : array-like\n",
      "     |          The data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      submatrix : ndarray\n",
      "     |          The submatrix corresponding to bicluster i.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Works with sparse matrices. Only works if ``rows_`` and\n",
      "     |      ``columns_`` attributes exist.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from sklearn.base.BiclusterMixin:\n",
      "     |  \n",
      "     |  biclusters_\n",
      "     |      Convenient way to get row and column indicators together.\n",
      "     |      \n",
      "     |      Returns the ``rows_`` and ``columns_`` members.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BiclusterMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "\n",
      "FUNCTIONS\n",
      "    affinity_propagation(S, *, preference=None, convergence_iter=15, max_iter=200, damping=0.5, copy=True, verbose=False, return_n_iter=False, random_state='warn')\n",
      "        Perform Affinity Propagation Clustering of data\n",
      "        \n",
      "        Read more in the :ref:`User Guide <affinity_propagation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        \n",
      "        S : array-like, shape (n_samples, n_samples)\n",
      "            Matrix of similarities between points\n",
      "        \n",
      "        preference : array-like, shape (n_samples,) or float, optional\n",
      "            Preferences for each point - points with larger values of\n",
      "            preferences are more likely to be chosen as exemplars. The number of\n",
      "            exemplars, i.e. of clusters, is influenced by the input preferences\n",
      "            value. If the preferences are not passed as arguments, they will be\n",
      "            set to the median of the input similarities (resulting in a moderate\n",
      "            number of clusters). For a smaller amount of clusters, this can be set\n",
      "            to the minimum value of the similarities.\n",
      "        \n",
      "        convergence_iter : int, optional, default: 15\n",
      "            Number of iterations with no change in the number\n",
      "            of estimated clusters that stops the convergence.\n",
      "        \n",
      "        max_iter : int, optional, default: 200\n",
      "            Maximum number of iterations\n",
      "        \n",
      "        damping : float, optional, default: 0.5\n",
      "            Damping factor between 0.5 and 1.\n",
      "        \n",
      "        copy : boolean, optional, default: True\n",
      "            If copy is False, the affinity matrix is modified inplace by the\n",
      "            algorithm, for memory efficiency\n",
      "        \n",
      "        verbose : boolean, optional, default: False\n",
      "            The verbosity level\n",
      "        \n",
      "        return_n_iter : bool, default False\n",
      "            Whether or not to return the number of iterations.\n",
      "        \n",
      "        random_state : int or np.random.RandomStateInstance, default: 0\n",
      "            Pseudo-random number generator to control the starting state.\n",
      "            Use an int for reproducible results across function calls.\n",
      "            See the :term:`Glossary <random_state>`.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "                this parameter was previously hardcoded as 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        \n",
      "        cluster_centers_indices : array, shape (n_clusters,)\n",
      "            index of clusters centers\n",
      "        \n",
      "        labels : array, shape (n_samples,)\n",
      "            cluster labels for each point\n",
      "        \n",
      "        n_iter : int\n",
      "            number of iterations run. Returned only if `return_n_iter` is\n",
      "            set to True.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n",
      "        <sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n",
      "        \n",
      "        When the algorithm does not converge, it returns an empty array as\n",
      "        ``cluster_center_indices`` and ``-1`` as label for each training sample.\n",
      "        \n",
      "        When all training samples have equal similarities and equal preferences,\n",
      "        the assignment of cluster centers and labels depends on the preference.\n",
      "        If the preference is smaller than the similarities, a single cluster center\n",
      "        and label ``0`` for every sample will be returned. Otherwise, every\n",
      "        training sample becomes its own cluster center and is assigned a unique\n",
      "        label.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n",
      "        Between Data Points\", Science Feb. 2007\n",
      "    \n",
      "    cluster_optics_dbscan(*, reachability, core_distances, ordering, eps)\n",
      "        Performs DBSCAN extraction for an arbitrary epsilon.\n",
      "        \n",
      "        Extracting the clusters runs in linear time. Note that this results in\n",
      "        ``labels_`` which are close to a :class:`~sklearn.cluster.DBSCAN` with\n",
      "        similar settings and ``eps``, only if ``eps`` is close to ``max_eps``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        reachability : array, shape (n_samples,)\n",
      "            Reachability distances calculated by OPTICS (``reachability_``)\n",
      "        \n",
      "        core_distances : array, shape (n_samples,)\n",
      "            Distances at which points become core (``core_distances_``)\n",
      "        \n",
      "        ordering : array, shape (n_samples,)\n",
      "            OPTICS ordered point indices (``ordering_``)\n",
      "        \n",
      "        eps : float\n",
      "            DBSCAN ``eps`` parameter. Must be set to < ``max_eps``. Results\n",
      "            will be close to DBSCAN algorithm if ``eps`` and ``max_eps`` are close\n",
      "            to one another.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        labels_ : array, shape (n_samples,)\n",
      "            The estimated labels.\n",
      "    \n",
      "    cluster_optics_xi(*, reachability, predecessor, ordering, min_samples, min_cluster_size=None, xi=0.05, predecessor_correction=True)\n",
      "        Automatically extract clusters according to the Xi-steep method.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        reachability : array, shape (n_samples,)\n",
      "            Reachability distances calculated by OPTICS (`reachability_`)\n",
      "        \n",
      "        predecessor : array, shape (n_samples,)\n",
      "            Predecessors calculated by OPTICS.\n",
      "        \n",
      "        ordering : array, shape (n_samples,)\n",
      "            OPTICS ordered point indices (`ordering_`)\n",
      "        \n",
      "        min_samples : int > 1 or float between 0 and 1\n",
      "            The same as the min_samples given to OPTICS. Up and down steep regions\n",
      "            can't have more then ``min_samples`` consecutive non-steep points.\n",
      "            Expressed as an absolute number or a fraction of the number of samples\n",
      "            (rounded to be at least 2).\n",
      "        \n",
      "        min_cluster_size : int > 1 or float between 0 and 1 (default=None)\n",
      "            Minimum number of samples in an OPTICS cluster, expressed as an\n",
      "            absolute number or a fraction of the number of samples (rounded to be\n",
      "            at least 2). If ``None``, the value of ``min_samples`` is used instead.\n",
      "        \n",
      "        xi : float, between 0 and 1, optional (default=0.05)\n",
      "            Determines the minimum steepness on the reachability plot that\n",
      "            constitutes a cluster boundary. For example, an upwards point in the\n",
      "            reachability plot is defined by the ratio from one point to its\n",
      "            successor being at most 1-xi.\n",
      "        \n",
      "        predecessor_correction : bool, optional (default=True)\n",
      "            Correct clusters based on the calculated predecessors.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        labels : array, shape (n_samples)\n",
      "            The labels assigned to samples. Points which are not included\n",
      "            in any cluster are labeled as -1.\n",
      "        \n",
      "        clusters : array, shape (n_clusters, 2)\n",
      "            The list of clusters in the form of ``[start, end]`` in each row, with\n",
      "            all indices inclusive. The clusters are ordered according to ``(end,\n",
      "            -start)`` (ascending) so that larger clusters encompassing smaller\n",
      "            clusters come after such nested smaller clusters. Since ``labels`` does\n",
      "            not reflect the hierarchy, usually ``len(clusters) >\n",
      "            np.unique(labels)``.\n",
      "    \n",
      "    compute_optics_graph(X, *, min_samples, max_eps, metric, p, metric_params, algorithm, leaf_size, n_jobs)\n",
      "        Computes the OPTICS reachability graph.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <optics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array, shape (n_samples, n_features), or (n_samples, n_samples)  if metric=’precomputed’.\n",
      "            A feature array, or array of distances between samples if\n",
      "            metric='precomputed'\n",
      "        \n",
      "        min_samples : int > 1 or float between 0 and 1\n",
      "            The number of samples in a neighborhood for a point to be considered\n",
      "            as a core point. Expressed as an absolute number or a fraction of the\n",
      "            number of samples (rounded to be at least 2).\n",
      "        \n",
      "        max_eps : float, optional (default=np.inf)\n",
      "            The maximum distance between two samples for one to be considered as\n",
      "            in the neighborhood of the other. Default value of ``np.inf`` will\n",
      "            identify clusters across all scales; reducing ``max_eps`` will result\n",
      "            in shorter run times.\n",
      "        \n",
      "        metric : string or callable, optional (default='minkowski')\n",
      "            Metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string. If metric is\n",
      "            \"precomputed\", X is assumed to be a distance matrix and must be square.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        p : integer, optional (default=2)\n",
      "            Parameter for the Minkowski metric from\n",
      "            :class:`sklearn.metrics.pairwise_distances`. When p = 1, this is\n",
      "            equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "            (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "        \n",
      "        metric_params : dict, optional (default=None)\n",
      "            Additional keyword arguments for the metric function.\n",
      "        \n",
      "        algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n",
      "            Algorithm used to compute the nearest neighbors:\n",
      "        \n",
      "            - 'ball_tree' will use :class:`BallTree`\n",
      "            - 'kd_tree' will use :class:`KDTree`\n",
      "            - 'brute' will use a brute-force search.\n",
      "            - 'auto' will attempt to decide the most appropriate algorithm\n",
      "              based on the values passed to :meth:`fit` method. (default)\n",
      "        \n",
      "            Note: fitting on sparse input will override the setting of\n",
      "            this parameter, using brute force.\n",
      "        \n",
      "        leaf_size : int, optional (default=30)\n",
      "            Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n",
      "            affect the speed of the construction and query, as well as the memory\n",
      "            required to store the tree. The optimal value depends on the\n",
      "            nature of the problem.\n",
      "        \n",
      "        n_jobs : int or None, optional (default=None)\n",
      "            The number of parallel jobs to run for neighbors search.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ordering_ : array, shape (n_samples,)\n",
      "            The cluster ordered list of sample indices.\n",
      "        \n",
      "        core_distances_ : array, shape (n_samples,)\n",
      "            Distance at which each sample becomes a core point, indexed by object\n",
      "            order. Points which will never be core have a distance of inf. Use\n",
      "            ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n",
      "        \n",
      "        reachability_ : array, shape (n_samples,)\n",
      "            Reachability distances per sample, indexed by object order. Use\n",
      "            ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n",
      "        \n",
      "        predecessor_ : array, shape (n_samples,)\n",
      "            Point that a sample was reached from, indexed by object order.\n",
      "            Seed points have a predecessor of -1.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n",
      "           and Jörg Sander. \"OPTICS: ordering points to identify the clustering\n",
      "           structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n",
      "    \n",
      "    dbscan(X, eps=0.5, *, min_samples=5, metric='minkowski', metric_params=None, algorithm='auto', leaf_size=30, p=2, sample_weight=None, n_jobs=None)\n",
      "        Perform DBSCAN clustering from vector array or distance matrix.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <dbscan>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse (CSR) matrix} of shape (n_samples, n_features) or             (n_samples, n_samples)\n",
      "            A feature array, or array of distances between samples if\n",
      "            ``metric='precomputed'``.\n",
      "        \n",
      "        eps : float, default=0.5\n",
      "            The maximum distance between two samples for one to be considered\n",
      "            as in the neighborhood of the other. This is not a maximum bound\n",
      "            on the distances of points within a cluster. This is the most\n",
      "            important DBSCAN parameter to choose appropriately for your data set\n",
      "            and distance function.\n",
      "        \n",
      "        min_samples : int, default=5\n",
      "            The number of samples (or total weight) in a neighborhood for a point\n",
      "            to be considered as a core point. This includes the point itself.\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string or callable, it must be one of\n",
      "            the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n",
      "            its metric parameter.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "            must be square during fit.\n",
      "            X may be a :term:`sparse graph <sparse graph>`,\n",
      "            in which case only \"nonzero\" elements may be considered neighbors.\n",
      "        \n",
      "        metric_params : dict, default=None\n",
      "            Additional keyword arguments for the metric function.\n",
      "        \n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "        algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "            The algorithm to be used by the NearestNeighbors module\n",
      "            to compute pointwise distances and find nearest neighbors.\n",
      "            See NearestNeighbors module documentation for details.\n",
      "        \n",
      "        leaf_size : int, default=30\n",
      "            Leaf size passed to BallTree or cKDTree. This can affect the speed\n",
      "            of the construction and query, as well as the memory required\n",
      "            to store the tree. The optimal value depends\n",
      "            on the nature of the problem.\n",
      "        \n",
      "        p : float, default=2\n",
      "            The power of the Minkowski metric to be used to calculate distance\n",
      "            between points.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Weight of each sample, such that a sample with a weight of at least\n",
      "            ``min_samples`` is by itself a core sample; a sample with negative\n",
      "            weight may inhibit its eps-neighbor from being core.\n",
      "            Note that weights are absolute, and default to 1.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of parallel jobs to run for neighbors search. ``None`` means\n",
      "            1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means\n",
      "            using all processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "            If precomputed distance are used, parallel execution is not available\n",
      "            and thus n_jobs will have no effect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        core_samples : ndarray of shape (n_core_samples,)\n",
      "            Indices of core samples.\n",
      "        \n",
      "        labels : ndarray of shape (n_samples,)\n",
      "            Cluster labels for each point.  Noisy samples are given the label -1.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        DBSCAN\n",
      "            An estimator interface for this clustering algorithm.\n",
      "        OPTICS\n",
      "            A similar estimator interface clustering at multiple values of eps. Our\n",
      "            implementation is optimized for memory usage.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For an example, see :ref:`examples/cluster/plot_dbscan.py\n",
      "        <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n",
      "        \n",
      "        This implementation bulk-computes all neighborhood queries, which increases\n",
      "        the memory complexity to O(n.d) where d is the average number of neighbors,\n",
      "        while original DBSCAN had memory complexity O(n). It may attract a higher\n",
      "        memory complexity when querying these nearest neighborhoods, depending\n",
      "        on the ``algorithm``.\n",
      "        \n",
      "        One way to avoid the query complexity is to pre-compute sparse\n",
      "        neighborhoods in chunks using\n",
      "        :func:`NearestNeighbors.radius_neighbors_graph\n",
      "        <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n",
      "        ``mode='distance'``, then using ``metric='precomputed'`` here.\n",
      "        \n",
      "        Another way to reduce memory and computation time is to remove\n",
      "        (near-)duplicate points and use ``sample_weight`` instead.\n",
      "        \n",
      "        :func:`cluster.optics <sklearn.cluster.optics>` provides a similar\n",
      "        clustering with lower memory usage.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Ester, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\n",
      "        Algorithm for Discovering Clusters in Large Spatial Databases with Noise\".\n",
      "        In: Proceedings of the 2nd International Conference on Knowledge Discovery\n",
      "        and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n",
      "        \n",
      "        Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n",
      "        DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\n",
      "        ACM Transactions on Database Systems (TODS), 42(3), 19.\n",
      "    \n",
      "    estimate_bandwidth(X, *, quantile=0.3, n_samples=None, random_state=0, n_jobs=None)\n",
      "        Estimate the bandwidth to use with the mean-shift algorithm.\n",
      "        \n",
      "        That this function takes time at least quadratic in n_samples. For large\n",
      "        datasets, it's wise to set that parameter to a small value.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            Input points.\n",
      "        \n",
      "        quantile : float, default=0.3\n",
      "            should be between [0, 1]\n",
      "            0.5 means that the median of all pairwise distances is used.\n",
      "        \n",
      "        n_samples : int, default=None\n",
      "            The number of samples to use. If not given, all samples are used.\n",
      "        \n",
      "        random_state : int, RandomState instance, default=None\n",
      "            The generator used to randomly select the samples from input points\n",
      "            for bandwidth estimation. Use an int to make the randomness\n",
      "            deterministic.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of parallel jobs to run for neighbors search.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        bandwidth : float\n",
      "            The bandwidth parameter.\n",
      "    \n",
      "    get_bin_seeds(X, bin_size, min_bin_freq=1)\n",
      "        Finds seeds for mean_shift.\n",
      "        \n",
      "        Finds seeds by first binning data onto a grid whose lines are\n",
      "        spaced bin_size apart, and then choosing those bins with at least\n",
      "        min_bin_freq points.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        \n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            Input points, the same points that will be used in mean_shift.\n",
      "        \n",
      "        bin_size : float\n",
      "            Controls the coarseness of the binning. Smaller values lead\n",
      "            to more seeding (which is computationally more expensive). If you're\n",
      "            not sure how to set this, set it to the value of the bandwidth used\n",
      "            in clustering.mean_shift.\n",
      "        \n",
      "        min_bin_freq : int, default=1\n",
      "            Only bins with at least min_bin_freq will be selected as seeds.\n",
      "            Raising this value decreases the number of seeds found, which\n",
      "            makes mean_shift computationally cheaper.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        bin_seeds : array-like of shape (n_samples, n_features)\n",
      "            Points used as initial kernel positions in clustering.mean_shift.\n",
      "    \n",
      "    k_means(X, n_clusters, *, sample_weight=None, init='k-means++', precompute_distances='deprecated', n_init=10, max_iter=300, verbose=False, tol=0.0001, random_state=None, copy_x=True, n_jobs='deprecated', algorithm='auto', return_n_iter=False)\n",
      "        K-means clustering algorithm.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <k_means>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse} matrix of shape (n_samples, n_features)\n",
      "            The observations to cluster. It must be noted that the data\n",
      "            will be converted to C ordering, which will cause a memory copy\n",
      "            if the given data is not C-contiguous.\n",
      "        \n",
      "        n_clusters : int\n",
      "            The number of clusters to form as well as the number of\n",
      "            centroids to generate.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            The weights for each observation in X. If None, all observations\n",
      "            are assigned equal weight\n",
      "        \n",
      "        init : {'k-means++', 'random', ndarray, callable}, default='k-means++'\n",
      "            Method for initialization:\n",
      "        \n",
      "            'k-means++' : selects initial cluster centers for k-mean\n",
      "            clustering in a smart way to speed up convergence. See section\n",
      "            Notes in k_init for more details.\n",
      "        \n",
      "            'random': choose `n_clusters` observations (rows) at random from data\n",
      "            for the initial centroids.\n",
      "        \n",
      "            If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
      "            and gives the initial centers.\n",
      "        \n",
      "            If a callable is passed, it should take arguments X, n_clusters and a\n",
      "            random state and return an initialization.\n",
      "        \n",
      "        precompute_distances : {'auto', True, False}\n",
      "            Precompute distances (faster but takes more memory).\n",
      "        \n",
      "            'auto' : do not precompute distances if n_samples * n_clusters > 12\n",
      "            million. This corresponds to about 100MB overhead per job using\n",
      "            double precision.\n",
      "        \n",
      "            True : always precompute distances\n",
      "        \n",
      "            False : never precompute distances\n",
      "        \n",
      "            .. deprecated:: 0.23\n",
      "                'precompute_distances' was deprecated in version 0.23 and will be\n",
      "                removed in 0.25. It has no effect.\n",
      "        \n",
      "        n_init : int, default=10\n",
      "            Number of time the k-means algorithm will be run with different\n",
      "            centroid seeds. The final results will be the best output of\n",
      "            n_init consecutive runs in terms of inertia.\n",
      "        \n",
      "        max_iter : int, default=300\n",
      "            Maximum number of iterations of the k-means algorithm to run.\n",
      "        \n",
      "        verbose : bool, default=False\n",
      "            Verbosity mode.\n",
      "        \n",
      "        tol : float, default=1e-4\n",
      "            Relative tolerance with regards to Frobenius norm of the difference\n",
      "            in the cluster centers of two consecutive iterations to declare\n",
      "            convergence.\n",
      "        \n",
      "        random_state : int, RandomState instance, default=None\n",
      "            Determines random number generation for centroid initialization. Use\n",
      "            an int to make the randomness deterministic.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        copy_x : bool, default=True\n",
      "            When pre-computing distances it is more numerically accurate to center\n",
      "            the data first. If copy_x is True (default), then the original data is\n",
      "            not modified. If False, the original data is modified, and put back\n",
      "            before the function returns, but small numerical differences may be\n",
      "            introduced by subtracting and then adding the data mean. Note that if\n",
      "            the original data is not C-contiguous, a copy will be made even if\n",
      "            copy_x is False. If the original data is sparse, but not in CSR format,\n",
      "            a copy will be made even if copy_x is False.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of OpenMP threads to use for the computation. Parallelism is\n",
      "            sample-wise on the main cython loop which assigns each sample to its\n",
      "            closest center.\n",
      "        \n",
      "            ``None`` or ``-1`` means using all processors.\n",
      "        \n",
      "            .. deprecated:: 0.23\n",
      "                ``n_jobs`` was deprecated in version 0.23 and will be removed in\n",
      "                0.25.\n",
      "        \n",
      "        algorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n",
      "            K-means algorithm to use. The classical EM-style algorithm is \"full\".\n",
      "            The \"elkan\" variation is more efficient on data with well-defined\n",
      "            clusters, by using the triangle inequality. However it's more memory\n",
      "            intensive due to the allocation of an extra array of shape\n",
      "            (n_samples, n_clusters).\n",
      "        \n",
      "            For now \"auto\" (kept for backward compatibiliy) chooses \"elkan\" but it\n",
      "            might change in the future for a better heuristic.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether or not to return the number of iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        centroid : ndarray of shape (n_clusters, n_features)\n",
      "            Centroids found at the last iteration of k-means.\n",
      "        \n",
      "        label : ndarray of shape (n_samples,)\n",
      "            label[i] is the code or index of the centroid the\n",
      "            i'th observation is closest to.\n",
      "        \n",
      "        inertia : float\n",
      "            The final value of the inertia criterion (sum of squared distances to\n",
      "            the closest centroid for all observations in the training set).\n",
      "        \n",
      "        best_n_iter : int\n",
      "            Number of iterations corresponding to the best results.\n",
      "            Returned only if `return_n_iter` is set to True.\n",
      "    \n",
      "    linkage_tree(X, connectivity=None, n_clusters=None, linkage='complete', affinity='euclidean', return_distance=False)\n",
      "        Linkage agglomerative clustering based on a Feature matrix.\n",
      "        \n",
      "        The inertia matrix uses a Heapq-based representation.\n",
      "        \n",
      "        This is the structured version, that takes into account some topological\n",
      "        structure between samples.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hierarchical_clustering>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array, shape (n_samples, n_features)\n",
      "            feature matrix representing n_samples samples to be clustered\n",
      "        \n",
      "        connectivity : sparse matrix (optional).\n",
      "            connectivity matrix. Defines for each sample the neighboring samples\n",
      "            following a given structure of the data. The matrix is assumed to\n",
      "            be symmetric and only the upper triangular half is used.\n",
      "            Default is None, i.e, the Ward algorithm is unstructured.\n",
      "        \n",
      "        n_clusters : int (optional)\n",
      "            Stop early the construction of the tree at n_clusters. This is\n",
      "            useful to decrease computation time if the number of clusters is\n",
      "            not small compared to the number of samples. In this case, the\n",
      "            complete tree is not computed, thus the 'children' output is of\n",
      "            limited use, and the 'parents' output should rather be used.\n",
      "            This option is valid only when specifying a connectivity matrix.\n",
      "        \n",
      "        linkage : {\"average\", \"complete\", \"single\"}, optional, default: \"complete\"\n",
      "            Which linkage criteria to use. The linkage criterion determines which\n",
      "            distance to use between sets of observation.\n",
      "                - average uses the average of the distances of each observation of\n",
      "                  the two sets\n",
      "                - complete or maximum linkage uses the maximum distances between\n",
      "                  all observations of the two sets.\n",
      "                - single uses the minimum of the distances between all observations\n",
      "                  of the two sets.\n",
      "        \n",
      "        affinity : string or callable, optional, default: \"euclidean\".\n",
      "            which metric to use. Can be \"euclidean\", \"manhattan\", or any\n",
      "            distance know to paired distance (see metric.pairwise)\n",
      "        \n",
      "        return_distance : bool, default False\n",
      "            whether or not to return the distances between the clusters.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        children : 2D array, shape (n_nodes-1, 2)\n",
      "            The children of each non-leaf node. Values less than `n_samples`\n",
      "            correspond to leaves of the tree which are the original samples.\n",
      "            A node `i` greater than or equal to `n_samples` is a non-leaf\n",
      "            node and has children `children_[i - n_samples]`. Alternatively\n",
      "            at the i-th iteration, children[i][0] and children[i][1]\n",
      "            are merged to form node `n_samples + i`\n",
      "        \n",
      "        n_connected_components : int\n",
      "            The number of connected components in the graph.\n",
      "        \n",
      "        n_leaves : int\n",
      "            The number of leaves in the tree.\n",
      "        \n",
      "        parents : 1D array, shape (n_nodes, ) or None\n",
      "            The parent of each node. Only returned when a connectivity matrix\n",
      "            is specified, elsewhere 'None' is returned.\n",
      "        \n",
      "        distances : ndarray, shape (n_nodes-1,)\n",
      "            Returned when return_distance is set to True.\n",
      "        \n",
      "            distances[i] refers to the distance between children[i][0] and\n",
      "            children[i][1] when they are merged.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        ward_tree : hierarchical clustering with ward linkage\n",
      "    \n",
      "    mean_shift(X, *, bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, max_iter=300, n_jobs=None)\n",
      "        Perform mean shift clustering of data using a flat kernel.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_shift>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        \n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            Input data.\n",
      "        \n",
      "        bandwidth : float, default=None\n",
      "            Kernel bandwidth.\n",
      "        \n",
      "            If bandwidth is not given, it is determined using a heuristic based on\n",
      "            the median of all pairwise distances. This will take quadratic time in\n",
      "            the number of samples. The sklearn.cluster.estimate_bandwidth function\n",
      "            can be used to do this more efficiently.\n",
      "        \n",
      "        seeds : array-like of shape (n_seeds, n_features) or None\n",
      "            Point used as initial kernel locations. If None and bin_seeding=False,\n",
      "            each data point is used as a seed. If None and bin_seeding=True,\n",
      "            see bin_seeding.\n",
      "        \n",
      "        bin_seeding : boolean, default=False\n",
      "            If true, initial kernel locations are not locations of all\n",
      "            points, but rather the location of the discretized version of\n",
      "            points, where points are binned onto a grid whose coarseness\n",
      "            corresponds to the bandwidth. Setting this option to True will speed\n",
      "            up the algorithm because fewer seeds will be initialized.\n",
      "            Ignored if seeds argument is not None.\n",
      "        \n",
      "        min_bin_freq : int, default=1\n",
      "           To speed up the algorithm, accept only those bins with at least\n",
      "           min_bin_freq points as seeds.\n",
      "        \n",
      "        cluster_all : bool, default=True\n",
      "            If true, then all points are clustered, even those orphans that are\n",
      "            not within any kernel. Orphans are assigned to the nearest kernel.\n",
      "            If false, then orphans are given cluster label -1.\n",
      "        \n",
      "        max_iter : int, default=300\n",
      "            Maximum number of iterations, per seed point before the clustering\n",
      "            operation terminates (for that seed point), if has not converged yet.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by computing\n",
      "            each of the n_init runs in parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               Parallel Execution using *n_jobs*.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        \n",
      "        cluster_centers : array, shape=[n_clusters, n_features]\n",
      "            Coordinates of cluster centers.\n",
      "        \n",
      "        labels : array, shape=[n_samples]\n",
      "            Cluster labels for each point.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For an example, see :ref:`examples/cluster/plot_mean_shift.py\n",
      "        <sphx_glr_auto_examples_cluster_plot_mean_shift.py>`.\n",
      "    \n",
      "    spectral_clustering(affinity, *, n_clusters=8, n_components=None, eigen_solver=None, random_state=None, n_init=10, eigen_tol=0.0, assign_labels='kmeans')\n",
      "        Apply clustering to a projection of the normalized Laplacian.\n",
      "        \n",
      "        In practice Spectral Clustering is very useful when the structure of\n",
      "        the individual clusters is highly non-convex or more generally when\n",
      "        a measure of the center and spread of the cluster is not a suitable\n",
      "        description of the complete cluster. For instance, when clusters are\n",
      "        nested circles on the 2D plane.\n",
      "        \n",
      "        If affinity is the adjacency matrix of a graph, this method can be\n",
      "        used to find normalized graph cuts.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <spectral_clustering>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        affinity : array-like or sparse matrix, shape: (n_samples, n_samples)\n",
      "            The affinity matrix describing the relationship of the samples to\n",
      "            embed. **Must be symmetric**.\n",
      "        \n",
      "            Possible examples:\n",
      "              - adjacency matrix of a graph,\n",
      "              - heat kernel of the pairwise distance matrix of the samples,\n",
      "              - symmetric k-nearest neighbours connectivity matrix of the samples.\n",
      "        \n",
      "        n_clusters : integer, optional\n",
      "            Number of clusters to extract.\n",
      "        \n",
      "        n_components : integer, optional, default is n_clusters\n",
      "            Number of eigen vectors to use for the spectral embedding\n",
      "        \n",
      "        eigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}\n",
      "            The eigenvalue decomposition strategy to use. AMG requires pyamg\n",
      "            to be installed. It can be faster on very large, sparse problems,\n",
      "            but may also lead to instabilities\n",
      "        \n",
      "        random_state : int, RandomState instance, default=None\n",
      "            A pseudo random number generator used for the initialization of the\n",
      "            lobpcg eigen vectors decomposition when eigen_solver == 'amg' and by\n",
      "            the K-Means initialization. Use an int to make the randomness\n",
      "            deterministic.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        n_init : int, optional, default: 10\n",
      "            Number of time the k-means algorithm will be run with different\n",
      "            centroid seeds. The final results will be the best output of\n",
      "            n_init consecutive runs in terms of inertia.\n",
      "        \n",
      "        eigen_tol : float, optional, default: 0.0\n",
      "            Stopping criterion for eigendecomposition of the Laplacian matrix\n",
      "            when using arpack eigen_solver.\n",
      "        \n",
      "        assign_labels : {'kmeans', 'discretize'}, default: 'kmeans'\n",
      "            The strategy to use to assign labels in the embedding\n",
      "            space.  There are two ways to assign labels after the laplacian\n",
      "            embedding.  k-means can be applied and is a popular choice. But it can\n",
      "            also be sensitive to initialization. Discretization is another\n",
      "            approach which is less sensitive to random initialization. See\n",
      "            the 'Multiclass spectral clustering' paper referenced below for\n",
      "            more details on the discretization approach.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        labels : array of integers, shape: n_samples\n",
      "            The labels of the clusters.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        - Normalized cuts and image segmentation, 2000\n",
      "          Jianbo Shi, Jitendra Malik\n",
      "          http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324\n",
      "        \n",
      "        - A Tutorial on Spectral Clustering, 2007\n",
      "          Ulrike von Luxburg\n",
      "          http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n",
      "        \n",
      "        - Multiclass spectral clustering, 2003\n",
      "          Stella X. Yu, Jianbo Shi\n",
      "          https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The graph should contain only one connect component, elsewhere\n",
      "        the results make little sense.\n",
      "        \n",
      "        This algorithm solves the normalized cut for k=2: it is a\n",
      "        normalized spectral clustering.\n",
      "    \n",
      "    ward_tree(X, *, connectivity=None, n_clusters=None, return_distance=False)\n",
      "        Ward clustering based on a Feature matrix.\n",
      "        \n",
      "        Recursively merges the pair of clusters that minimally increases\n",
      "        within-cluster variance.\n",
      "        \n",
      "        The inertia matrix uses a Heapq-based representation.\n",
      "        \n",
      "        This is the structured version, that takes into account some topological\n",
      "        structure between samples.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hierarchical_clustering>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array, shape (n_samples, n_features)\n",
      "            feature matrix representing n_samples samples to be clustered\n",
      "        \n",
      "        connectivity : sparse matrix (optional).\n",
      "            connectivity matrix. Defines for each sample the neighboring samples\n",
      "            following a given structure of the data. The matrix is assumed to\n",
      "            be symmetric and only the upper triangular half is used.\n",
      "            Default is None, i.e, the Ward algorithm is unstructured.\n",
      "        \n",
      "        n_clusters : int (optional)\n",
      "            Stop early the construction of the tree at n_clusters. This is\n",
      "            useful to decrease computation time if the number of clusters is\n",
      "            not small compared to the number of samples. In this case, the\n",
      "            complete tree is not computed, thus the 'children' output is of\n",
      "            limited use, and the 'parents' output should rather be used.\n",
      "            This option is valid only when specifying a connectivity matrix.\n",
      "        \n",
      "        return_distance : bool (optional)\n",
      "            If True, return the distance between the clusters.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        children : 2D array, shape (n_nodes-1, 2)\n",
      "            The children of each non-leaf node. Values less than `n_samples`\n",
      "            correspond to leaves of the tree which are the original samples.\n",
      "            A node `i` greater than or equal to `n_samples` is a non-leaf\n",
      "            node and has children `children_[i - n_samples]`. Alternatively\n",
      "            at the i-th iteration, children[i][0] and children[i][1]\n",
      "            are merged to form node `n_samples + i`\n",
      "        \n",
      "        n_connected_components : int\n",
      "            The number of connected components in the graph.\n",
      "        \n",
      "        n_leaves : int\n",
      "            The number of leaves in the tree\n",
      "        \n",
      "        parents : 1D array, shape (n_nodes, ) or None\n",
      "            The parent of each node. Only returned when a connectivity matrix\n",
      "            is specified, elsewhere 'None' is returned.\n",
      "        \n",
      "        distances : 1D array, shape (n_nodes-1, )\n",
      "            Only returned if return_distance is set to True (for compatibility).\n",
      "            The distances between the centers of the nodes. `distances[i]`\n",
      "            corresponds to a weighted euclidean distance between\n",
      "            the nodes `children[i, 1]` and `children[i, 2]`. If the nodes refer to\n",
      "            leaves of the tree, then `distances[i]` is their unweighted euclidean\n",
      "            distance. Distances are updated in the following way\n",
      "            (from scipy.hierarchy.linkage):\n",
      "        \n",
      "            The new entry :math:`d(u,v)` is computed as follows,\n",
      "        \n",
      "            .. math::\n",
      "        \n",
      "               d(u,v) = \\sqrt{\\frac{|v|+|s|}\n",
      "                                   {T}d(v,s)^2\n",
      "                            + \\frac{|v|+|t|}\n",
      "                                   {T}d(v,t)^2\n",
      "                            - \\frac{|v|}\n",
      "                                   {T}d(s,t)^2}\n",
      "        \n",
      "            where :math:`u` is the newly joined cluster consisting of\n",
      "            clusters :math:`s` and :math:`t`, :math:`v` is an unused\n",
      "            cluster in the forest, :math:`T=|v|+|s|+|t|`, and\n",
      "            :math:`|*|` is the cardinality of its argument. This is also\n",
      "            known as the incremental algorithm.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['AffinityPropagation', 'AgglomerativeClustering', 'Birch', ...\n",
      "\n",
      "FILE\n",
      "    c:\\anaconda\\envs\\textanalytics\\lib\\site-packages\\sklearn\\cluster\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module sklearn.metrics.pairwise in sklearn.metrics:\n",
      "\n",
      "NAME\n",
      "    sklearn.metrics.pairwise - # -*- coding: utf-8 -*-\n",
      "\n",
      "FUNCTIONS\n",
      "    additive_chi2_kernel(X, Y=None)\n",
      "        Computes the additive chi-squared kernel between observations in X and\n",
      "        Y.\n",
      "        \n",
      "        The chi-squared kernel is computed between each pair of rows in X and Y.  X\n",
      "        and Y have to be non-negative. This kernel is most commonly applied to\n",
      "        histograms.\n",
      "        \n",
      "        The chi-squared kernel is given by::\n",
      "        \n",
      "            k(x, y) = -Sum [(x - y)^2 / (x + y)]\n",
      "        \n",
      "        It can be interpreted as a weighted difference per entry.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <chi2_kernel>`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        As the negative of a distance, this kernel is only conditionally positive\n",
      "        definite.\n",
      "        \n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_X, n_features)\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            If `None`, uses `Y=X`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kernel_matrix : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        chi2_kernel : The exponentiated version of the kernel, which is usually\n",
      "            preferable.\n",
      "        sklearn.kernel_approximation.AdditiveChi2Sampler : A Fourier approximation\n",
      "            to this kernel.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C.\n",
      "          Local features and kernels for classification of texture and object\n",
      "          categories: A comprehensive study\n",
      "          International Journal of Computer Vision 2007\n",
      "          https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf\n",
      "    \n",
      "    check_paired_arrays(X, Y)\n",
      "        Set X and Y appropriately and checks inputs for paired distances.\n",
      "        \n",
      "        All paired distance metrics should use this function first to assert that\n",
      "        the given parameters are correct and safe to use.\n",
      "        \n",
      "        Specifically, this function first ensures that both X and Y are arrays,\n",
      "        then checks that they are at least two dimensional while ensuring that\n",
      "        their elements are floats. Finally, the function checks that the size\n",
      "        of the dimensions of the two arrays are equal.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        safe_X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            An array equal to X, guaranteed to be a numpy array.\n",
      "        \n",
      "        safe_Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n",
      "            An array equal to Y if Y was not None, guaranteed to be a numpy array.\n",
      "            If Y was None, safe_Y will be a pointer to X.\n",
      "    \n",
      "    check_pairwise_arrays(X, Y, *, precomputed=False, dtype=None, accept_sparse='csr', force_all_finite=True, copy=False)\n",
      "        Set X and Y appropriately and checks inputs.\n",
      "        \n",
      "        If Y is None, it is set as a pointer to X (i.e. not a copy).\n",
      "        If Y is given, this does not happen.\n",
      "        All distance metrics should use this function first to assert that the\n",
      "        given parameters are correct and safe to use.\n",
      "        \n",
      "        Specifically, this function first ensures that both X and Y are arrays,\n",
      "        then checks that they are at least two dimensional while ensuring that\n",
      "        their elements are floats (or dtype if provided). Finally, the function\n",
      "        checks that the size of the second dimension of the two arrays is equal, or\n",
      "        the equivalent check for a precomputed distance matrix.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n",
      "        \n",
      "        precomputed : bool, default=False\n",
      "            True if X is to be treated as precomputed distances to the samples in\n",
      "            Y.\n",
      "        \n",
      "        dtype : str, type, list of type, default=None\n",
      "            Data type required for X and Y. If None, the dtype will be an\n",
      "            appropriate float type selected by _return_float_dtype.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        accept_sparse : str, bool or list/tuple of str, default='csr'\n",
      "            String[s] representing allowed sparse matrix formats, such as 'csc',\n",
      "            'csr', etc. If the input is sparse but not in the allowed format,\n",
      "            it will be converted to the first listed format. True allows the input\n",
      "            to be any format. False means that a sparse matrix input will\n",
      "            raise an error.\n",
      "        \n",
      "        force_all_finite : bool or 'allow-nan', default=True\n",
      "            Whether to raise an error on np.inf, np.nan, pd.NA in array. The\n",
      "            possibilities are:\n",
      "        \n",
      "            - True: Force all values of array to be finite.\n",
      "            - False: accepts np.inf, np.nan, pd.NA in array.\n",
      "            - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n",
      "              cannot be infinite.\n",
      "        \n",
      "            .. versionadded:: 0.22\n",
      "               ``force_all_finite`` accepts the string ``'allow-nan'``.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Accepts `pd.NA` and converts it into `np.nan`.\n",
      "        \n",
      "        copy : bool, default=False\n",
      "            Whether a forced copy will be triggered. If copy=False, a copy might\n",
      "            be triggered by a conversion.\n",
      "        \n",
      "            .. versionadded:: 0.22\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        safe_X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            An array equal to X, guaranteed to be a numpy array.\n",
      "        \n",
      "        safe_Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n",
      "            An array equal to Y if Y was not None, guaranteed to be a numpy array.\n",
      "            If Y was None, safe_Y will be a pointer to X.\n",
      "    \n",
      "    chi2_kernel(X, Y=None, gamma=1.0)\n",
      "        Computes the exponential chi-squared kernel X and Y.\n",
      "        \n",
      "        The chi-squared kernel is computed between each pair of rows in X and Y.  X\n",
      "        and Y have to be non-negative. This kernel is most commonly applied to\n",
      "        histograms.\n",
      "        \n",
      "        The chi-squared kernel is given by::\n",
      "        \n",
      "            k(x, y) = exp(-gamma Sum [(x - y)^2 / (x + y)])\n",
      "        \n",
      "        It can be interpreted as a weighted difference per entry.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <chi2_kernel>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_X, n_features)\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "        \n",
      "        gamma : float, default=1.\n",
      "            Scaling parameter of the chi2 kernel.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kernel_matrix : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        additive_chi2_kernel : The additive version of this kernel.\n",
      "        sklearn.kernel_approximation.AdditiveChi2Sampler : A Fourier approximation\n",
      "            to the additive version of this kernel.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C.\n",
      "          Local features and kernels for classification of texture and object\n",
      "          categories: A comprehensive study\n",
      "          International Journal of Computer Vision 2007\n",
      "          https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf\n",
      "    \n",
      "    cosine_distances(X, Y=None)\n",
      "        Compute cosine distance between samples in X and Y.\n",
      "        \n",
      "        Cosine distance is defined as 1.0 minus the cosine similarity.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            Matrix `X`.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None\n",
      "            Matrix `Y`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distance matrix : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        cosine_similarity\n",
      "        scipy.spatial.distance.cosine : Dense matrices only.\n",
      "    \n",
      "    cosine_similarity(X, Y=None, dense_output=True)\n",
      "        Compute cosine similarity between samples in X and Y.\n",
      "        \n",
      "        Cosine similarity, or the cosine kernel, computes similarity as the\n",
      "        normalized dot product of X and Y:\n",
      "        \n",
      "            K(X, Y) = <X, Y> / (||X||*||Y||)\n",
      "        \n",
      "        On L2-normalized data, this function is equivalent to linear_kernel.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cosine_similarity>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {ndarray, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            Input data.\n",
      "        \n",
      "        Y : {ndarray, sparse matrix} of shape (n_samples_Y, n_features),             default=None\n",
      "            Input data. If ``None``, the output will be the pairwise\n",
      "            similarities between all samples in ``X``.\n",
      "        \n",
      "        dense_output : bool, default=True\n",
      "            Whether to return dense output even when the input is sparse. If\n",
      "            ``False``, the output is sparse if both input arrays are sparse.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               parameter ``dense_output`` for dense output.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kernel matrix : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "    \n",
      "    distance_metrics()\n",
      "        Valid metrics for pairwise_distances.\n",
      "        \n",
      "        This function simply returns the valid pairwise distance metrics.\n",
      "        It exists to allow for a description of the mapping for\n",
      "        each of the valid strings.\n",
      "        \n",
      "        The valid distance metrics, and the function they map to, are:\n",
      "        \n",
      "        =============== ========================================\n",
      "        metric          Function\n",
      "        =============== ========================================\n",
      "        'cityblock'     metrics.pairwise.manhattan_distances\n",
      "        'cosine'        metrics.pairwise.cosine_distances\n",
      "        'euclidean'     metrics.pairwise.euclidean_distances\n",
      "        'haversine'     metrics.pairwise.haversine_distances\n",
      "        'l1'            metrics.pairwise.manhattan_distances\n",
      "        'l2'            metrics.pairwise.euclidean_distances\n",
      "        'manhattan'     metrics.pairwise.manhattan_distances\n",
      "        'nan_euclidean' metrics.pairwise.nan_euclidean_distances\n",
      "        =============== ========================================\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "    \n",
      "    euclidean_distances(X, Y=None, *, Y_norm_squared=None, squared=False, X_norm_squared=None)\n",
      "        Compute the distance matrix between each pair from a vector array X and Y.\n",
      "        \n",
      "        For efficiency reasons, the euclidean distance between a pair of row\n",
      "        vector x and y is computed as::\n",
      "        \n",
      "            dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))\n",
      "        \n",
      "        This formulation has two advantages over other ways of computing distances.\n",
      "        First, it is computationally efficient when dealing with sparse data.\n",
      "        Second, if one argument varies but the other remains unchanged, then\n",
      "        `dot(x, x)` and/or `dot(y, y)` can be pre-computed.\n",
      "        \n",
      "        However, this is not the most precise way of doing this computation,\n",
      "        because this equation potentially suffers from \"catastrophic cancellation\".\n",
      "        Also, the distance matrix returned by this function may not be exactly\n",
      "        symmetric as required by, e.g., ``scipy.spatial.distance`` functions.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            An array where each row is a sample and each column is a feature.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None\n",
      "            An array where each row is a sample and each column is a feature.\n",
      "            If `None`, method uses `Y=X`.\n",
      "        \n",
      "        Y_norm_squared : array-like of shape (n_samples_Y,) or (n_samples_Y, 1)             or (1, n_samples_Y), default=None\n",
      "            Pre-computed dot-products of vectors in Y (e.g.,\n",
      "            ``(Y**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        squared : bool, default=False\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        X_norm_squared : array-like of shape (n_samples_X,) or (n_samples_X, 1)             or (1, n_samples_X), default=None\n",
      "            Pre-computed dot-products of vectors in X (e.g.,\n",
      "            ``(X**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "            Returns the distances between the row vectors of `X`\n",
      "            and the row vectors of `Y`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        paired_distances : Distances betweens pairs of elements of X and Y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        To achieve a better accuracy, `X_norm_squared` and `Y_norm_squared` may be\n",
      "        unused if they are passed as `np.float32`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import euclidean_distances\n",
      "        >>> X = [[0, 1], [1, 1]]\n",
      "        >>> # distance between rows of X\n",
      "        >>> euclidean_distances(X, X)\n",
      "        array([[0., 1.],\n",
      "               [1., 0.]])\n",
      "        >>> # get distance to origin\n",
      "        >>> euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "    \n",
      "    haversine_distances(X, Y=None)\n",
      "        Compute the Haversine distance between samples in X and Y.\n",
      "        \n",
      "        The Haversine (or great circle) distance is the angular distance between\n",
      "        two points on the surface of a sphere. The first coordinate of each point\n",
      "        is assumed to be the latitude, the second is the longitude, given\n",
      "        in radians. The dimension of the data must be 2.\n",
      "        \n",
      "        .. math::\n",
      "           D(x, y) = 2\\arcsin[\\sqrt{\\sin^2((x1 - y1) / 2)\n",
      "                                    + \\cos(x1)\\cos(y1)\\sin^2((x2 - y2) / 2)}]\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_X, 2)\n",
      "        \n",
      "        Y : array-like of shape (n_samples_Y, 2), default=None\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distance : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        As the Earth is nearly spherical, the haversine formula provides a good\n",
      "        approximation of the distance between two points of the Earth surface, with\n",
      "        a less than 1% error on average.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We want to calculate the distance between the Ezeiza Airport\n",
      "        (Buenos Aires, Argentina) and the Charles de Gaulle Airport (Paris,\n",
      "        France).\n",
      "        \n",
      "        >>> from sklearn.metrics.pairwise import haversine_distances\n",
      "        >>> from math import radians\n",
      "        >>> bsas = [-34.83333, -58.5166646]\n",
      "        >>> paris = [49.0083899664, 2.53844117956]\n",
      "        >>> bsas_in_radians = [radians(_) for _ in bsas]\n",
      "        >>> paris_in_radians = [radians(_) for _ in paris]\n",
      "        >>> result = haversine_distances([bsas_in_radians, paris_in_radians])\n",
      "        >>> result * 6371000/1000  # multiply by Earth radius to get kilometers\n",
      "        array([[    0.        , 11099.54035582],\n",
      "               [11099.54035582,     0.        ]])\n",
      "    \n",
      "    kernel_metrics()\n",
      "        Valid metrics for pairwise_kernels.\n",
      "        \n",
      "        This function simply returns the valid pairwise distance metrics.\n",
      "        It exists, however, to allow for a verbose description of the mapping for\n",
      "        each of the valid strings.\n",
      "        \n",
      "        The valid distance metrics, and the function they map to, are:\n",
      "          ===============   ========================================\n",
      "          metric            Function\n",
      "          ===============   ========================================\n",
      "          'additive_chi2'   sklearn.pairwise.additive_chi2_kernel\n",
      "          'chi2'            sklearn.pairwise.chi2_kernel\n",
      "          'linear'          sklearn.pairwise.linear_kernel\n",
      "          'poly'            sklearn.pairwise.polynomial_kernel\n",
      "          'polynomial'      sklearn.pairwise.polynomial_kernel\n",
      "          'rbf'             sklearn.pairwise.rbf_kernel\n",
      "          'laplacian'       sklearn.pairwise.laplacian_kernel\n",
      "          'sigmoid'         sklearn.pairwise.sigmoid_kernel\n",
      "          'cosine'          sklearn.pairwise.cosine_similarity\n",
      "          ===============   ========================================\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "    \n",
      "    laplacian_kernel(X, Y=None, gamma=None)\n",
      "        Compute the laplacian kernel between X and Y.\n",
      "        \n",
      "        The laplacian kernel is defined as::\n",
      "        \n",
      "            K(x, y) = exp(-gamma ||x-y||_1)\n",
      "        \n",
      "        for each pair of rows x in X and y in Y.\n",
      "        Read more in the :ref:`User Guide <laplacian_kernel>`.\n",
      "        \n",
      "        .. versionadded:: 0.17\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_features)\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            If `None`, uses `Y=X`.\n",
      "        \n",
      "        gamma : float, default=None\n",
      "            If None, defaults to 1.0 / n_features.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kernel_matrix : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "    \n",
      "    linear_kernel(X, Y=None, dense_output=True)\n",
      "        Compute the linear kernel between X and Y.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <linear_kernel>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_features)\n",
      "            A feature array.\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            An optional second feature array. If `None`, uses `Y=X`.\n",
      "        \n",
      "        dense_output : bool, default=True\n",
      "            Whether to return dense output even when the input is sparse. If\n",
      "            ``False``, the output is sparse if both input arrays are sparse.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Gram matrix : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "            The Gram matrix of the linear kernel, i.e. `X @ Y.T`.\n",
      "    \n",
      "    manhattan_distances(X, Y=None, *, sum_over_features=True)\n",
      "        Compute the L1 distances between the vectors in X and Y.\n",
      "        \n",
      "        With sum_over_features equal to False it returns the componentwise\n",
      "        distances.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_X, n_features)\n",
      "        \n",
      "        Y : array-like of shape (n_samples_Y, n_features), default=None\n",
      "            If `None`, uses `Y=X`.\n",
      "        \n",
      "        sum_over_features : bool, default=True\n",
      "            If True the function returns the pairwise distance matrix\n",
      "            else it returns the componentwise L1 pairwise-distances.\n",
      "            Not supported for sparse matrix inputs.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        D : ndarray of shape (n_samples_X * n_samples_Y, n_features) or             (n_samples_X, n_samples_Y)\n",
      "            If sum_over_features is False shape is\n",
      "            (n_samples_X * n_samples_Y, n_features) and D contains the\n",
      "            componentwise L1 pairwise-distances (ie. absolute difference),\n",
      "            else shape is (n_samples_X, n_samples_Y) and D contains\n",
      "            the pairwise L1 distances.\n",
      "        \n",
      "        Notes\n",
      "        --------\n",
      "        When X and/or Y are CSR sparse matrices and they are not already\n",
      "        in canonical format, this function modifies them in-place to\n",
      "        make them canonical.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import manhattan_distances\n",
      "        >>> manhattan_distances([[3]], [[3]])\n",
      "        array([[0.]])\n",
      "        >>> manhattan_distances([[3]], [[2]])\n",
      "        array([[1.]])\n",
      "        >>> manhattan_distances([[2]], [[3]])\n",
      "        array([[1.]])\n",
      "        >>> manhattan_distances([[1, 2], [3, 4]],         [[1, 2], [0, 3]])\n",
      "        array([[0., 2.],\n",
      "               [4., 4.]])\n",
      "        >>> import numpy as np\n",
      "        >>> X = np.ones((1, 2))\n",
      "        >>> y = np.full((2, 2), 2.)\n",
      "        >>> manhattan_distances(X, y, sum_over_features=False)\n",
      "        array([[1., 1.],\n",
      "               [1., 1.]])\n",
      "    \n",
      "    nan_euclidean_distances(X, Y=None, *, squared=False, missing_values=nan, copy=True)\n",
      "        Calculate the euclidean distances in the presence of missing values.\n",
      "        \n",
      "        Compute the euclidean distance between each pair of samples in X and Y,\n",
      "        where Y=X is assumed if Y=None. When calculating the distance between a\n",
      "        pair of samples, this formulation ignores feature coordinates with a\n",
      "        missing value in either sample and scales up the weight of the remaining\n",
      "        coordinates:\n",
      "        \n",
      "            dist(x,y) = sqrt(weight * sq. distance from present coordinates)\n",
      "            where,\n",
      "            weight = Total # of coordinates / # of present coordinates\n",
      "        \n",
      "        For example, the distance between ``[3, na, na, 6]`` and ``[1, na, 4, 5]``\n",
      "        is:\n",
      "        \n",
      "            .. math::\n",
      "                \\sqrt{\\frac{4}{2}((3-1)^2 + (6-5)^2)}\n",
      "        \n",
      "        If all the coordinates are missing or if there are no common present\n",
      "        coordinates then NaN is returned for that pair.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        .. versionadded:: 0.22\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape=(n_samples_X, n_features)\n",
      "        \n",
      "        Y : array-like of shape=(n_samples_Y, n_features), default=None\n",
      "        \n",
      "        squared : bool, default=False\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        missing_values : np.nan or int, default=np.nan\n",
      "            Representation of missing value.\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            Make and use a deep copy of X and Y (if Y exists).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        paired_distances : Distances between pairs of elements of X and Y.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import nan_euclidean_distances\n",
      "        >>> nan = float(\"NaN\")\n",
      "        >>> X = [[0, 1], [1, nan]]\n",
      "        >>> nan_euclidean_distances(X, X) # distance between rows of X\n",
      "        array([[0.        , 1.41421356],\n",
      "               [1.41421356, 0.        ]])\n",
      "        \n",
      "        >>> # get distance to origin\n",
      "        >>> nan_euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * John K. Dixon, \"Pattern Recognition with Partly Missing Data\",\n",
      "          IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue:\n",
      "          10, pp. 617 - 621, Oct. 1979.\n",
      "          http://ieeexplore.ieee.org/abstract/document/4310090/\n",
      "    \n",
      "    paired_cosine_distances(X, Y)\n",
      "        Computes the paired cosine distances between X and Y.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "        \n",
      "        Y : array-like of shape (n_samples, n_features)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples,)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The cosine distance is equivalent to the half the squared\n",
      "        euclidean distance if each sample is normalized to unit norm.\n",
      "    \n",
      "    paired_distances(X, Y, *, metric='euclidean', **kwds)\n",
      "        Computes the paired distances between X and Y.\n",
      "        \n",
      "        Computes the distances between (X[0], Y[0]), (X[1], Y[1]), etc...\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples, n_features)\n",
      "            Array 1 for distance computation.\n",
      "        \n",
      "        Y : ndarray of shape (n_samples, n_features)\n",
      "            Array 2 for distance computation.\n",
      "        \n",
      "        metric : str or callable, default=\"euclidean\"\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            specified in PAIRED_DISTANCES, including \"euclidean\",\n",
      "            \"manhattan\", or \"cosine\".\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples,)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pairwise_distances : Computes the distance between every pair of samples.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import paired_distances\n",
      "        >>> X = [[0, 1], [1, 1]]\n",
      "        >>> Y = [[0, 1], [2, 1]]\n",
      "        >>> paired_distances(X, Y)\n",
      "        array([0., 1.])\n",
      "    \n",
      "    paired_euclidean_distances(X, Y)\n",
      "        Computes the paired euclidean distances between X and Y.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "        \n",
      "        Y : array-like of shape (n_samples, n_features)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples,)\n",
      "    \n",
      "    paired_manhattan_distances(X, Y)\n",
      "        Compute the L1 distances between the vectors in X and Y.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "        \n",
      "        Y : array-like of shape (n_samples, n_features)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples,)\n",
      "    \n",
      "    pairwise_distances(X, Y=None, metric='euclidean', *, n_jobs=None, force_all_finite=True, **kwds)\n",
      "        Compute the distance matrix from a vector array X and optional Y.\n",
      "        \n",
      "        This method takes either a vector array or a distance matrix, and returns\n",
      "        a distance matrix. If the input is a vector array, the distances are\n",
      "        computed. If the input is a distances matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a distance matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        distance between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "        \n",
      "        - From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "          'manhattan']. These metrics support sparse matrix\n",
      "          inputs.\n",
      "          ['nan_euclidean'] but it does not yet support sparse matrices.\n",
      "        \n",
      "        - From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',\n",
      "          'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n",
      "          'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n",
      "          See the documentation for scipy.spatial.distance for details on these\n",
      "          metrics. These metrics do not support sparse matrix inputs.\n",
      "        \n",
      "        Note that in the case of 'cityblock', 'cosine' and 'euclidean' (which are\n",
      "        valid scipy.spatial.distance metrics), the scikit-learn implementation\n",
      "        will be used, which is faster and has support for sparse matrices (except\n",
      "        for 'cityblock'). For a verbose description of the metrics from\n",
      "        scikit-learn, see the __doc__ of the sklearn.pairwise.distance_metrics\n",
      "        function.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "            The shape of the array should be (n_samples_X, n_samples_X) if\n",
      "            metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      "            a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        force_all_finite : bool or 'allow-nan', default=True\n",
      "            Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored\n",
      "            for a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``. The\n",
      "            possibilities are:\n",
      "        \n",
      "            - True: Force all values of array to be finite.\n",
      "            - False: accepts np.inf, np.nan, pd.NA in array.\n",
      "            - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n",
      "              cannot be infinite.\n",
      "        \n",
      "            .. versionadded:: 0.22\n",
      "               ``force_all_finite`` accepts the string ``'allow-nan'``.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Accepts `pd.NA` and converts it into `np.nan`.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        D : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)\n",
      "            A distance matrix D such that D_{i, j} is the distance between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then D_{i, j} is the distance between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pairwise_distances_chunked : Performs the same calculation as this\n",
      "            function, but returns a generator of chunks of the distance matrix, in\n",
      "            order to limit memory usage.\n",
      "        paired_distances : Computes the distances between corresponding elements\n",
      "            of two arrays.\n",
      "    \n",
      "    pairwise_distances_argmin(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance).\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        This function works with dense 2D arrays only.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_X, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : array-like of shape (n_samples_Y, n_features)\n",
      "            Arrays containing points.\n",
      "        \n",
      "        axis : int, default=1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : str or callable, default=\"euclidean\"\n",
      "            Metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        metric_kwargs : dict, default=None\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : numpy.ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        sklearn.metrics.pairwise_distances\n",
      "        sklearn.metrics.pairwise_distances_argmin_min\n",
      "    \n",
      "    pairwise_distances_argmin_min(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance). The minimal distances are\n",
      "        also returned.\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            (pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),\n",
      "             pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        axis : int, default=1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            Metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        metric_kwargs : dict, default=None\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        distances : ndarray\n",
      "            distances[i] is the distance between the i-th row in X and the\n",
      "            argmin[i]-th row in Y.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        sklearn.metrics.pairwise_distances\n",
      "        sklearn.metrics.pairwise_distances_argmin\n",
      "    \n",
      "    pairwise_distances_chunked(X, Y=None, *, reduce_func=None, metric='euclidean', n_jobs=None, working_memory=None, **kwds)\n",
      "        Generate a distance matrix chunk by chunk with optional reduction.\n",
      "        \n",
      "        In cases where not all of a pairwise distance matrix needs to be stored at\n",
      "        once, this is used to calculate pairwise distances in\n",
      "        ``working_memory``-sized chunks.  If ``reduce_func`` is given, it is run\n",
      "        on each chunk and its return values are concatenated into lists, arrays\n",
      "        or sparse matrices.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "            The shape the array should be (n_samples_X, n_samples_X) if\n",
      "            metric='precomputed' and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        reduce_func : callable, default=None\n",
      "            The function which is applied on each chunk of the distance matrix,\n",
      "            reducing it to needed values.  ``reduce_func(D_chunk, start)``\n",
      "            is called repeatedly, where ``D_chunk`` is a contiguous vertical\n",
      "            slice of the pairwise distance matrix, starting at row ``start``.\n",
      "            It should return one of: None; an array, a list, or a sparse matrix\n",
      "            of length ``D_chunk.shape[0]``; or a tuple of such objects. Returning\n",
      "            None is useful for in-place operations, rather than reductions.\n",
      "        \n",
      "            If None, pairwise_distances_chunked returns a generator of vertical\n",
      "            chunks of the distance matrix.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      "            a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            The sought maximum memory for temporary distance matrix chunks.\n",
      "            When None (default), the value of\n",
      "            ``sklearn.get_config()['working_memory']`` is used.\n",
      "        \n",
      "        `**kwds` : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Yields\n",
      "        ------\n",
      "        D_chunk : {ndarray, sparse matrix}\n",
      "            A contiguous slice of distance matrix, optionally processed by\n",
      "            ``reduce_func``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Without reduce_func:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import pairwise_distances_chunked\n",
      "        >>> X = np.random.RandomState(0).rand(5, 3)\n",
      "        >>> D_chunk = next(pairwise_distances_chunked(X))\n",
      "        >>> D_chunk\n",
      "        array([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],\n",
      "               [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],\n",
      "               [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],\n",
      "               [0.19..., 0.41..., 0.44..., 0.  ..., 0.51...],\n",
      "               [0.57..., 0.76..., 0.90..., 0.51..., 0.  ...]])\n",
      "        \n",
      "        Retrieve all neighbors and average distance within radius r:\n",
      "        \n",
      "        >>> r = .2\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r) for d in D_chunk]\n",
      "        ...     avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n",
      "        ...     return neigh, avg_dist\n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n",
      "        >>> neigh, avg_dist = next(gen)\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]\n",
      "        >>> avg_dist\n",
      "        array([0.039..., 0.        , 0.        , 0.039..., 0.        ])\n",
      "        \n",
      "        Where r is defined per sample, we need to make use of ``start``:\n",
      "        \n",
      "        >>> r = [.2, .4, .4, .3, .1]\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r[i])\n",
      "        ...              for i, d in enumerate(D_chunk, start)]\n",
      "        ...     return neigh\n",
      "        >>> neigh = next(pairwise_distances_chunked(X, reduce_func=reduce_func))\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]\n",
      "        \n",
      "        Force row-by-row generation by reducing ``working_memory``:\n",
      "        \n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func,\n",
      "        ...                                  working_memory=0)\n",
      "        >>> next(gen)\n",
      "        [array([0, 3])]\n",
      "        >>> next(gen)\n",
      "        [array([0, 1])]\n",
      "    \n",
      "    pairwise_kernels(X, Y=None, metric='linear', *, filter_params=False, n_jobs=None, **kwds)\n",
      "        Compute the kernel between arrays X and optional array Y.\n",
      "        \n",
      "        This method takes either a vector array or a kernel matrix, and returns\n",
      "        a kernel matrix. If the input is a vector array, the kernels are\n",
      "        computed. If the input is a kernel matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a kernel matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        kernel between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "            ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',\n",
      "            'laplacian', 'sigmoid', 'cosine']\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise kernels between samples, or a feature array.\n",
      "            The shape of the array should be (n_samples_X, n_samples_X) if\n",
      "            metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            A second feature array only if X has shape (n_samples_X, n_features).\n",
      "        \n",
      "        metric : str or callable, default=\"linear\"\n",
      "            The metric to use when calculating kernel between instances in a\n",
      "            feature array. If metric is a string, it must be one of the metrics\n",
      "            in pairwise.PAIRWISE_KERNEL_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a kernel matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two rows from X as input and return the corresponding\n",
      "            kernel value as a single number. This means that callables from\n",
      "            :mod:`sklearn.metrics.pairwise` are not allowed, as they operate on\n",
      "            matrices, not single samples. Use the string identifying the kernel\n",
      "            instead.\n",
      "        \n",
      "        filter_params : bool, default=False\n",
      "            Whether to filter invalid parameters or not.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the kernel function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)\n",
      "            A kernel matrix K such that K_{i, j} is the kernel between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then K_{i, j} is the kernel between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If metric is 'precomputed', Y is ignored and X is returned.\n",
      "    \n",
      "    polynomial_kernel(X, Y=None, degree=3, gamma=None, coef0=1)\n",
      "        Compute the polynomial kernel between X and Y::\n",
      "        \n",
      "            K(X, Y) = (gamma <X, Y> + coef0)^degree\n",
      "        \n",
      "        Read more in the :ref:`User Guide <polynomial_kernel>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_features)\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "        \n",
      "        degree : int, default=3\n",
      "        \n",
      "        gamma : float, default=None\n",
      "            If None, defaults to 1.0 / n_features.\n",
      "        \n",
      "        coef0 : float, default=1\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Gram matrix : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "    \n",
      "    rbf_kernel(X, Y=None, gamma=None)\n",
      "        Compute the rbf (gaussian) kernel between X and Y::\n",
      "        \n",
      "            K(x, y) = exp(-gamma ||x-y||^2)\n",
      "        \n",
      "        for each pair of rows x in X and y in Y.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <rbf_kernel>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_features)\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            If `None`, uses `Y=X`.\n",
      "        \n",
      "        gamma : float, default=None\n",
      "            If None, defaults to 1.0 / n_features.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kernel_matrix : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "    \n",
      "    sigmoid_kernel(X, Y=None, gamma=None, coef0=1)\n",
      "        Compute the sigmoid kernel between X and Y::\n",
      "        \n",
      "            K(X, Y) = tanh(gamma <X, Y> + coef0)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sigmoid_kernel>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_features)\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            If `None`, uses `Y=X`.\n",
      "        \n",
      "        gamma : float, default=None\n",
      "            If None, defaults to 1.0 / n_features.\n",
      "        \n",
      "        coef0 : float, default=1\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Gram matrix : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "\n",
      "DATA\n",
      "    KERNEL_PARAMS = {'additive_chi2': (), 'chi2': frozenset({'gamma'}), 'c...\n",
      "    PAIRED_DISTANCES = {'cityblock': <function paired_manhattan_distances>...\n",
      "    PAIRWISE_BOOLEAN_FUNCTIONS = ['dice', 'jaccard', 'kulsinski', 'matchin...\n",
      "    PAIRWISE_DISTANCE_FUNCTIONS = {'cityblock': <function manhattan_distan...\n",
      "    PAIRWISE_KERNEL_FUNCTIONS = {'additive_chi2': <function additive_chi2_...\n",
      "    sp_version = <Version('1.7.3')>\n",
      "\n",
      "FILE\n",
      "    c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.metrics.pairwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2614097603.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [59]\u001b[1;36m\u001b[0m\n\u001b[1;33m    from scipy.spatial.distance\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module scipy.spatial.distance in scipy.spatial:\n",
      "\n",
      "NAME\n",
      "    scipy.spatial.distance\n",
      "\n",
      "DESCRIPTION\n",
      "    Distance computations (:mod:`scipy.spatial.distance`)\n",
      "    =====================================================\n",
      "    \n",
      "    .. sectionauthor:: Damian Eads\n",
      "    \n",
      "    Function reference\n",
      "    ------------------\n",
      "    \n",
      "    Distance matrix computation from a collection of raw observation vectors\n",
      "    stored in a rectangular array.\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       pdist   -- pairwise distances between observation vectors.\n",
      "       cdist   -- distances between two collections of observation vectors\n",
      "       squareform -- convert distance matrix to a condensed one and vice versa\n",
      "       directed_hausdorff -- directed Hausdorff distance between arrays\n",
      "    \n",
      "    Predicates for checking the validity of distance matrices, both\n",
      "    condensed and redundant. Also contained in this module are functions\n",
      "    for computing the number of observations in a distance matrix.\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       is_valid_dm -- checks for a valid distance matrix\n",
      "       is_valid_y  -- checks for a valid condensed distance matrix\n",
      "       num_obs_dm  -- # of observations in a distance matrix\n",
      "       num_obs_y   -- # of observations in a condensed distance matrix\n",
      "    \n",
      "    Distance functions between two numeric vectors ``u`` and ``v``. Computing\n",
      "    distances over a large collection of vectors is inefficient for these\n",
      "    functions. Use ``pdist`` for this purpose.\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       braycurtis       -- the Bray-Curtis distance.\n",
      "       canberra         -- the Canberra distance.\n",
      "       chebyshev        -- the Chebyshev distance.\n",
      "       cityblock        -- the Manhattan distance.\n",
      "       correlation      -- the Correlation distance.\n",
      "       cosine           -- the Cosine distance.\n",
      "       euclidean        -- the Euclidean distance.\n",
      "       jensenshannon    -- the Jensen-Shannon distance.\n",
      "       mahalanobis      -- the Mahalanobis distance.\n",
      "       minkowski        -- the Minkowski distance.\n",
      "       seuclidean       -- the normalized Euclidean distance.\n",
      "       sqeuclidean      -- the squared Euclidean distance.\n",
      "       wminkowski       -- (deprecated) alias of `minkowski`.\n",
      "    \n",
      "    Distance functions between two boolean vectors (representing sets) ``u`` and\n",
      "    ``v``.  As in the case of numerical vectors, ``pdist`` is more efficient for\n",
      "    computing the distances between all pairs.\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       dice             -- the Dice dissimilarity.\n",
      "       hamming          -- the Hamming distance.\n",
      "       jaccard          -- the Jaccard distance.\n",
      "       kulsinski        -- the Kulsinski distance.\n",
      "       rogerstanimoto   -- the Rogers-Tanimoto dissimilarity.\n",
      "       russellrao       -- the Russell-Rao dissimilarity.\n",
      "       sokalmichener    -- the Sokal-Michener dissimilarity.\n",
      "       sokalsneath      -- the Sokal-Sneath dissimilarity.\n",
      "       yule             -- the Yule dissimilarity.\n",
      "    \n",
      "    :func:`hamming` also operates over discrete numerical vectors.\n",
      "\n",
      "FUNCTIONS\n",
      "    braycurtis(u, v, w=None)\n",
      "        Compute the Bray-Curtis distance between two 1-D arrays.\n",
      "        \n",
      "        Bray-Curtis distance is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\sum{|u_i-v_i|} / \\sum{|u_i+v_i|}\n",
      "        \n",
      "        The Bray-Curtis distance is in the range [0, 1] if all coordinates are\n",
      "        positive, and is undefined if the inputs are of length zero.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like\n",
      "            Input array.\n",
      "        v : (N,) array_like\n",
      "            Input array.\n",
      "        w : (N,) array_like, optional\n",
      "            The weights for each value in `u` and `v`. Default is None,\n",
      "            which gives each value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        braycurtis : double\n",
      "            The Bray-Curtis distance between 1-D arrays `u` and `v`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.braycurtis([1, 0, 0], [0, 1, 0])\n",
      "        1.0\n",
      "        >>> distance.braycurtis([1, 1, 0], [0, 1, 0])\n",
      "        0.33333333333333331\n",
      "    \n",
      "    canberra(u, v, w=None)\n",
      "        Compute the Canberra distance between two 1-D arrays.\n",
      "        \n",
      "        The Canberra distance is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "             d(u,v) = \\sum_i \\frac{|u_i-v_i|}\n",
      "                                  {|u_i|+|v_i|}.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like\n",
      "            Input array.\n",
      "        v : (N,) array_like\n",
      "            Input array.\n",
      "        w : (N,) array_like, optional\n",
      "            The weights for each value in `u` and `v`. Default is None,\n",
      "            which gives each value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        canberra : double\n",
      "            The Canberra distance between vectors `u` and `v`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When `u[i]` and `v[i]` are 0 for given i, then the fraction 0/0 = 0 is\n",
      "        used in the calculation.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.canberra([1, 0, 0], [0, 1, 0])\n",
      "        2.0\n",
      "        >>> distance.canberra([1, 1, 0], [0, 1, 0])\n",
      "        1.0\n",
      "    \n",
      "    cdist(XA, XB, metric='euclidean', *, out=None, **kwargs)\n",
      "        Compute distance between each pair of the two collections of inputs.\n",
      "        \n",
      "        See Notes for common calling conventions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        XA : array_like\n",
      "            An :math:`m_A` by :math:`n` array of :math:`m_A`\n",
      "            original observations in an :math:`n`-dimensional space.\n",
      "            Inputs are converted to float type.\n",
      "        XB : array_like\n",
      "            An :math:`m_B` by :math:`n` array of :math:`m_B`\n",
      "            original observations in an :math:`n`-dimensional space.\n",
      "            Inputs are converted to float type.\n",
      "        metric : str or callable, optional\n",
      "            The distance metric to use. If a string, the distance function can be\n",
      "            'braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation',\n",
      "            'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'jensenshannon',\n",
      "            'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',\n",
      "            'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath',\n",
      "            'sqeuclidean', 'wminkowski', 'yule'.\n",
      "        **kwargs : dict, optional\n",
      "            Extra arguments to `metric`: refer to each metric documentation for a\n",
      "            list of all possible arguments.\n",
      "        \n",
      "            Some possible arguments:\n",
      "        \n",
      "            p : scalar\n",
      "            The p-norm to apply for Minkowski, weighted and unweighted.\n",
      "            Default: 2.\n",
      "        \n",
      "            w : array_like\n",
      "            The weight vector for metrics that support weights (e.g., Minkowski).\n",
      "        \n",
      "            V : array_like\n",
      "            The variance vector for standardized Euclidean.\n",
      "            Default: var(vstack([XA, XB]), axis=0, ddof=1)\n",
      "        \n",
      "            VI : array_like\n",
      "            The inverse of the covariance matrix for Mahalanobis.\n",
      "            Default: inv(cov(vstack([XA, XB].T))).T\n",
      "        \n",
      "            out : ndarray\n",
      "            The output array\n",
      "            If not None, the distance matrix Y is stored in this array.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Y : ndarray\n",
      "            A :math:`m_A` by :math:`m_B` distance matrix is returned.\n",
      "            For each :math:`i` and :math:`j`, the metric\n",
      "            ``dist(u=XA[i], v=XB[j])`` is computed and stored in the\n",
      "            :math:`ij` th entry.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            An exception is thrown if `XA` and `XB` do not have\n",
      "            the same number of columns.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The following are common calling conventions:\n",
      "        \n",
      "        1. ``Y = cdist(XA, XB, 'euclidean')``\n",
      "        \n",
      "           Computes the distance between :math:`m` points using\n",
      "           Euclidean distance (2-norm) as the distance metric between the\n",
      "           points. The points are arranged as :math:`m`\n",
      "           :math:`n`-dimensional row vectors in the matrix X.\n",
      "        \n",
      "        2. ``Y = cdist(XA, XB, 'minkowski', p=2.)``\n",
      "        \n",
      "           Computes the distances using the Minkowski distance\n",
      "           :math:`||u-v||_p` (:math:`p`-norm) where :math:`p \\geq 1`.\n",
      "        \n",
      "        3. ``Y = cdist(XA, XB, 'cityblock')``\n",
      "        \n",
      "           Computes the city block or Manhattan distance between the\n",
      "           points.\n",
      "        \n",
      "        4. ``Y = cdist(XA, XB, 'seuclidean', V=None)``\n",
      "        \n",
      "           Computes the standardized Euclidean distance. The standardized\n",
      "           Euclidean distance between two n-vectors ``u`` and ``v`` is\n",
      "        \n",
      "           .. math::\n",
      "        \n",
      "              \\sqrt{\\sum {(u_i-v_i)^2 / V[x_i]}}.\n",
      "        \n",
      "           V is the variance vector; V[i] is the variance computed over all\n",
      "           the i'th components of the points. If not passed, it is\n",
      "           automatically computed.\n",
      "        \n",
      "        5. ``Y = cdist(XA, XB, 'sqeuclidean')``\n",
      "        \n",
      "           Computes the squared Euclidean distance :math:`||u-v||_2^2` between\n",
      "           the vectors.\n",
      "        \n",
      "        6. ``Y = cdist(XA, XB, 'cosine')``\n",
      "        \n",
      "           Computes the cosine distance between vectors u and v,\n",
      "        \n",
      "           .. math::\n",
      "        \n",
      "              1 - \\frac{u \\cdot v}\n",
      "                       {{||u||}_2 {||v||}_2}\n",
      "        \n",
      "           where :math:`||*||_2` is the 2-norm of its argument ``*``, and\n",
      "           :math:`u \\cdot v` is the dot product of :math:`u` and :math:`v`.\n",
      "        \n",
      "        7. ``Y = cdist(XA, XB, 'correlation')``\n",
      "        \n",
      "           Computes the correlation distance between vectors u and v. This is\n",
      "        \n",
      "           .. math::\n",
      "        \n",
      "              1 - \\frac{(u - \\bar{u}) \\cdot (v - \\bar{v})}\n",
      "                       {{||(u - \\bar{u})||}_2 {||(v - \\bar{v})||}_2}\n",
      "        \n",
      "           where :math:`\\bar{v}` is the mean of the elements of vector v,\n",
      "           and :math:`x \\cdot y` is the dot product of :math:`x` and :math:`y`.\n",
      "        \n",
      "        \n",
      "        8. ``Y = cdist(XA, XB, 'hamming')``\n",
      "        \n",
      "           Computes the normalized Hamming distance, or the proportion of\n",
      "           those vector elements between two n-vectors ``u`` and ``v``\n",
      "           which disagree. To save memory, the matrix ``X`` can be of type\n",
      "           boolean.\n",
      "        \n",
      "        9. ``Y = cdist(XA, XB, 'jaccard')``\n",
      "        \n",
      "           Computes the Jaccard distance between the points. Given two\n",
      "           vectors, ``u`` and ``v``, the Jaccard distance is the\n",
      "           proportion of those elements ``u[i]`` and ``v[i]`` that\n",
      "           disagree where at least one of them is non-zero.\n",
      "        \n",
      "        10. ``Y = cdist(XA, XB, 'jensenshannon')``\n",
      "        \n",
      "            Computes the Jensen-Shannon distance between two probability arrays.\n",
      "            Given two probability vectors, :math:`p` and :math:`q`, the\n",
      "            Jensen-Shannon distance is\n",
      "        \n",
      "            .. math::\n",
      "        \n",
      "               \\sqrt{\\frac{D(p \\parallel m) + D(q \\parallel m)}{2}}\n",
      "        \n",
      "            where :math:`m` is the pointwise mean of :math:`p` and :math:`q`\n",
      "            and :math:`D` is the Kullback-Leibler divergence.\n",
      "        \n",
      "        11. ``Y = cdist(XA, XB, 'chebyshev')``\n",
      "        \n",
      "            Computes the Chebyshev distance between the points. The\n",
      "            Chebyshev distance between two n-vectors ``u`` and ``v`` is the\n",
      "            maximum norm-1 distance between their respective elements. More\n",
      "            precisely, the distance is given by\n",
      "        \n",
      "            .. math::\n",
      "        \n",
      "               d(u,v) = \\max_i {|u_i-v_i|}.\n",
      "        \n",
      "        12. ``Y = cdist(XA, XB, 'canberra')``\n",
      "        \n",
      "            Computes the Canberra distance between the points. The\n",
      "            Canberra distance between two points ``u`` and ``v`` is\n",
      "        \n",
      "            .. math::\n",
      "        \n",
      "              d(u,v) = \\sum_i \\frac{|u_i-v_i|}\n",
      "                                   {|u_i|+|v_i|}.\n",
      "        \n",
      "        13. ``Y = cdist(XA, XB, 'braycurtis')``\n",
      "        \n",
      "            Computes the Bray-Curtis distance between the points. The\n",
      "            Bray-Curtis distance between two points ``u`` and ``v`` is\n",
      "        \n",
      "        \n",
      "            .. math::\n",
      "        \n",
      "                 d(u,v) = \\frac{\\sum_i (|u_i-v_i|)}\n",
      "                               {\\sum_i (|u_i+v_i|)}\n",
      "        \n",
      "        14. ``Y = cdist(XA, XB, 'mahalanobis', VI=None)``\n",
      "        \n",
      "            Computes the Mahalanobis distance between the points. The\n",
      "            Mahalanobis distance between two points ``u`` and ``v`` is\n",
      "            :math:`\\sqrt{(u-v)(1/V)(u-v)^T}` where :math:`(1/V)` (the ``VI``\n",
      "            variable) is the inverse covariance. If ``VI`` is not None,\n",
      "            ``VI`` will be used as the inverse covariance matrix.\n",
      "        \n",
      "        15. ``Y = cdist(XA, XB, 'yule')``\n",
      "        \n",
      "            Computes the Yule distance between the boolean\n",
      "            vectors. (see `yule` function documentation)\n",
      "        \n",
      "        16. ``Y = cdist(XA, XB, 'matching')``\n",
      "        \n",
      "            Synonym for 'hamming'.\n",
      "        \n",
      "        17. ``Y = cdist(XA, XB, 'dice')``\n",
      "        \n",
      "            Computes the Dice distance between the boolean vectors. (see\n",
      "            `dice` function documentation)\n",
      "        \n",
      "        18. ``Y = cdist(XA, XB, 'kulsinski')``\n",
      "        \n",
      "            Computes the Kulsinski distance between the boolean\n",
      "            vectors. (see `kulsinski` function documentation)\n",
      "        \n",
      "        19. ``Y = cdist(XA, XB, 'rogerstanimoto')``\n",
      "        \n",
      "            Computes the Rogers-Tanimoto distance between the boolean\n",
      "            vectors. (see `rogerstanimoto` function documentation)\n",
      "        \n",
      "        20. ``Y = cdist(XA, XB, 'russellrao')``\n",
      "        \n",
      "            Computes the Russell-Rao distance between the boolean\n",
      "            vectors. (see `russellrao` function documentation)\n",
      "        \n",
      "        21. ``Y = cdist(XA, XB, 'sokalmichener')``\n",
      "        \n",
      "            Computes the Sokal-Michener distance between the boolean\n",
      "            vectors. (see `sokalmichener` function documentation)\n",
      "        \n",
      "        22. ``Y = cdist(XA, XB, 'sokalsneath')``\n",
      "        \n",
      "            Computes the Sokal-Sneath distance between the vectors. (see\n",
      "            `sokalsneath` function documentation)\n",
      "        \n",
      "        \n",
      "        23. ``Y = cdist(XA, XB, 'wminkowski', p=2., w=w)``\n",
      "        \n",
      "            Computes the weighted Minkowski distance between the\n",
      "            vectors. (see `wminkowski` function documentation)\n",
      "        \n",
      "            'wminkowski' is deprecated and will be removed in SciPy 1.8.0.\n",
      "            Use 'minkowski' instead.\n",
      "        \n",
      "        24. ``Y = cdist(XA, XB, f)``\n",
      "        \n",
      "            Computes the distance between all pairs of vectors in X\n",
      "            using the user supplied 2-arity function f. For example,\n",
      "            Euclidean distance between the vectors could be computed\n",
      "            as follows::\n",
      "        \n",
      "              dm = cdist(XA, XB, lambda u, v: np.sqrt(((u-v)**2).sum()))\n",
      "        \n",
      "            Note that you should avoid passing a reference to one of\n",
      "            the distance functions defined in this library. For example,::\n",
      "        \n",
      "              dm = cdist(XA, XB, sokalsneath)\n",
      "        \n",
      "            would calculate the pair-wise distances between the vectors in\n",
      "            X using the Python function `sokalsneath`. This would result in\n",
      "            sokalsneath being called :math:`{n \\choose 2}` times, which\n",
      "            is inefficient. Instead, the optimized C version is more\n",
      "            efficient, and we call it using the following syntax::\n",
      "        \n",
      "              dm = cdist(XA, XB, 'sokalsneath')\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Find the Euclidean distances between four 2-D coordinates:\n",
      "        \n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> coords = [(35.0456, -85.2672),\n",
      "        ...           (35.1174, -89.9711),\n",
      "        ...           (35.9728, -83.9422),\n",
      "        ...           (36.1667, -86.7833)]\n",
      "        >>> distance.cdist(coords, coords, 'euclidean')\n",
      "        array([[ 0.    ,  4.7044,  1.6172,  1.8856],\n",
      "               [ 4.7044,  0.    ,  6.0893,  3.3561],\n",
      "               [ 1.6172,  6.0893,  0.    ,  2.8477],\n",
      "               [ 1.8856,  3.3561,  2.8477,  0.    ]])\n",
      "        \n",
      "        \n",
      "        Find the Manhattan distance from a 3-D point to the corners of the unit\n",
      "        cube:\n",
      "        \n",
      "        >>> a = np.array([[0, 0, 0],\n",
      "        ...               [0, 0, 1],\n",
      "        ...               [0, 1, 0],\n",
      "        ...               [0, 1, 1],\n",
      "        ...               [1, 0, 0],\n",
      "        ...               [1, 0, 1],\n",
      "        ...               [1, 1, 0],\n",
      "        ...               [1, 1, 1]])\n",
      "        >>> b = np.array([[ 0.1,  0.2,  0.4]])\n",
      "        >>> distance.cdist(a, b, 'cityblock')\n",
      "        array([[ 0.7],\n",
      "               [ 0.9],\n",
      "               [ 1.3],\n",
      "               [ 1.5],\n",
      "               [ 1.5],\n",
      "               [ 1.7],\n",
      "               [ 2.1],\n",
      "               [ 2.3]])\n",
      "    \n",
      "    chebyshev(u, v, w=None)\n",
      "        Compute the Chebyshev distance.\n",
      "        \n",
      "        Computes the Chebyshev distance between two 1-D arrays `u` and `v`,\n",
      "        which is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\max_i {|u_i-v_i|}.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like\n",
      "            Input vector.\n",
      "        v : (N,) array_like\n",
      "            Input vector.\n",
      "        w : (N,) array_like, optional\n",
      "            Unused, as 'max' is a weightless operation. Here for API consistency.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        chebyshev : double\n",
      "            The Chebyshev distance between vectors `u` and `v`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.chebyshev([1, 0, 0], [0, 1, 0])\n",
      "        1\n",
      "        >>> distance.chebyshev([1, 1, 0], [0, 1, 0])\n",
      "        1\n",
      "    \n",
      "    cityblock(u, v, w=None)\n",
      "        Compute the City Block (Manhattan) distance.\n",
      "        \n",
      "        Computes the Manhattan distance between two 1-D arrays `u` and `v`,\n",
      "        which is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\sum_i {\\left| u_i - v_i \\right|}.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like\n",
      "            Input array.\n",
      "        v : (N,) array_like\n",
      "            Input array.\n",
      "        w : (N,) array_like, optional\n",
      "            The weights for each value in `u` and `v`. Default is None,\n",
      "            which gives each value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        cityblock : double\n",
      "            The City Block (Manhattan) distance between vectors `u` and `v`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.cityblock([1, 0, 0], [0, 1, 0])\n",
      "        2\n",
      "        >>> distance.cityblock([1, 0, 0], [0, 2, 0])\n",
      "        3\n",
      "        >>> distance.cityblock([1, 0, 0], [1, 1, 0])\n",
      "        1\n",
      "    \n",
      "    correlation(u, v, w=None, centered=True)\n",
      "        Compute the correlation distance between two 1-D arrays.\n",
      "        \n",
      "        The correlation distance between `u` and `v`, is\n",
      "        defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            1 - \\frac{(u - \\bar{u}) \\cdot (v - \\bar{v})}\n",
      "                      {{||(u - \\bar{u})||}_2 {||(v - \\bar{v})||}_2}\n",
      "        \n",
      "        where :math:`\\bar{u}` is the mean of the elements of `u`\n",
      "        and :math:`x \\cdot y` is the dot product of :math:`x` and :math:`y`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like\n",
      "            Input array.\n",
      "        v : (N,) array_like\n",
      "            Input array.\n",
      "        w : (N,) array_like, optional\n",
      "            The weights for each value in `u` and `v`. Default is None,\n",
      "            which gives each value a weight of 1.0\n",
      "        centered : bool, optional\n",
      "            If True, `u` and `v` will be centered. Default is True.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        correlation : double\n",
      "            The correlation distance between 1-D array `u` and `v`.\n",
      "    \n",
      "    cosine(u, v, w=None)\n",
      "        Compute the Cosine distance between 1-D arrays.\n",
      "        \n",
      "        The Cosine distance between `u` and `v`, is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            1 - \\frac{u \\cdot v}\n",
      "                      {||u||_2 ||v||_2}.\n",
      "        \n",
      "        where :math:`u \\cdot v` is the dot product of :math:`u` and\n",
      "        :math:`v`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like\n",
      "            Input array.\n",
      "        v : (N,) array_like\n",
      "            Input array.\n",
      "        w : (N,) array_like, optional\n",
      "            The weights for each value in `u` and `v`. Default is None,\n",
      "            which gives each value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        cosine : double\n",
      "            The Cosine distance between vectors `u` and `v`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.cosine([1, 0, 0], [0, 1, 0])\n",
      "        1.0\n",
      "        >>> distance.cosine([100, 0, 0], [0, 1, 0])\n",
      "        1.0\n",
      "        >>> distance.cosine([1, 1, 0], [0, 1, 0])\n",
      "        0.29289321881345254\n",
      "    \n",
      "    dice(u, v, w=None)\n",
      "        Compute the Dice dissimilarity between two boolean 1-D arrays.\n",
      "        \n",
      "        The Dice dissimilarity between `u` and `v`, is\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "             \\frac{c_{TF} + c_{FT}}\n",
      "                  {2c_{TT} + c_{FT} + c_{TF}}\n",
      "        \n",
      "        where :math:`c_{ij}` is the number of occurrences of\n",
      "        :math:`\\mathtt{u[k]} = i` and :math:`\\mathtt{v[k]} = j` for\n",
      "        :math:`k < n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like, bool\n",
      "            Input 1-D array.\n",
      "        v : (N,) array_like, bool\n",
      "            Input 1-D array.\n",
      "        w : (N,) array_like, optional\n",
      "            The weights for each value in `u` and `v`. Default is None,\n",
      "            which gives each value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dice : double\n",
      "            The Dice dissimilarity between 1-D arrays `u` and `v`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.dice([1, 0, 0], [0, 1, 0])\n",
      "        1.0\n",
      "        >>> distance.dice([1, 0, 0], [1, 1, 0])\n",
      "        0.3333333333333333\n",
      "        >>> distance.dice([1, 0, 0], [2, 0, 0])\n",
      "        -0.3333333333333333\n",
      "    \n",
      "    directed_hausdorff(u, v, seed=0)\n",
      "        Compute the directed Hausdorff distance between two N-D arrays.\n",
      "        \n",
      "        Distances between pairs are calculated using a Euclidean metric.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (M,N) array_like\n",
      "            Input array.\n",
      "        v : (O,N) array_like\n",
      "            Input array.\n",
      "        seed : int or None\n",
      "            Local `numpy.random.RandomState` seed. Default is 0, a random\n",
      "            shuffling of u and v that guarantees reproducibility.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        d : double\n",
      "            The directed Hausdorff distance between arrays `u` and `v`,\n",
      "        \n",
      "        index_1 : int\n",
      "            index of point contributing to Hausdorff pair in `u`\n",
      "        \n",
      "        index_2 : int\n",
      "            index of point contributing to Hausdorff pair in `v`\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            An exception is thrown if `u` and `v` do not have\n",
      "            the same number of columns.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses the early break technique and the random sampling approach\n",
      "        described by [1]_. Although worst-case performance is ``O(m * o)``\n",
      "        (as with the brute force algorithm), this is unlikely in practice\n",
      "        as the input data would have to require the algorithm to explore\n",
      "        every single point interaction, and after the algorithm shuffles\n",
      "        the input points at that. The best case performance is O(m), which\n",
      "        is satisfied by selecting an inner loop distance that is less than\n",
      "        cmax and leads to an early break as often as possible. The authors\n",
      "        have formally shown that the average runtime is closer to O(m).\n",
      "        \n",
      "        .. versionadded:: 0.19.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] A. A. Taha and A. Hanbury, \"An efficient algorithm for\n",
      "               calculating the exact Hausdorff distance.\" IEEE Transactions On\n",
      "               Pattern Analysis And Machine Intelligence, vol. 37 pp. 2153-63,\n",
      "               2015.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.spatial.procrustes : Another similarity test for two data sets\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Find the directed Hausdorff distance between two 2-D arrays of\n",
      "        coordinates:\n",
      "        \n",
      "        >>> from scipy.spatial.distance import directed_hausdorff\n",
      "        >>> u = np.array([(1.0, 0.0),\n",
      "        ...               (0.0, 1.0),\n",
      "        ...               (-1.0, 0.0),\n",
      "        ...               (0.0, -1.0)])\n",
      "        >>> v = np.array([(2.0, 0.0),\n",
      "        ...               (0.0, 2.0),\n",
      "        ...               (-2.0, 0.0),\n",
      "        ...               (0.0, -4.0)])\n",
      "        \n",
      "        >>> directed_hausdorff(u, v)[0]\n",
      "        2.23606797749979\n",
      "        >>> directed_hausdorff(v, u)[0]\n",
      "        3.0\n",
      "        \n",
      "        Find the general (symmetric) Hausdorff distance between two 2-D\n",
      "        arrays of coordinates:\n",
      "        \n",
      "        >>> max(directed_hausdorff(u, v)[0], directed_hausdorff(v, u)[0])\n",
      "        3.0\n",
      "        \n",
      "        Find the indices of the points that generate the Hausdorff distance\n",
      "        (the Hausdorff pair):\n",
      "        \n",
      "        >>> directed_hausdorff(v, u)[1:]\n",
      "        (3, 3)\n",
      "    \n",
      "    euclidean(u, v, w=None)\n",
      "        Computes the Euclidean distance between two 1-D arrays.\n",
      "        \n",
      "        The Euclidean distance between 1-D arrays `u` and `v`, is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           {||u-v||}_2\n",
      "        \n",
      "           \\left(\\sum{(w_i |(u_i - v_i)|^2)}\\right)^{1/2}\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like\n",
      "            Input array.\n",
      "        v : (N,) array_like\n",
      "            Input array.\n",
      "        w : (N,) array_like, optional\n",
      "            The weights for each value in `u` and `v`. Default is None,\n",
      "            which gives each value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        euclidean : double\n",
      "            The Euclidean distance between vectors `u` and `v`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.euclidean([1, 0, 0], [0, 1, 0])\n",
      "        1.4142135623730951\n",
      "        >>> distance.euclidean([1, 1, 0], [0, 1, 0])\n",
      "        1.0\n",
      "    \n",
      "    hamming(u, v, w=None)\n",
      "        Compute the Hamming distance between two 1-D arrays.\n",
      "        \n",
      "        The Hamming distance between 1-D arrays `u` and `v`, is simply the\n",
      "        proportion of disagreeing components in `u` and `v`. If `u` and `v` are\n",
      "        boolean vectors, the Hamming distance is\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\frac{c_{01} + c_{10}}{n}\n",
      "        \n",
      "        where :math:`c_{ij}` is the number of occurrences of\n",
      "        :math:`\\mathtt{u[k]} = i` and :math:`\\mathtt{v[k]} = j` for\n",
      "        :math:`k < n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like\n",
      "            Input array.\n",
      "        v : (N,) array_like\n",
      "            Input array.\n",
      "        w : (N,) array_like, optional\n",
      "            The weights for each value in `u` and `v`. Default is None,\n",
      "            which gives each value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        hamming : double\n",
      "            The Hamming distance between vectors `u` and `v`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.hamming([1, 0, 0], [0, 1, 0])\n",
      "        0.66666666666666663\n",
      "        >>> distance.hamming([1, 0, 0], [1, 1, 0])\n",
      "        0.33333333333333331\n",
      "        >>> distance.hamming([1, 0, 0], [2, 0, 0])\n",
      "        0.33333333333333331\n",
      "        >>> distance.hamming([1, 0, 0], [3, 0, 0])\n",
      "        0.33333333333333331\n",
      "    \n",
      "    is_valid_dm(D, tol=0.0, throw=False, name='D', warning=False)\n",
      "        Return True if input array is a valid distance matrix.\n",
      "        \n",
      "        Distance matrices must be 2-dimensional numpy arrays.\n",
      "        They must have a zero-diagonal, and they must be symmetric.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        D : array_like\n",
      "            The candidate object to test for validity.\n",
      "        tol : float, optional\n",
      "            The distance matrix should be symmetric. `tol` is the maximum\n",
      "            difference between entries ``ij`` and ``ji`` for the distance\n",
      "            metric to be considered symmetric.\n",
      "        throw : bool, optional\n",
      "            An exception is thrown if the distance matrix passed is not valid.\n",
      "        name : str, optional\n",
      "            The name of the variable to checked. This is useful if\n",
      "            throw is set to True so the offending variable can be identified\n",
      "            in the exception message when an exception is thrown.\n",
      "        warning : bool, optional\n",
      "            Instead of throwing an exception, a warning message is\n",
      "            raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        valid : bool\n",
      "            True if the variable `D` passed is a valid distance matrix.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Small numerical differences in `D` and `D.T` and non-zeroness of\n",
      "        the diagonal are ignored if they are within the tolerance specified\n",
      "        by `tol`.\n",
      "    \n",
      "    is_valid_y(y, warning=False, throw=False, name=None)\n",
      "        Return True if the input array is a valid condensed distance matrix.\n",
      "        \n",
      "        Condensed distance matrices must be 1-dimensional numpy arrays.\n",
      "        Their length must be a binomial coefficient :math:`{n \\choose 2}`\n",
      "        for some positive integer n.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : array_like\n",
      "            The condensed distance matrix.\n",
      "        warning : bool, optional\n",
      "            Invokes a warning if the variable passed is not a valid\n",
      "            condensed distance matrix. The warning message explains why\n",
      "            the distance matrix is not valid.  `name` is used when\n",
      "            referencing the offending variable.\n",
      "        throw : bool, optional\n",
      "            Throws an exception if the variable passed is not a valid\n",
      "            condensed distance matrix.\n",
      "        name : bool, optional\n",
      "            Used when referencing the offending variable in the\n",
      "            warning or exception message.\n",
      "    \n",
      "    jaccard(u, v, w=None)\n",
      "        Compute the Jaccard-Needham dissimilarity between two boolean 1-D arrays.\n",
      "        \n",
      "        The Jaccard-Needham dissimilarity between 1-D boolean arrays `u` and `v`,\n",
      "        is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\frac{c_{TF} + c_{FT}}\n",
      "                {c_{TT} + c_{FT} + c_{TF}}\n",
      "        \n",
      "        where :math:`c_{ij}` is the number of occurrences of\n",
      "        :math:`\\mathtt{u[k]} = i` and :math:`\\mathtt{v[k]} = j` for\n",
      "        :math:`k < n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like, bool\n",
      "            Input array.\n",
      "        v : (N,) array_like, bool\n",
      "            Input array.\n",
      "        w : (N,) array_like, optional\n",
      "            The weights for each value in `u` and `v`. Default is None,\n",
      "            which gives each value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        jaccard : double\n",
      "            The Jaccard distance between vectors `u` and `v`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When both `u` and `v` lead to a `0/0` division i.e. there is no overlap\n",
      "        between the items in the vectors the returned distance is 0. See the\n",
      "        Wikipedia page on the Jaccard index [1]_, and this paper [2]_.\n",
      "        \n",
      "        .. versionchanged:: 1.2.0\n",
      "            Previously, when `u` and `v` lead to a `0/0` division, the function\n",
      "            would return NaN. This was changed to return 0 instead.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Jaccard_index\n",
      "        .. [2] S. Kosub, \"A note on the triangle inequality for the Jaccard\n",
      "           distance\", 2016, :arxiv:`1612.02696`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.jaccard([1, 0, 0], [0, 1, 0])\n",
      "        1.0\n",
      "        >>> distance.jaccard([1, 0, 0], [1, 1, 0])\n",
      "        0.5\n",
      "        >>> distance.jaccard([1, 0, 0], [1, 2, 0])\n",
      "        0.5\n",
      "        >>> distance.jaccard([1, 0, 0], [1, 1, 1])\n",
      "        0.66666666666666663\n",
      "    \n",
      "    jensenshannon(p, q, base=None, *, axis=0, keepdims=False)\n",
      "        Compute the Jensen-Shannon distance (metric) between\n",
      "        two probability arrays. This is the square root\n",
      "        of the Jensen-Shannon divergence.\n",
      "        \n",
      "        The Jensen-Shannon distance between two probability\n",
      "        vectors `p` and `q` is defined as,\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\sqrt{\\frac{D(p \\parallel m) + D(q \\parallel m)}{2}}\n",
      "        \n",
      "        where :math:`m` is the pointwise mean of :math:`p` and :math:`q`\n",
      "        and :math:`D` is the Kullback-Leibler divergence.\n",
      "        \n",
      "        This routine will normalize `p` and `q` if they don't sum to 1.0.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        p : (N,) array_like\n",
      "            left probability vector\n",
      "        q : (N,) array_like\n",
      "            right probability vector\n",
      "        base : double, optional\n",
      "            the base of the logarithm used to compute the output\n",
      "            if not given, then the routine uses the default base of\n",
      "            scipy.stats.entropy.\n",
      "        axis : int, optional\n",
      "            Axis along which the Jensen-Shannon distances are computed. The default\n",
      "            is 0.\n",
      "        \n",
      "            .. versionadded:: 1.7.0\n",
      "        keepdims : bool, optional\n",
      "            If this is set to `True`, the reduced axes are left in the\n",
      "            result as dimensions with size one. With this option,\n",
      "            the result will broadcast correctly against the input array.\n",
      "            Default is False.\n",
      "        \n",
      "            .. versionadded:: 1.7.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        js : double or ndarray\n",
      "            The Jensen-Shannon distances between `p` and `q` along the `axis`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        .. versionadded:: 1.2.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.jensenshannon([1.0, 0.0, 0.0], [0.0, 1.0, 0.0], 2.0)\n",
      "        1.0\n",
      "        >>> distance.jensenshannon([1.0, 0.0], [0.5, 0.5])\n",
      "        0.46450140402245893\n",
      "        >>> distance.jensenshannon([1.0, 0.0, 0.0], [1.0, 0.0, 0.0])\n",
      "        0.0\n",
      "        >>> a = np.array([[1, 2, 3, 4],\n",
      "        ...               [5, 6, 7, 8],\n",
      "        ...               [9, 10, 11, 12]])\n",
      "        >>> b = np.array([[13, 14, 15, 16],\n",
      "        ...               [17, 18, 19, 20],\n",
      "        ...               [21, 22, 23, 24]])\n",
      "        >>> distance.jensenshannon(a, b, axis=0)\n",
      "        array([0.1954288, 0.1447697, 0.1138377, 0.0927636])\n",
      "        >>> distance.jensenshannon(a, b, axis=1)\n",
      "        array([0.1402339, 0.0399106, 0.0201815])\n",
      "    \n",
      "    kulsinski(u, v, w=None)\n",
      "        Compute the Kulsinski dissimilarity between two boolean 1-D arrays.\n",
      "        \n",
      "        The Kulsinski dissimilarity between two boolean 1-D arrays `u` and `v`,\n",
      "        is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "             \\frac{c_{TF} + c_{FT} - c_{TT} + n}\n",
      "                  {c_{FT} + c_{TF} + n}\n",
      "        \n",
      "        where :math:`c_{ij}` is the number of occurrences of\n",
      "        :math:`\\mathtt{u[k]} = i` and :math:`\\mathtt{v[k]} = j` for\n",
      "        :math:`k < n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like, bool\n",
      "            Input array.\n",
      "        v : (N,) array_like, bool\n",
      "            Input array.\n",
      "        w : (N,) array_like, optional\n",
      "            The weights for each value in `u` and `v`. Default is None,\n",
      "            which gives each value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kulsinski : double\n",
      "            The Kulsinski distance between vectors `u` and `v`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.kulsinski([1, 0, 0], [0, 1, 0])\n",
      "        1.0\n",
      "        >>> distance.kulsinski([1, 0, 0], [1, 1, 0])\n",
      "        0.75\n",
      "        >>> distance.kulsinski([1, 0, 0], [2, 1, 0])\n",
      "        0.33333333333333331\n",
      "        >>> distance.kulsinski([1, 0, 0], [3, 1, 0])\n",
      "        -0.5\n",
      "    \n",
      "    mahalanobis(u, v, VI)\n",
      "        Compute the Mahalanobis distance between two 1-D arrays.\n",
      "        \n",
      "        The Mahalanobis distance between 1-D arrays `u` and `v`, is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\sqrt{ (u-v) V^{-1} (u-v)^T }\n",
      "        \n",
      "        where ``V`` is the covariance matrix.  Note that the argument `VI`\n",
      "        is the inverse of ``V``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like\n",
      "            Input array.\n",
      "        v : (N,) array_like\n",
      "            Input array.\n",
      "        VI : array_like\n",
      "            The inverse of the covariance matrix.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mahalanobis : double\n",
      "            The Mahalanobis distance between vectors `u` and `v`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> iv = [[1, 0.5, 0.5], [0.5, 1, 0.5], [0.5, 0.5, 1]]\n",
      "        >>> distance.mahalanobis([1, 0, 0], [0, 1, 0], iv)\n",
      "        1.0\n",
      "        >>> distance.mahalanobis([0, 2, 0], [0, 1, 0], iv)\n",
      "        1.0\n",
      "        >>> distance.mahalanobis([2, 0, 0], [0, 1, 0], iv)\n",
      "        1.7320508075688772\n",
      "    \n",
      "    matching(*args, **kwds)\n",
      "        `matching` is deprecated!\n",
      "        spatial.distance.matching is deprecated in scipy 1.0.0; use spatial.distance.hamming instead.\n",
      "        \n",
      "        Compute the Hamming distance between two boolean 1-D arrays.\n",
      "        \n",
      "        This is a deprecated synonym for :func:`hamming`.\n",
      "    \n",
      "    minkowski(u, v, p=2, w=None)\n",
      "        Compute the Minkowski distance between two 1-D arrays.\n",
      "        \n",
      "        The Minkowski distance between 1-D arrays `u` and `v`,\n",
      "        is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           {||u-v||}_p = (\\sum{|u_i - v_i|^p})^{1/p}.\n",
      "        \n",
      "        \n",
      "           \\left(\\sum{w_i(|(u_i - v_i)|^p)}\\right)^{1/p}.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like\n",
      "            Input array.\n",
      "        v : (N,) array_like\n",
      "            Input array.\n",
      "        p : scalar\n",
      "            The order of the norm of the difference :math:`{||u-v||}_p`.\n",
      "        w : (N,) array_like, optional\n",
      "            The weights for each value in `u` and `v`. Default is None,\n",
      "            which gives each value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        minkowski : double\n",
      "            The Minkowski distance between vectors `u` and `v`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.minkowski([1, 0, 0], [0, 1, 0], 1)\n",
      "        2.0\n",
      "        >>> distance.minkowski([1, 0, 0], [0, 1, 0], 2)\n",
      "        1.4142135623730951\n",
      "        >>> distance.minkowski([1, 0, 0], [0, 1, 0], 3)\n",
      "        1.2599210498948732\n",
      "        >>> distance.minkowski([1, 1, 0], [0, 1, 0], 1)\n",
      "        1.0\n",
      "        >>> distance.minkowski([1, 1, 0], [0, 1, 0], 2)\n",
      "        1.0\n",
      "        >>> distance.minkowski([1, 1, 0], [0, 1, 0], 3)\n",
      "        1.0\n",
      "    \n",
      "    num_obs_dm(d)\n",
      "        Return the number of original observations that correspond to a\n",
      "        square, redundant distance matrix.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        d : array_like\n",
      "            The target distance matrix.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        num_obs_dm : int\n",
      "            The number of observations in the redundant distance matrix.\n",
      "    \n",
      "    num_obs_y(Y)\n",
      "        Return the number of original observations that correspond to a\n",
      "        condensed distance matrix.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        Y : array_like\n",
      "            Condensed distance matrix.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        n : int\n",
      "            The number of observations in the condensed distance matrix `Y`.\n",
      "    \n",
      "    pdist(X, metric='euclidean', *, out=None, **kwargs)\n",
      "        Pairwise distances between observations in n-dimensional space.\n",
      "        \n",
      "        See Notes for common calling conventions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array_like\n",
      "            An m by n array of m original observations in an\n",
      "            n-dimensional space.\n",
      "        metric : str or function, optional\n",
      "            The distance metric to use. The distance function can\n",
      "            be 'braycurtis', 'canberra', 'chebyshev', 'cityblock',\n",
      "            'correlation', 'cosine', 'dice', 'euclidean', 'hamming',\n",
      "            'jaccard', 'jensenshannon', 'kulsinski', 'mahalanobis', 'matching',\n",
      "            'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n",
      "            'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'.\n",
      "        **kwargs : dict, optional\n",
      "            Extra arguments to `metric`: refer to each metric documentation for a\n",
      "            list of all possible arguments.\n",
      "        \n",
      "            Some possible arguments:\n",
      "        \n",
      "            p : scalar\n",
      "            The p-norm to apply for Minkowski, weighted and unweighted.\n",
      "            Default: 2.\n",
      "        \n",
      "            w : ndarray\n",
      "            The weight vector for metrics that support weights (e.g., Minkowski).\n",
      "        \n",
      "            V : ndarray\n",
      "            The variance vector for standardized Euclidean.\n",
      "            Default: var(X, axis=0, ddof=1)\n",
      "        \n",
      "            VI : ndarray\n",
      "            The inverse of the covariance matrix for Mahalanobis.\n",
      "            Default: inv(cov(X.T)).T\n",
      "        \n",
      "            out : ndarray.\n",
      "            The output array\n",
      "            If not None, condensed distance matrix Y is stored in this array.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Y : ndarray\n",
      "            Returns a condensed distance matrix Y. For each :math:`i` and :math:`j`\n",
      "            (where :math:`i<j<m`),where m is the number of original observations.\n",
      "            The metric ``dist(u=X[i], v=X[j])`` is computed and stored in entry ``m\n",
      "            * i + j - ((i + 2) * (i + 1)) // 2``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        squareform : converts between condensed distance matrices and\n",
      "                     square distance matrices.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        See ``squareform`` for information on how to calculate the index of\n",
      "        this entry or to convert the condensed distance matrix to a\n",
      "        redundant square matrix.\n",
      "        \n",
      "        The following are common calling conventions.\n",
      "        \n",
      "        1. ``Y = pdist(X, 'euclidean')``\n",
      "        \n",
      "           Computes the distance between m points using Euclidean distance\n",
      "           (2-norm) as the distance metric between the points. The points\n",
      "           are arranged as m n-dimensional row vectors in the matrix X.\n",
      "        \n",
      "        2. ``Y = pdist(X, 'minkowski', p=2.)``\n",
      "        \n",
      "           Computes the distances using the Minkowski distance\n",
      "           :math:`||u-v||_p` (p-norm) where :math:`p \\geq 1`.\n",
      "        \n",
      "        3. ``Y = pdist(X, 'cityblock')``\n",
      "        \n",
      "           Computes the city block or Manhattan distance between the\n",
      "           points.\n",
      "        \n",
      "        4. ``Y = pdist(X, 'seuclidean', V=None)``\n",
      "        \n",
      "           Computes the standardized Euclidean distance. The standardized\n",
      "           Euclidean distance between two n-vectors ``u`` and ``v`` is\n",
      "        \n",
      "           .. math::\n",
      "        \n",
      "              \\sqrt{\\sum {(u_i-v_i)^2 / V[x_i]}}\n",
      "        \n",
      "        \n",
      "           V is the variance vector; V[i] is the variance computed over all\n",
      "           the i'th components of the points.  If not passed, it is\n",
      "           automatically computed.\n",
      "        \n",
      "        5. ``Y = pdist(X, 'sqeuclidean')``\n",
      "        \n",
      "           Computes the squared Euclidean distance :math:`||u-v||_2^2` between\n",
      "           the vectors.\n",
      "        \n",
      "        6. ``Y = pdist(X, 'cosine')``\n",
      "        \n",
      "           Computes the cosine distance between vectors u and v,\n",
      "        \n",
      "           .. math::\n",
      "        \n",
      "              1 - \\frac{u \\cdot v}\n",
      "                       {{||u||}_2 {||v||}_2}\n",
      "        \n",
      "           where :math:`||*||_2` is the 2-norm of its argument ``*``, and\n",
      "           :math:`u \\cdot v` is the dot product of ``u`` and ``v``.\n",
      "        \n",
      "        7. ``Y = pdist(X, 'correlation')``\n",
      "        \n",
      "           Computes the correlation distance between vectors u and v. This is\n",
      "        \n",
      "           .. math::\n",
      "        \n",
      "              1 - \\frac{(u - \\bar{u}) \\cdot (v - \\bar{v})}\n",
      "                       {{||(u - \\bar{u})||}_2 {||(v - \\bar{v})||}_2}\n",
      "        \n",
      "           where :math:`\\bar{v}` is the mean of the elements of vector v,\n",
      "           and :math:`x \\cdot y` is the dot product of :math:`x` and :math:`y`.\n",
      "        \n",
      "        8. ``Y = pdist(X, 'hamming')``\n",
      "        \n",
      "           Computes the normalized Hamming distance, or the proportion of\n",
      "           those vector elements between two n-vectors ``u`` and ``v``\n",
      "           which disagree. To save memory, the matrix ``X`` can be of type\n",
      "           boolean.\n",
      "        \n",
      "        9. ``Y = pdist(X, 'jaccard')``\n",
      "        \n",
      "           Computes the Jaccard distance between the points. Given two\n",
      "           vectors, ``u`` and ``v``, the Jaccard distance is the\n",
      "           proportion of those elements ``u[i]`` and ``v[i]`` that\n",
      "           disagree.\n",
      "        \n",
      "        10. ``Y = pdist(X, 'jensenshannon')``\n",
      "        \n",
      "            Computes the Jensen-Shannon distance between two probability arrays.\n",
      "            Given two probability vectors, :math:`p` and :math:`q`, the\n",
      "            Jensen-Shannon distance is\n",
      "        \n",
      "            .. math::\n",
      "        \n",
      "               \\sqrt{\\frac{D(p \\parallel m) + D(q \\parallel m)}{2}}\n",
      "        \n",
      "            where :math:`m` is the pointwise mean of :math:`p` and :math:`q`\n",
      "            and :math:`D` is the Kullback-Leibler divergence.\n",
      "        \n",
      "        11. ``Y = pdist(X, 'chebyshev')``\n",
      "        \n",
      "            Computes the Chebyshev distance between the points. The\n",
      "            Chebyshev distance between two n-vectors ``u`` and ``v`` is the\n",
      "            maximum norm-1 distance between their respective elements. More\n",
      "            precisely, the distance is given by\n",
      "        \n",
      "            .. math::\n",
      "        \n",
      "               d(u,v) = \\max_i {|u_i-v_i|}\n",
      "        \n",
      "        12. ``Y = pdist(X, 'canberra')``\n",
      "        \n",
      "            Computes the Canberra distance between the points. The\n",
      "            Canberra distance between two points ``u`` and ``v`` is\n",
      "        \n",
      "            .. math::\n",
      "        \n",
      "              d(u,v) = \\sum_i \\frac{|u_i-v_i|}\n",
      "                                   {|u_i|+|v_i|}\n",
      "        \n",
      "        \n",
      "        13. ``Y = pdist(X, 'braycurtis')``\n",
      "        \n",
      "            Computes the Bray-Curtis distance between the points. The\n",
      "            Bray-Curtis distance between two points ``u`` and ``v`` is\n",
      "        \n",
      "        \n",
      "            .. math::\n",
      "        \n",
      "                 d(u,v) = \\frac{\\sum_i {|u_i-v_i|}}\n",
      "                                {\\sum_i {|u_i+v_i|}}\n",
      "        \n",
      "        14. ``Y = pdist(X, 'mahalanobis', VI=None)``\n",
      "        \n",
      "            Computes the Mahalanobis distance between the points. The\n",
      "            Mahalanobis distance between two points ``u`` and ``v`` is\n",
      "            :math:`\\sqrt{(u-v)(1/V)(u-v)^T}` where :math:`(1/V)` (the ``VI``\n",
      "            variable) is the inverse covariance. If ``VI`` is not None,\n",
      "            ``VI`` will be used as the inverse covariance matrix.\n",
      "        \n",
      "        15. ``Y = pdist(X, 'yule')``\n",
      "        \n",
      "            Computes the Yule distance between each pair of boolean\n",
      "            vectors. (see yule function documentation)\n",
      "        \n",
      "        16. ``Y = pdist(X, 'matching')``\n",
      "        \n",
      "            Synonym for 'hamming'.\n",
      "        \n",
      "        17. ``Y = pdist(X, 'dice')``\n",
      "        \n",
      "            Computes the Dice distance between each pair of boolean\n",
      "            vectors. (see dice function documentation)\n",
      "        \n",
      "        18. ``Y = pdist(X, 'kulsinski')``\n",
      "        \n",
      "            Computes the Kulsinski distance between each pair of\n",
      "            boolean vectors. (see kulsinski function documentation)\n",
      "        \n",
      "        19. ``Y = pdist(X, 'rogerstanimoto')``\n",
      "        \n",
      "            Computes the Rogers-Tanimoto distance between each pair of\n",
      "            boolean vectors. (see rogerstanimoto function documentation)\n",
      "        \n",
      "        20. ``Y = pdist(X, 'russellrao')``\n",
      "        \n",
      "            Computes the Russell-Rao distance between each pair of\n",
      "            boolean vectors. (see russellrao function documentation)\n",
      "        \n",
      "        21. ``Y = pdist(X, 'sokalmichener')``\n",
      "        \n",
      "            Computes the Sokal-Michener distance between each pair of\n",
      "            boolean vectors. (see sokalmichener function documentation)\n",
      "        \n",
      "        22. ``Y = pdist(X, 'sokalsneath')``\n",
      "        \n",
      "            Computes the Sokal-Sneath distance between each pair of\n",
      "            boolean vectors. (see sokalsneath function documentation)\n",
      "        \n",
      "        23. ``Y = pdist(X, 'wminkowski', p=2, w=w)``\n",
      "        \n",
      "            Computes the weighted Minkowski distance between each pair of\n",
      "            vectors. (see wminkowski function documentation)\n",
      "        \n",
      "            'wminkowski' is deprecated and will be removed in SciPy 1.8.0.\n",
      "            Use 'minkowski' instead.\n",
      "        \n",
      "        24. ``Y = pdist(X, f)``\n",
      "        \n",
      "            Computes the distance between all pairs of vectors in X\n",
      "            using the user supplied 2-arity function f. For example,\n",
      "            Euclidean distance between the vectors could be computed\n",
      "            as follows::\n",
      "        \n",
      "              dm = pdist(X, lambda u, v: np.sqrt(((u-v)**2).sum()))\n",
      "        \n",
      "            Note that you should avoid passing a reference to one of\n",
      "            the distance functions defined in this library. For example,::\n",
      "        \n",
      "              dm = pdist(X, sokalsneath)\n",
      "        \n",
      "            would calculate the pair-wise distances between the vectors in\n",
      "            X using the Python function sokalsneath. This would result in\n",
      "            sokalsneath being called :math:`{n \\choose 2}` times, which\n",
      "            is inefficient. Instead, the optimized C version is more\n",
      "            efficient, and we call it using the following syntax.::\n",
      "        \n",
      "              dm = pdist(X, 'sokalsneath')\n",
      "    \n",
      "    rogerstanimoto(u, v, w=None)\n",
      "        Compute the Rogers-Tanimoto dissimilarity between two boolean 1-D arrays.\n",
      "        \n",
      "        The Rogers-Tanimoto dissimilarity between two boolean 1-D arrays\n",
      "        `u` and `v`, is defined as\n",
      "        \n",
      "        .. math::\n",
      "           \\frac{R}\n",
      "                {c_{TT} + c_{FF} + R}\n",
      "        \n",
      "        where :math:`c_{ij}` is the number of occurrences of\n",
      "        :math:`\\mathtt{u[k]} = i` and :math:`\\mathtt{v[k]} = j` for\n",
      "        :math:`k < n` and :math:`R = 2(c_{TF} + c_{FT})`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like, bool\n",
      "            Input array.\n",
      "        v : (N,) array_like, bool\n",
      "            Input array.\n",
      "        w : (N,) array_like, optional\n",
      "            The weights for each value in `u` and `v`. Default is None,\n",
      "            which gives each value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rogerstanimoto : double\n",
      "            The Rogers-Tanimoto dissimilarity between vectors\n",
      "            `u` and `v`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.rogerstanimoto([1, 0, 0], [0, 1, 0])\n",
      "        0.8\n",
      "        >>> distance.rogerstanimoto([1, 0, 0], [1, 1, 0])\n",
      "        0.5\n",
      "        >>> distance.rogerstanimoto([1, 0, 0], [2, 0, 0])\n",
      "        -1.0\n",
      "    \n",
      "    russellrao(u, v, w=None)\n",
      "        Compute the Russell-Rao dissimilarity between two boolean 1-D arrays.\n",
      "        \n",
      "        The Russell-Rao dissimilarity between two boolean 1-D arrays, `u` and\n",
      "        `v`, is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "          \\frac{n - c_{TT}}\n",
      "               {n}\n",
      "        \n",
      "        where :math:`c_{ij}` is the number of occurrences of\n",
      "        :math:`\\mathtt{u[k]} = i` and :math:`\\mathtt{v[k]} = j` for\n",
      "        :math:`k < n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like, bool\n",
      "            Input array.\n",
      "        v : (N,) array_like, bool\n",
      "            Input array.\n",
      "        w : (N,) array_like, optional\n",
      "            The weights for each value in `u` and `v`. Default is None,\n",
      "            which gives each value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        russellrao : double\n",
      "            The Russell-Rao dissimilarity between vectors `u` and `v`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.russellrao([1, 0, 0], [0, 1, 0])\n",
      "        1.0\n",
      "        >>> distance.russellrao([1, 0, 0], [1, 1, 0])\n",
      "        0.6666666666666666\n",
      "        >>> distance.russellrao([1, 0, 0], [2, 0, 0])\n",
      "        0.3333333333333333\n",
      "    \n",
      "    seuclidean(u, v, V)\n",
      "        Return the standardized Euclidean distance between two 1-D arrays.\n",
      "        \n",
      "        The standardized Euclidean distance between `u` and `v`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like\n",
      "            Input array.\n",
      "        v : (N,) array_like\n",
      "            Input array.\n",
      "        V : (N,) array_like\n",
      "            `V` is an 1-D array of component variances. It is usually computed\n",
      "            among a larger collection vectors.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        seuclidean : double\n",
      "            The standardized Euclidean distance between vectors `u` and `v`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.seuclidean([1, 0, 0], [0, 1, 0], [0.1, 0.1, 0.1])\n",
      "        4.4721359549995796\n",
      "        >>> distance.seuclidean([1, 0, 0], [0, 1, 0], [1, 0.1, 0.1])\n",
      "        3.3166247903553998\n",
      "        >>> distance.seuclidean([1, 0, 0], [0, 1, 0], [10, 0.1, 0.1])\n",
      "        3.1780497164141406\n",
      "    \n",
      "    sokalmichener(u, v, w=None)\n",
      "        Compute the Sokal-Michener dissimilarity between two boolean 1-D arrays.\n",
      "        \n",
      "        The Sokal-Michener dissimilarity between boolean 1-D arrays `u` and `v`,\n",
      "        is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\frac{R}\n",
      "                {S + R}\n",
      "        \n",
      "        where :math:`c_{ij}` is the number of occurrences of\n",
      "        :math:`\\mathtt{u[k]} = i` and :math:`\\mathtt{v[k]} = j` for\n",
      "        :math:`k < n`, :math:`R = 2 * (c_{TF} + c_{FT})` and\n",
      "        :math:`S = c_{FF} + c_{TT}`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like, bool\n",
      "            Input array.\n",
      "        v : (N,) array_like, bool\n",
      "            Input array.\n",
      "        w : (N,) array_like, optional\n",
      "            The weights for each value in `u` and `v`. Default is None,\n",
      "            which gives each value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sokalmichener : double\n",
      "            The Sokal-Michener dissimilarity between vectors `u` and `v`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.sokalmichener([1, 0, 0], [0, 1, 0])\n",
      "        0.8\n",
      "        >>> distance.sokalmichener([1, 0, 0], [1, 1, 0])\n",
      "        0.5\n",
      "        >>> distance.sokalmichener([1, 0, 0], [2, 0, 0])\n",
      "        -1.0\n",
      "    \n",
      "    sokalsneath(u, v, w=None)\n",
      "        Compute the Sokal-Sneath dissimilarity between two boolean 1-D arrays.\n",
      "        \n",
      "        The Sokal-Sneath dissimilarity between `u` and `v`,\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\frac{R}\n",
      "                {c_{TT} + R}\n",
      "        \n",
      "        where :math:`c_{ij}` is the number of occurrences of\n",
      "        :math:`\\mathtt{u[k]} = i` and :math:`\\mathtt{v[k]} = j` for\n",
      "        :math:`k < n` and :math:`R = 2(c_{TF} + c_{FT})`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like, bool\n",
      "            Input array.\n",
      "        v : (N,) array_like, bool\n",
      "            Input array.\n",
      "        w : (N,) array_like, optional\n",
      "            The weights for each value in `u` and `v`. Default is None,\n",
      "            which gives each value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sokalsneath : double\n",
      "            The Sokal-Sneath dissimilarity between vectors `u` and `v`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.sokalsneath([1, 0, 0], [0, 1, 0])\n",
      "        1.0\n",
      "        >>> distance.sokalsneath([1, 0, 0], [1, 1, 0])\n",
      "        0.66666666666666663\n",
      "        >>> distance.sokalsneath([1, 0, 0], [2, 1, 0])\n",
      "        0.0\n",
      "        >>> distance.sokalsneath([1, 0, 0], [3, 1, 0])\n",
      "        -2.0\n",
      "    \n",
      "    sqeuclidean(u, v, w=None)\n",
      "        Compute the squared Euclidean distance between two 1-D arrays.\n",
      "        \n",
      "        The squared Euclidean distance between `u` and `v` is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           {||u-v||}_2^2\n",
      "        \n",
      "           \\left(\\sum{(w_i |(u_i - v_i)|^2)}\\right)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like\n",
      "            Input array.\n",
      "        v : (N,) array_like\n",
      "            Input array.\n",
      "        w : (N,) array_like, optional\n",
      "            The weights for each value in `u` and `v`. Default is None,\n",
      "            which gives each value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sqeuclidean : double\n",
      "            The squared Euclidean distance between vectors `u` and `v`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.sqeuclidean([1, 0, 0], [0, 1, 0])\n",
      "        2.0\n",
      "        >>> distance.sqeuclidean([1, 1, 0], [0, 1, 0])\n",
      "        1.0\n",
      "    \n",
      "    squareform(X, force='no', checks=True)\n",
      "        Convert a vector-form distance vector to a square-form distance\n",
      "        matrix, and vice-versa.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array_like\n",
      "            Either a condensed or redundant distance matrix.\n",
      "        force : str, optional\n",
      "            As with MATLAB(TM), if force is equal to ``'tovector'`` or\n",
      "            ``'tomatrix'``, the input will be treated as a distance matrix or\n",
      "            distance vector respectively.\n",
      "        checks : bool, optional\n",
      "            If set to False, no checks will be made for matrix\n",
      "            symmetry nor zero diagonals. This is useful if it is known that\n",
      "            ``X - X.T1`` is small and ``diag(X)`` is close to zero.\n",
      "            These values are ignored any way so they do not disrupt the\n",
      "            squareform transformation.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Y : ndarray\n",
      "            If a condensed distance matrix is passed, a redundant one is\n",
      "            returned, or if a redundant one is passed, a condensed distance\n",
      "            matrix is returned.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        1. ``v = squareform(X)``\n",
      "        \n",
      "           Given a square n-by-n symmetric distance matrix ``X``,\n",
      "           ``v = squareform(X)`` returns a ``n * (n-1) / 2``\n",
      "           (i.e. binomial coefficient n choose 2) sized vector `v`\n",
      "           where :math:`v[{n \\choose 2} - {n-i \\choose 2} + (j-i-1)]`\n",
      "           is the distance between distinct points ``i`` and ``j``.\n",
      "           If ``X`` is non-square or asymmetric, an error is raised.\n",
      "        \n",
      "        2. ``X = squareform(v)``\n",
      "        \n",
      "           Given a ``n * (n-1) / 2`` sized vector ``v``\n",
      "           for some integer ``n >= 1`` encoding distances as described,\n",
      "           ``X = squareform(v)`` returns a n-by-n distance matrix ``X``.\n",
      "           The ``X[i, j]`` and ``X[j, i]`` values are set to\n",
      "           :math:`v[{n \\choose 2} - {n-i \\choose 2} + (j-i-1)]`\n",
      "           and all diagonal elements are zero.\n",
      "        \n",
      "        In SciPy 0.19.0, ``squareform`` stopped casting all input types to\n",
      "        float64, and started returning arrays of the same dtype as the input.\n",
      "    \n",
      "    wminkowski(u, v, p, w)\n",
      "        Compute the weighted Minkowski distance between two 1-D arrays.\n",
      "        \n",
      "        The weighted Minkowski distance between `u` and `v`, defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\left(\\sum{(|w_i (u_i - v_i)|^p)}\\right)^{1/p}.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like\n",
      "            Input array.\n",
      "        v : (N,) array_like\n",
      "            Input array.\n",
      "        p : scalar\n",
      "            The order of the norm of the difference :math:`{||u-v||}_p`.\n",
      "        w : (N,) array_like\n",
      "            The weight vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        wminkowski : double\n",
      "            The weighted Minkowski distance between vectors `u` and `v`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `wminkowski` is deprecated and will be removed in SciPy 1.8.0.\n",
      "        Use `minkowski` with the ``w`` argument instead.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.wminkowski([1, 0, 0], [0, 1, 0], 1, np.ones(3))\n",
      "        2.0\n",
      "        >>> distance.wminkowski([1, 0, 0], [0, 1, 0], 2, np.ones(3))\n",
      "        1.4142135623730951\n",
      "        >>> distance.wminkowski([1, 0, 0], [0, 1, 0], 3, np.ones(3))\n",
      "        1.2599210498948732\n",
      "        >>> distance.wminkowski([1, 1, 0], [0, 1, 0], 1, np.ones(3))\n",
      "        1.0\n",
      "        >>> distance.wminkowski([1, 1, 0], [0, 1, 0], 2, np.ones(3))\n",
      "        1.0\n",
      "        >>> distance.wminkowski([1, 1, 0], [0, 1, 0], 3, np.ones(3))\n",
      "        1.0\n",
      "    \n",
      "    yule(u, v, w=None)\n",
      "        Compute the Yule dissimilarity between two boolean 1-D arrays.\n",
      "        \n",
      "        The Yule dissimilarity is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "             \\frac{R}{c_{TT} * c_{FF} + \\frac{R}{2}}\n",
      "        \n",
      "        where :math:`c_{ij}` is the number of occurrences of\n",
      "        :math:`\\mathtt{u[k]} = i` and :math:`\\mathtt{v[k]} = j` for\n",
      "        :math:`k < n` and :math:`R = 2.0 * c_{TF} * c_{FT}`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u : (N,) array_like, bool\n",
      "            Input array.\n",
      "        v : (N,) array_like, bool\n",
      "            Input array.\n",
      "        w : (N,) array_like, optional\n",
      "            The weights for each value in `u` and `v`. Default is None,\n",
      "            which gives each value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        yule : double\n",
      "            The Yule dissimilarity between vectors `u` and `v`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.spatial import distance\n",
      "        >>> distance.yule([1, 0, 0], [0, 1, 0])\n",
      "        2.0\n",
      "        >>> distance.yule([1, 1, 0], [0, 1, 0])\n",
      "        0.0\n",
      "\n",
      "DATA\n",
      "    __all__ = ['braycurtis', 'canberra', 'cdist', 'chebyshev', 'cityblock'...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages\\scipy\\spatial\\distance.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(scipy.spatial.distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['braycurtis', 'canberra', 'chebyshev',\n",
    "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
    "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
    "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
    "              'yule']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textdistance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading textdistance-4.3.0-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: textdistance\n",
      "Successfully installed textdistance-4.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package textdistance:\n",
      "\n",
      "NAME\n",
      "    textdistance\n",
      "\n",
      "DESCRIPTION\n",
      "    TextDistance.\n",
      "    Compute distance between sequences.\n",
      "    30+ algorithms, pure python implementation, common interface.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    algorithms (package)\n",
      "    benchmark\n",
      "    libraries\n",
      "    utils\n",
      "\n",
      "SUBMODULES\n",
      "    base\n",
      "    compression_based\n",
      "    edit_based\n",
      "    phonetic\n",
      "    sequence_based\n",
      "    simple\n",
      "    token_based\n",
      "\n",
      "DATA\n",
      "    VERSION = '4.3.0'\n",
      "    __license__ = 'MIT'\n",
      "    __title__ = 'TextDistance'\n",
      "    arith_ncd = ArithNCD({'base': 2, 'terminator': None, 'qval': 1})\n",
      "    bag = Bag({'qval': 1, 'external': True})\n",
      "    bwtrle_ncd = BWTRLENCD({'terminator': '\\x00'})\n",
      "    bz2_ncd = BZ2NCD({})\n",
      "    cosine = Cosine({'qval': 1, 'as_set': False, 'external': True})\n",
      "    damerau_levenshtein = DamerauLevenshtein({'qval': 1, 'test_func': <fun...\n",
      "    editex = Editex({'match_cost': 0, 'group_cost': 1, 'misma..., 'N', 'J'...\n",
      "    entropy_ncd = EntropyNCD({'qval': 1, 'coef': 1, 'base': 2})\n",
      "    gotoh = Gotoh({'qval': 1, 'gap_open': 1, 'gap_ext': 0.4,..._ident at 0...\n",
      "    hamming = Hamming({'qval': 1, 'test_func': <function Base....4F8A91700...\n",
      "    identity = Identity({'qval': 1, 'external': True})\n",
      "    jaccard = Jaccard({'qval': 1, 'as_set': False, 'external': True})\n",
      "    jaro = Jaro({'qval': 1, 'long_tolerance': False, 'winklerize': False, ...\n",
      "    jaro_winkler = JaroWinkler({'qval': 1, 'long_tolerance': False, 'winkl...\n",
      "    lcsseq = LCSSeq({'qval': 1, 'test_func': <function Base._ident at 0x00...\n",
      "    lcsstr = LCSStr({'qval': 1, 'external': True})\n",
      "    length = Length({'qval': 1, 'external': True})\n",
      "    levenshtein = Levenshtein({'qval': 1, 'test_func': <function B..._iden...\n",
      "    lzma_ncd = LZMANCD({})\n",
      "    matrix = Matrix({'mat': None, 'mismatch_cost': 0, 'match_cost': 1, 'sy...\n",
      "    mlipns = MLIPNS({'qval': 1, 'threshold': 0.25, 'maxmismatches': 2, 'ex...\n",
      "    monge_elkan = MongeElkan({'algorithm': DamerauLevenshtein({'qv...'symm...\n",
      "    mra = MRA({'qval': 1, 'external': True})\n",
      "    needleman_wunsch = NeedlemanWunsch({'qval': 1, 'gap_cost': 1.0, 'si......\n",
      "    overlap = Overlap({'qval': 1, 'as_set': False, 'external': True})\n",
      "    postfix = Postfix({'qval': 1, 'sim_test': <function Base._ident at 0x0...\n",
      "    prefix = Prefix({'qval': 1, 'sim_test': <function Base._ident at 0x000...\n",
      "    ratcliff_obershelp = RatcliffObershelp({'qval': 1, 'external': True})\n",
      "    rle_ncd = RLENCD({'qval': 1})\n",
      "    smith_waterman = SmithWaterman({'qval': 1, 'gap_cost': 1.0, 'sim_..._i...\n",
      "    sorensen = Sorensen({'qval': 1, 'as_set': False, 'external': True})\n",
      "    sorensen_dice = Sorensen({'qval': 1, 'as_set': False, 'external': True...\n",
      "    sqrt_ncd = SqrtNCD({'qval': 1})\n",
      "    strcmp95 = StrCmp95({'long_strings': False, 'external': True})\n",
      "    tanimoto = Tanimoto({'qval': 1, 'as_set': False, 'external': True})\n",
      "    tversky = Tversky({'qval': 1, 'ks': repeat(1), 'bias': None, 'as_set':...\n",
      "    zlib_ncd = ZLIBNCD({})\n",
      "\n",
      "VERSION\n",
      "    4.3.0\n",
      "\n",
      "AUTHOR\n",
      "    Gram (@orsinium)\n",
      "\n",
      "FILE\n",
      "    c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages\\textdistance\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(textdistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (4.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from gensim) (1.20.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from gensim) (0.29.28)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_in_the_hat_docs=[\n",
    "       \"One Cent, Two Cents, Old Cent, New Cent: All About Money (Cat in the Hat's Learning Library\",\n",
    "       \"Inside Your Outside: All About the Human Body (Cat in the Hat's Learning Library)\",\n",
    "       \"Oh, The Things You Can Do That Are Good for You: All About Staying Healthy (Cat in the Hat's Learning Library)\",\n",
    "       \"On Beyond Bugs: All About Insects (Cat in the Hat's Learning Library)\",\n",
    "       \"There's No Place Like Space: All About Our Solar System (Cat in the Hat's Learning Library)\" \n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Inside Your Outside: All About the Human Body (Cat in the Hat's Learning Library)\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_in_the_hat_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuation\n",
    "# initializing punctuations string\n",
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    " \n",
    "# Removing punctuations in string\n",
    "# Using loop + punctuation string\n",
    "for ele in test_str:\n",
    "    if ele in punc:\n",
    "        test_str = test_str.replace(ele, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def remove_punctuation(texts):\n",
    "    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    return [[word for word in str(doc) if word not in stop_words] for doc in texts]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(texts):\n",
    "    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    \n",
    "    for ele in texts:\n",
    "        if ele in punc:\n",
    "            texts = str(texts).replace(ele, \"\")\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inside Your Outside All About the Human Body Cat in the Hats Learning Library'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punctuation(cat_in_the_hat_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_doc_pt=[]\n",
    "for i in range(len(cat_in_the_hat_docs)):\n",
    "    cat_doc_pt.append(remove_punctuation(cat_in_the_hat_docs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One Cent Two Cents Old Cent New Cent All About Money Cat in the Hats Learning Library',\n",
       " 'Inside Your Outside All About the Human Body Cat in the Hats Learning Library',\n",
       " 'Oh The Things You Can Do That Are Good for You All About Staying Healthy Cat in the Hats Learning Library',\n",
       " 'On Beyond Bugs All About Insects Cat in the Hats Learning Library',\n",
       " 'Theres No Place Like Space All About Our Solar System Cat in the Hats Learning Library']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_doc_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "     \n",
    "    for ele in texts:\n",
    "        if ele in stop_words:\n",
    "            texts = str(texts).replace(ele, \"\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_doc_sw=[]\n",
    "for i in range(len(cat_doc_pt)):\n",
    "    cat_doc_sw.append(remove_stopwords(cat_doc_pt[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One Cen Tw Cen Ol Cen New Cen All Abu Mne C n he H Lernng Lbrr',\n",
       " 'Ine Yur Oue All Abu he Hun B C n he H Lernng Lbrr',\n",
       " 'Oh The Thng Yu Cn D Th Are G fr Yu All Abu Sng Helh C n he H Lernng Lbrr',\n",
       " 'On Ben Bug All Abu Inec C n he H Lernng Lbrr',\n",
       " 'There N Plce Lke Spce All Abu Our Slr Se C n he H Lernng Lbrr']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_doc_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_doc_tk=[]\n",
    "for i in range(len(cat_doc_pt)):\n",
    "    cat_doc_tk.append(word_tokenize(cat_doc_pt[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['One',\n",
       "  'Cent',\n",
       "  'Two',\n",
       "  'Cents',\n",
       "  'Old',\n",
       "  'Cent',\n",
       "  'New',\n",
       "  'Cent',\n",
       "  'All',\n",
       "  'About',\n",
       "  'Money',\n",
       "  'Cat',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Hats',\n",
       "  'Learning',\n",
       "  'Library'],\n",
       " ['Inside',\n",
       "  'Your',\n",
       "  'Outside',\n",
       "  'All',\n",
       "  'About',\n",
       "  'the',\n",
       "  'Human',\n",
       "  'Body',\n",
       "  'Cat',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Hats',\n",
       "  'Learning',\n",
       "  'Library'],\n",
       " ['Oh',\n",
       "  'The',\n",
       "  'Things',\n",
       "  'You',\n",
       "  'Can',\n",
       "  'Do',\n",
       "  'That',\n",
       "  'Are',\n",
       "  'Good',\n",
       "  'for',\n",
       "  'You',\n",
       "  'All',\n",
       "  'About',\n",
       "  'Staying',\n",
       "  'Healthy',\n",
       "  'Cat',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Hats',\n",
       "  'Learning',\n",
       "  'Library'],\n",
       " ['On',\n",
       "  'Beyond',\n",
       "  'Bugs',\n",
       "  'All',\n",
       "  'About',\n",
       "  'Insects',\n",
       "  'Cat',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Hats',\n",
       "  'Learning',\n",
       "  'Library'],\n",
       " ['Theres',\n",
       "  'No',\n",
       "  'Place',\n",
       "  'Like',\n",
       "  'Space',\n",
       "  'All',\n",
       "  'About',\n",
       "  'Our',\n",
       "  'Solar',\n",
       "  'System',\n",
       "  'Cat',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Hats',\n",
       "  'Learning',\n",
       "  'Library']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_doc_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_doc_sw=[]\n",
    "for i in range(len(cat_doc_tk)):\n",
    "    cat_doc_sw.append(remove_stopwords(cat_doc_tk[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['one'],\n",
       "  ['cent'],\n",
       "  ['two'],\n",
       "  ['cents'],\n",
       "  ['old'],\n",
       "  ['cent'],\n",
       "  ['new'],\n",
       "  ['cent'],\n",
       "  [],\n",
       "  [],\n",
       "  ['money'],\n",
       "  ['cat'],\n",
       "  [],\n",
       "  [],\n",
       "  ['hats'],\n",
       "  ['learning'],\n",
       "  ['library']],\n",
       " [['inside'],\n",
       "  [],\n",
       "  ['outside'],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  ['human'],\n",
       "  ['body'],\n",
       "  ['cat'],\n",
       "  [],\n",
       "  [],\n",
       "  ['hats'],\n",
       "  ['learning'],\n",
       "  ['library']],\n",
       " [['oh'],\n",
       "  [],\n",
       "  ['things'],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  ['good'],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  ['staying'],\n",
       "  ['healthy'],\n",
       "  ['cat'],\n",
       "  [],\n",
       "  [],\n",
       "  ['hats'],\n",
       "  ['learning'],\n",
       "  ['library']],\n",
       " [[],\n",
       "  ['beyond'],\n",
       "  ['bugs'],\n",
       "  [],\n",
       "  [],\n",
       "  ['insects'],\n",
       "  ['cat'],\n",
       "  [],\n",
       "  [],\n",
       "  ['hats'],\n",
       "  ['learning'],\n",
       "  ['library']],\n",
       " [['theres'],\n",
       "  [],\n",
       "  ['place'],\n",
       "  ['like'],\n",
       "  ['space'],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  ['solar'],\n",
       "  ['system'],\n",
       "  ['cat'],\n",
       "  [],\n",
       "  [],\n",
       "  ['hats'],\n",
       "  ['learning'],\n",
       "  ['library']]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_doc_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh The Thng Yu Cn D Th Are G fr Yu All Abu Sng Helh C n he H Lernng Lbrr'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_doc_sw[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the blank list item\n",
    "\n",
    "def remove_blanklist(texts):\n",
    "    nl=[]\n",
    "    for ele in texts:\n",
    "            if ele != []:\n",
    "                nl.append(ele)\n",
    "    return nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_doc=[]\n",
    "for i in range(len(cat_doc_sw)):\n",
    "    cat_doc.append(remove_blanklist(cat_doc_sw[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O',\n",
       "  'n',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'C',\n",
       "  'e',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'T',\n",
       "  'w',\n",
       "  ' ',\n",
       "  'C',\n",
       "  'e',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'O',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'C',\n",
       "  'e',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'N',\n",
       "  'e',\n",
       "  'w',\n",
       "  ' ',\n",
       "  'C',\n",
       "  'e',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'A',\n",
       "  'l',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'A',\n",
       "  'b',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'M',\n",
       "  'n',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'C',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'H',\n",
       "  ' ',\n",
       "  'L',\n",
       "  'e',\n",
       "  'r',\n",
       "  'n',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'L',\n",
       "  'b',\n",
       "  'r',\n",
       "  'r'],\n",
       " ['I',\n",
       "  'n',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'Y',\n",
       "  'u',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'O',\n",
       "  'u',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'A',\n",
       "  'l',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'A',\n",
       "  'b',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'H',\n",
       "  'u',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'B',\n",
       "  ' ',\n",
       "  'C',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'H',\n",
       "  ' ',\n",
       "  'L',\n",
       "  'e',\n",
       "  'r',\n",
       "  'n',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'L',\n",
       "  'b',\n",
       "  'r',\n",
       "  'r'],\n",
       " ['O',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'T',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'T',\n",
       "  'h',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'Y',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'C',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'D',\n",
       "  ' ',\n",
       "  'T',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'A',\n",
       "  'r',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'G',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'Y',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'A',\n",
       "  'l',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'A',\n",
       "  'b',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'S',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'H',\n",
       "  'e',\n",
       "  'l',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'C',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'H',\n",
       "  ' ',\n",
       "  'L',\n",
       "  'e',\n",
       "  'r',\n",
       "  'n',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'L',\n",
       "  'b',\n",
       "  'r',\n",
       "  'r'],\n",
       " ['O',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'B',\n",
       "  'e',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'B',\n",
       "  'u',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'A',\n",
       "  'l',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'A',\n",
       "  'b',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'I',\n",
       "  'n',\n",
       "  'e',\n",
       "  'c',\n",
       "  ' ',\n",
       "  'C',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'H',\n",
       "  ' ',\n",
       "  'L',\n",
       "  'e',\n",
       "  'r',\n",
       "  'n',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'L',\n",
       "  'b',\n",
       "  'r',\n",
       "  'r'],\n",
       " ['T',\n",
       "  'h',\n",
       "  'e',\n",
       "  'r',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'N',\n",
       "  ' ',\n",
       "  'P',\n",
       "  'l',\n",
       "  'c',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'L',\n",
       "  'k',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'S',\n",
       "  'p',\n",
       "  'c',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'A',\n",
       "  'l',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'A',\n",
       "  'b',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'O',\n",
       "  'u',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'S',\n",
       "  'l',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'S',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'C',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'H',\n",
       "  ' ',\n",
       "  'L',\n",
       "  'e',\n",
       "  'r',\n",
       "  'n',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'L',\n",
       "  'b',\n",
       "  'r',\n",
       "  'r']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['One',\n",
       "  'Cent',\n",
       "  'Two',\n",
       "  'Cents',\n",
       "  'Old',\n",
       "  'Cent',\n",
       "  'New',\n",
       "  'Cent',\n",
       "  'All',\n",
       "  'About',\n",
       "  'Money',\n",
       "  'Cat',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Hats',\n",
       "  'Learning',\n",
       "  'Library'],\n",
       " ['Inside',\n",
       "  'Your',\n",
       "  'Outside',\n",
       "  'All',\n",
       "  'About',\n",
       "  'the',\n",
       "  'Human',\n",
       "  'Body',\n",
       "  'Cat',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Hats',\n",
       "  'Learning',\n",
       "  'Library'],\n",
       " ['Oh',\n",
       "  'The',\n",
       "  'Things',\n",
       "  'You',\n",
       "  'Can',\n",
       "  'Do',\n",
       "  'That',\n",
       "  'Are',\n",
       "  'Good',\n",
       "  'for',\n",
       "  'You',\n",
       "  'All',\n",
       "  'About',\n",
       "  'Staying',\n",
       "  'Healthy',\n",
       "  'Cat',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Hats',\n",
       "  'Learning',\n",
       "  'Library'],\n",
       " ['On',\n",
       "  'Beyond',\n",
       "  'Bugs',\n",
       "  'All',\n",
       "  'About',\n",
       "  'Insects',\n",
       "  'Cat',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Hats',\n",
       "  'Learning',\n",
       "  'Library'],\n",
       " ['Theres',\n",
       "  'No',\n",
       "  'Place',\n",
       "  'Like',\n",
       "  'Space',\n",
       "  'All',\n",
       "  'About',\n",
       "  'Our',\n",
       "  'Solar',\n",
       "  'System',\n",
       "  'Cat',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Hats',\n",
       "  'Learning',\n",
       "  'Library']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_doc_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 3), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(cat_doc_tk)\n",
    "\n",
    "# Create Corpus\n",
    "texts = cat_doc_tk\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency).\n",
    "\n",
    "For example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs once and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 3), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)], [(0, 1), (1, 1), (2, 1), (5, 1), (6, 1), (7, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'About'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('About', 1),\n",
       "  ('All', 1),\n",
       "  ('Cat', 1),\n",
       "  ('Cent', 3),\n",
       "  ('Cents', 1),\n",
       "  ('Hats', 1),\n",
       "  ('Learning', 1),\n",
       "  ('Library', 1),\n",
       "  ('Money', 1),\n",
       "  ('New', 1),\n",
       "  ('Old', 1),\n",
       "  ('One', 1),\n",
       "  ('Two', 1),\n",
       "  ('in', 1),\n",
       "  ('the', 1)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('About', 1),\n",
       "  ('All', 1),\n",
       "  ('Cat', 1),\n",
       "  ('Cent', 3),\n",
       "  ('Cents', 1),\n",
       "  ('Hats', 1),\n",
       "  ('Learning', 1),\n",
       "  ('Library', 1),\n",
       "  ('Money', 1),\n",
       "  ('New', 1),\n",
       "  ('Old', 1),\n",
       "  ('One', 1),\n",
       "  ('Two', 1),\n",
       "  ('in', 1),\n",
       "  ('the', 1)],\n",
       " [('About', 1),\n",
       "  ('All', 1),\n",
       "  ('Cat', 1),\n",
       "  ('Hats', 1),\n",
       "  ('Learning', 1),\n",
       "  ('Library', 1),\n",
       "  ('in', 1),\n",
       "  ('the', 2),\n",
       "  ('Body', 1),\n",
       "  ('Human', 1),\n",
       "  ('Inside', 1),\n",
       "  ('Outside', 1),\n",
       "  ('Your', 1)]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Details about the parameters: https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (1,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (2,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (3,\n",
      "  '0.024*\"No\" + 0.024*\"Like\" + 0.023*\"Library\" + 0.023*\"System\" + 0.023*\"Our\" '\n",
      "  '+ 0.023*\"Hats\" + 0.023*\"Learning\" + 0.023*\"Theres\" + 0.023*\"About\" + '\n",
      "  '0.023*\"the\"'),\n",
      " (4,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (5,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (6,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (7,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (8,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (9,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (10,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (11,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (12,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (13,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (14,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (15,\n",
      "  '0.088*\"You\" + 0.045*\"Do\" + 0.045*\"for\" + 0.045*\"the\" + 0.045*\"Can\" + '\n",
      "  '0.045*\"All\" + 0.045*\"in\" + 0.045*\"Healthy\" + 0.045*\"Things\" + 0.045*\"That\"'),\n",
      " (16,\n",
      "  '0.098*\"Cent\" + 0.066*\"All\" + 0.066*\"Learning\" + 0.066*\"in\" + '\n",
      "  '0.066*\"Library\" + 0.066*\"About\" + 0.066*\"Cat\" + 0.066*\"Hats\" + 0.066*\"the\" '\n",
      "  '+ 0.034*\"Beyond\"'),\n",
      " (17,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (18,\n",
      "  '0.058*\"in\" + 0.058*\"Cat\" + 0.058*\"Place\" + 0.058*\"All\" + 0.058*\"Space\" + '\n",
      "  '0.058*\"Solar\" + 0.058*\"the\" + 0.058*\"About\" + 0.058*\"Theres\" + '\n",
      "  '0.058*\"Learning\"'),\n",
      " (19,\n",
      "  '0.127*\"the\" + 0.065*\"Library\" + 0.065*\"Cat\" + 0.065*\"Learning\" + '\n",
      "  '0.065*\"Hats\" + 0.065*\"About\" + 0.065*\"in\" + 0.065*\"All\" + 0.065*\"Body\" + '\n",
      "  '0.065*\"Human\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the num_topics \n",
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + 0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + 0.023*\"Beyond\"'),\n",
       " (1,\n",
       "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + 0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + 0.023*\"Beyond\"'),\n",
       " (2,\n",
       "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + 0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + 0.023*\"Beyond\"'),\n",
       " (3,\n",
       "  '0.024*\"No\" + 0.024*\"Like\" + 0.023*\"Library\" + 0.023*\"System\" + 0.023*\"Our\" + 0.023*\"Hats\" + 0.023*\"Learning\" + 0.023*\"Theres\" + 0.023*\"About\" + 0.023*\"the\"'),\n",
       " (4,\n",
       "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + 0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + 0.023*\"Beyond\"'),\n",
       " (5,\n",
       "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + 0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + 0.023*\"Beyond\"'),\n",
       " (6,\n",
       "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + 0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + 0.023*\"Beyond\"'),\n",
       " (7,\n",
       "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + 0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + 0.023*\"Beyond\"'),\n",
       " (8,\n",
       "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + 0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + 0.023*\"Beyond\"'),\n",
       " (9,\n",
       "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + 0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + 0.023*\"Beyond\"'),\n",
       " (10,\n",
       "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + 0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + 0.023*\"Beyond\"'),\n",
       " (11,\n",
       "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + 0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + 0.023*\"Beyond\"'),\n",
       " (12,\n",
       "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + 0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + 0.023*\"Beyond\"'),\n",
       " (13,\n",
       "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + 0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + 0.023*\"Beyond\"'),\n",
       " (14,\n",
       "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + 0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + 0.023*\"Beyond\"'),\n",
       " (15,\n",
       "  '0.088*\"You\" + 0.045*\"Do\" + 0.045*\"for\" + 0.045*\"the\" + 0.045*\"Can\" + 0.045*\"All\" + 0.045*\"in\" + 0.045*\"Healthy\" + 0.045*\"Things\" + 0.045*\"That\"'),\n",
       " (16,\n",
       "  '0.098*\"Cent\" + 0.066*\"All\" + 0.066*\"Learning\" + 0.066*\"in\" + 0.066*\"Library\" + 0.066*\"About\" + 0.066*\"Cat\" + 0.066*\"Hats\" + 0.066*\"the\" + 0.034*\"Beyond\"'),\n",
       " (17,\n",
       "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + 0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + 0.023*\"Beyond\"'),\n",
       " (18,\n",
       "  '0.058*\"in\" + 0.058*\"Cat\" + 0.058*\"Place\" + 0.058*\"All\" + 0.058*\"Space\" + 0.058*\"Solar\" + 0.058*\"the\" + 0.058*\"About\" + 0.058*\"Theres\" + 0.058*\"Learning\"'),\n",
       " (19,\n",
       "  '0.127*\"the\" + 0.065*\"Library\" + 0.065*\"Cat\" + 0.065*\"Learning\" + 0.065*\"Hats\" + 0.065*\"About\" + 0.065*\"in\" + 0.065*\"All\" + 0.065*\"Body\" + 0.065*\"Human\"')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\" + 0.023*\"Good\" + 0.023*\"Solar\" + 0.023*\"Our\" + '\n",
      "  '0.023*\"System\" + 0.023*\"Space\"'),\n",
      " (1,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\" + 0.023*\"Good\" + 0.023*\"Solar\" + 0.023*\"Our\" + '\n",
      "  '0.023*\"System\" + 0.023*\"Space\"'),\n",
      " (2,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\" + 0.023*\"Good\" + 0.023*\"Solar\" + 0.023*\"Our\" + '\n",
      "  '0.023*\"System\" + 0.023*\"Space\"'),\n",
      " (3,\n",
      "  '0.024*\"No\" + 0.024*\"Like\" + 0.023*\"Library\" + 0.023*\"System\" + 0.023*\"Our\" '\n",
      "  '+ 0.023*\"Hats\" + 0.023*\"Learning\" + 0.023*\"Theres\" + 0.023*\"About\" + '\n",
      "  '0.023*\"the\" + 0.023*\"Solar\" + 0.023*\"Space\" + 0.023*\"All\" + 0.023*\"Place\" + '\n",
      "  '0.023*\"Cat\"'),\n",
      " (4,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\" + 0.023*\"Good\" + 0.023*\"Solar\" + 0.023*\"Our\" + '\n",
      "  '0.023*\"System\" + 0.023*\"Space\"'),\n",
      " (5,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\" + 0.023*\"Good\" + 0.023*\"Solar\" + 0.023*\"Our\" + '\n",
      "  '0.023*\"System\" + 0.023*\"Space\"'),\n",
      " (6,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\" + 0.023*\"Good\" + 0.023*\"Solar\" + 0.023*\"Our\" + '\n",
      "  '0.023*\"System\" + 0.023*\"Space\"'),\n",
      " (7,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\" + 0.023*\"Good\" + 0.023*\"Solar\" + 0.023*\"Our\" + '\n",
      "  '0.023*\"System\" + 0.023*\"Space\"'),\n",
      " (8,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\" + 0.023*\"Good\" + 0.023*\"Solar\" + 0.023*\"Our\" + '\n",
      "  '0.023*\"System\" + 0.023*\"Space\"'),\n",
      " (9,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\" + 0.023*\"Good\" + 0.023*\"Solar\" + 0.023*\"Our\" + '\n",
      "  '0.023*\"System\" + 0.023*\"Space\"'),\n",
      " (10,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\" + 0.023*\"Good\" + 0.023*\"Solar\" + 0.023*\"Our\" + '\n",
      "  '0.023*\"System\" + 0.023*\"Space\"'),\n",
      " (11,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\" + 0.023*\"Good\" + 0.023*\"Solar\" + 0.023*\"Our\" + '\n",
      "  '0.023*\"System\" + 0.023*\"Space\"'),\n",
      " (12,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\" + 0.023*\"Good\" + 0.023*\"Solar\" + 0.023*\"Our\" + '\n",
      "  '0.023*\"System\" + 0.023*\"Space\"'),\n",
      " (13,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\" + 0.023*\"Good\" + 0.023*\"Solar\" + 0.023*\"Our\" + '\n",
      "  '0.023*\"System\" + 0.023*\"Space\"'),\n",
      " (14,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\" + 0.023*\"Good\" + 0.023*\"Solar\" + 0.023*\"Our\" + '\n",
      "  '0.023*\"System\" + 0.023*\"Space\"'),\n",
      " (15,\n",
      "  '0.088*\"You\" + 0.045*\"Do\" + 0.045*\"for\" + 0.045*\"the\" + 0.045*\"Can\" + '\n",
      "  '0.045*\"All\" + 0.045*\"in\" + 0.045*\"Healthy\" + 0.045*\"Things\" + 0.045*\"That\" '\n",
      "  '+ 0.045*\"About\" + 0.045*\"Hats\" + 0.045*\"Staying\" + 0.045*\"Good\" + '\n",
      "  '0.045*\"Learning\"'),\n",
      " (16,\n",
      "  '0.098*\"Cent\" + 0.066*\"About\" + 0.066*\"All\" + 0.066*\"Cat\" + 0.066*\"Hats\" + '\n",
      "  '0.066*\"Learning\" + 0.066*\"Library\" + 0.066*\"in\" + 0.066*\"the\" + 0.034*\"On\" '\n",
      "  '+ 0.034*\"Insects\" + 0.034*\"Bugs\" + 0.034*\"Beyond\" + 0.034*\"New\" + '\n",
      "  '0.034*\"Cents\"'),\n",
      " (17,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\" + 0.023*\"Good\" + 0.023*\"Solar\" + 0.023*\"Our\" + '\n",
      "  '0.023*\"System\" + 0.023*\"Space\"'),\n",
      " (18,\n",
      "  '0.058*\"in\" + 0.058*\"Cat\" + 0.058*\"Place\" + 0.058*\"All\" + 0.058*\"Space\" + '\n",
      "  '0.058*\"Solar\" + 0.058*\"the\" + 0.058*\"About\" + 0.058*\"Theres\" + '\n",
      "  '0.058*\"Learning\" + 0.058*\"Hats\" + 0.058*\"Our\" + 0.058*\"System\" + '\n",
      "  '0.058*\"Library\" + 0.058*\"Like\"'),\n",
      " (19,\n",
      "  '0.127*\"the\" + 0.065*\"Library\" + 0.065*\"Cat\" + 0.065*\"Learning\" + '\n",
      "  '0.065*\"Hats\" + 0.065*\"About\" + 0.065*\"in\" + 0.065*\"All\" + 0.065*\"Body\" + '\n",
      "  '0.065*\"Outside\" + 0.065*\"Human\" + 0.065*\"Inside\" + 0.065*\"Your\" + '\n",
      "  '0.003*\"You\" + 0.003*\"Oh\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics(20,15)) # number of words in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x1911cab69d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.interfaces.TransformedCorpus object at 0x000001911CAB69D0>\n"
     ]
    }
   ],
   "source": [
    "pprint(doc_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -5.112233056128025\n",
      "\n",
      "Coherence Score:  0.33842089435486344\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=cat_doc_tk, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model2 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (1,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (2,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (3,\n",
      "  '0.024*\"No\" + 0.024*\"Like\" + 0.023*\"Library\" + 0.023*\"System\" + 0.023*\"Our\" '\n",
      "  '+ 0.023*\"Hats\" + 0.023*\"Learning\" + 0.023*\"Theres\" + 0.023*\"About\" + '\n",
      "  '0.023*\"the\"'),\n",
      " (4,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (5,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (6,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (7,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (8,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (9,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (10,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (11,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (12,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (13,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (14,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (15,\n",
      "  '0.088*\"You\" + 0.045*\"Do\" + 0.045*\"for\" + 0.045*\"the\" + 0.045*\"Can\" + '\n",
      "  '0.045*\"All\" + 0.045*\"in\" + 0.045*\"Healthy\" + 0.045*\"Things\" + 0.045*\"That\"'),\n",
      " (16,\n",
      "  '0.098*\"Cent\" + 0.066*\"All\" + 0.066*\"Learning\" + 0.066*\"in\" + '\n",
      "  '0.066*\"Library\" + 0.066*\"About\" + 0.066*\"Cat\" + 0.066*\"Hats\" + 0.066*\"the\" '\n",
      "  '+ 0.034*\"Beyond\"'),\n",
      " (17,\n",
      "  '0.023*\"Bugs\" + 0.023*\"Insects\" + 0.023*\"Oh\" + 0.023*\"Staying\" + '\n",
      "  '0.023*\"That\" + 0.023*\"The\" + 0.023*\"Things\" + 0.023*\"You\" + 0.023*\"for\" + '\n",
      "  '0.023*\"Beyond\"'),\n",
      " (18,\n",
      "  '0.058*\"in\" + 0.058*\"Cat\" + 0.058*\"Place\" + 0.058*\"All\" + 0.058*\"Space\" + '\n",
      "  '0.058*\"Solar\" + 0.058*\"the\" + 0.058*\"About\" + 0.058*\"Theres\" + '\n",
      "  '0.058*\"Learning\"'),\n",
      " (19,\n",
      "  '0.127*\"the\" + 0.065*\"Library\" + 0.065*\"Cat\" + 0.065*\"Learning\" + '\n",
      "  '0.065*\"Hats\" + 0.065*\"About\" + 0.065*\"in\" + 0.065*\"All\" + 0.065*\"Body\" + '\n",
      "  '0.065*\"Human\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model2.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model3 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=3, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.073*\"the\" + 0.059*\"in\" + 0.059*\"Hats\" + 0.059*\"Cat\" + 0.059*\"All\" + '\n",
      "  '0.059*\"Learning\" + 0.059*\"Library\" + 0.059*\"About\" + 0.045*\"Cent\" + '\n",
      "  '0.018*\"Your\"'),\n",
      " (1,\n",
      "  '0.023*\"Learning\" + 0.023*\"All\" + 0.023*\"the\" + 0.023*\"Cat\" + 0.023*\"in\" + '\n",
      "  '0.023*\"Hats\" + 0.023*\"You\" + 0.023*\"Library\" + 0.023*\"About\" + '\n",
      "  '0.023*\"Things\"'),\n",
      " (2,\n",
      "  '0.067*\"You\" + 0.038*\"Oh\" + 0.038*\"Do\" + 0.038*\"The\" + 0.038*\"Are\" + '\n",
      "  '0.038*\"Can\" + 0.038*\"Staying\" + 0.038*\"for\" + 0.038*\"Healthy\" + '\n",
      "  '0.038*\"Good\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model3.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (3.3.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from pyLDAvis) (1.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from pyLDAvis) (3.0.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from pyLDAvis) (1.0.2)\n",
      "Requirement already satisfied: future in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: funcy in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from pyLDAvis) (1.17)\n",
      "Requirement already satisfied: joblib in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from pyLDAvis) (4.2.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from pyLDAvis) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from pyLDAvis) (1.20.3)\n",
      "Requirement already satisfied: numexpr in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from pyLDAvis) (61.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2021.3)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from gensim->pyLDAvis) (0.29.28)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from jinja2->pyLDAvis) (2.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from numexpr->pyLDAvis) (21.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from scikit-learn->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ajey kumar\\.conda\\envs\\ta\\lib\\site-packages (from packaging->numexpr->pyLDAvis) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "#import pyLDAvis.gensim  # don't skip this\n",
    "#from pyLDAvis import gensim\n",
    "import pyLDAvis.gensim_models \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajey Kumar\\.conda\\envs\\TA\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1416417230352155683180311256\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1416417230352155683180311256_data = {\"mdsDat\": {\"x\": [0.08984466172712458, 0.14368108043963623, -0.17630691928838224, -0.11764212425835903, 0.003418092732533662, 0.0038003472431633943, 0.0038003472431634177, 0.003800347243163118, 0.0038003472431630218, 0.003800347243163404, 0.0038003472431634034, 0.003800347243162858, 0.00380034724316298, 0.0038003472431628674, 0.0038003472431628544, 0.003800347243162881, 0.0038003472431630625, 0.0038003472431628912, 0.003800347243163133, 0.003800347243163585], \"y\": [-0.16258716387234096, 0.08622744486905973, 0.030166521764581775, -0.15661090575423886, 0.012578548620718175, 0.012681703624813976, 0.01268170362481391, 0.012681703624814692, 0.012681703624814945, 0.012681703624813946, 0.012681703624813946, 0.012681703624815369, 0.012681703624815048, 0.012681703624815344, 0.012681703624815372, 0.012681703624815313, 0.012681703624814834, 0.012681703624815289, 0.01268170362481465, 0.012681703624813476], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [35.2544355611728, 25.743639365809095, 19.498479489157344, 17.032015147276862, 0.17496726141371166, 0.15309754501134604, 0.15309754501134604, 0.15309754501134604, 0.15309754501134604, 0.15309754501134604, 0.15309754501134604, 0.15309754501134604, 0.15309754501134604, 0.15309754501134604, 0.15309754501134604, 0.15309754501134604, 0.15309754501134604, 0.15309754501134604, 0.15309754501134604, 0.15309754501134604]}, \"tinfo\": {\"Term\": [\"the\", \"Library\", \"Hats\", \"Learning\", \"About\", \"All\", \"Cat\", \"in\", \"Cent\", \"You\", \"New\", \"One\", \"Two\", \"Money\", \"Cents\", \"On\", \"Insects\", \"Bugs\", \"Beyond\", \"Old\", \"Do\", \"for\", \"Can\", \"Healthy\", \"Things\", \"That\", \"Staying\", \"Good\", \"The\", \"Are\", \"Cent\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"Beyond\", \"Bugs\", \"Insects\", \"On\", \"Library\", \"Hats\", \"Learning\", \"About\", \"All\", \"Cat\", \"in\", \"the\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Are\", \"Can\", \"Do\", \"Good\", \"Healthy\", \"Oh\", \"You\", \"Do\", \"for\", \"Can\", \"Healthy\", \"Things\", \"That\", \"Staying\", \"Good\", \"The\", \"Are\", \"Oh\", \"Hats\", \"Learning\", \"About\", \"All\", \"in\", \"Library\", \"Cat\", \"the\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Cent\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"Place\", \"Space\", \"Solar\", \"Theres\", \"Our\", \"System\", \"Like\", \"No\", \"in\", \"Cat\", \"All\", \"About\", \"Learning\", \"Hats\", \"Library\", \"the\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"Cent\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"Body\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"the\", \"Library\", \"Learning\", \"Hats\", \"About\", \"Cat\", \"All\", \"in\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Space\", \"Place\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"for\", \"Do\", \"You\", \"Cent\", \"Cents\", \"Money\", \"New\", \"Old\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Space\", \"Place\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"for\", \"Do\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"Library\", \"Hats\", \"Learning\", \"About\", \"All\", \"the\", \"Cat\", \"in\", \"Cent\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Space\", \"Place\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"for\", \"Do\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"About\", \"All\", \"Cat\", \"Cent\", \"Hats\", \"Learning\", \"Library\", \"in\", \"the\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Space\", \"Place\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"for\", \"Do\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"About\", \"All\", \"Cat\", \"Cent\", \"Hats\", \"Learning\", \"Library\", \"in\", \"the\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Space\", \"Place\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"for\", \"Do\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"About\", \"All\", \"Cat\", \"Cent\", \"Hats\", \"Learning\", \"Library\", \"in\", \"the\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Space\", \"Place\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"for\", \"Do\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"About\", \"All\", \"Cat\", \"Cent\", \"Hats\", \"Learning\", \"Library\", \"in\", \"the\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Space\", \"Place\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"for\", \"Do\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"About\", \"All\", \"Cat\", \"Cent\", \"Hats\", \"Learning\", \"Library\", \"in\", \"the\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Space\", \"Place\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"for\", \"Do\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"About\", \"All\", \"Cat\", \"Cent\", \"Hats\", \"Learning\", \"Library\", \"in\", \"the\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Space\", \"Place\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"for\", \"Do\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"About\", \"All\", \"Cat\", \"Cent\", \"Hats\", \"Learning\", \"Library\", \"in\", \"the\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Space\", \"Place\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"for\", \"Do\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"About\", \"All\", \"Cat\", \"Cent\", \"Hats\", \"Learning\", \"Library\", \"in\", \"the\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Space\", \"Place\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"for\", \"Do\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"About\", \"All\", \"Cat\", \"Cent\", \"Hats\", \"Learning\", \"Library\", \"in\", \"the\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Space\", \"Place\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"for\", \"Do\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"About\", \"All\", \"Cat\", \"Cent\", \"Hats\", \"Learning\", \"Library\", \"in\", \"the\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Space\", \"Place\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"for\", \"Do\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"About\", \"All\", \"Cat\", \"Cent\", \"Hats\", \"Learning\", \"Library\", \"in\", \"the\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Space\", \"Place\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"for\", \"Do\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"About\", \"All\", \"Cat\", \"Cent\", \"Hats\", \"Learning\", \"Library\", \"in\", \"the\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Space\", \"Place\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"for\", \"Do\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"About\", \"All\", \"Cat\", \"Cent\", \"Hats\", \"Learning\", \"Library\", \"in\", \"the\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Space\", \"Place\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"for\", \"Do\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"About\", \"All\", \"Cat\", \"Cent\", \"Hats\", \"Learning\", \"Library\", \"in\", \"the\", \"Human\", \"Inside\", \"Outside\", \"Your\", \"Body\", \"No\", \"Like\", \"System\", \"Our\", \"Theres\", \"Solar\", \"Space\", \"Place\", \"Oh\", \"Are\", \"The\", \"Good\", \"Staying\", \"That\", \"Things\", \"Healthy\", \"Can\", \"for\", \"Do\", \"Cents\", \"Money\", \"New\", \"Old\", \"One\", \"Two\", \"About\", \"All\", \"Cat\", \"Cent\", \"Hats\", \"Learning\", \"Library\", \"in\", \"the\"], \"Freq\": [5.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.757073605590918, 0.9491522569560188, 0.9491522569560188, 0.9491522569560188, 0.9491522569560188, 0.9491522569560188, 0.9491522569560188, 0.9491522569560188, 0.9491522569560188, 0.9491522569560188, 0.9491522569560188, 1.8531259595077616, 1.8531259595077616, 1.8531259595077616, 1.8531259595077616, 1.8531259595077616, 1.8531259595077616, 1.8531259595077616, 1.8531244885780833, 0.04519799825596059, 0.04519799825596059, 0.04519799825596059, 0.04519799825596059, 0.04519799825596059, 0.04519799825596059, 0.04519799825596059, 0.04519799825596059, 0.04519799825596059, 0.04519799825596059, 0.04519799825596059, 0.04519799825596059, 0.04519799825596059, 0.04519799825596059, 0.04519799825596059, 0.04519799825596059, 0.04519799825596059, 1.8197337838881893, 0.9321614682073425, 0.9321505736799193, 0.9321284777369765, 0.9321120592238176, 0.9321050775196238, 0.9321029293029488, 0.9320839789629943, 0.9320831350207291, 0.9320576633087256, 0.9320212970692989, 0.9319910685918007, 0.9320925718296943, 0.9320781480891622, 0.9320962544868515, 0.9321165858232399, 0.932115972047047, 0.931989150541198, 0.932072240493306, 0.9321389886542791, 0.04439511293727596, 0.04439511293727596, 0.04439511293727596, 0.04439511293727596, 0.04439511293727596, 0.04439511293727596, 0.04439511293727596, 0.04439511293727596, 0.04439511293727596, 0.04439511293727596, 0.04439511293727596, 0.04439511293727596, 0.04439511293727596, 0.04439511293727596, 0.04439511293727596, 0.04439511293727596, 0.04439511293727596, 0.9001194995534868, 0.9000852146550941, 0.9000350076174477, 0.8997739775096845, 0.8996771662542228, 0.8996576994051354, 0.8990161650352098, 0.8988780956816823, 0.9001945195599699, 0.900153668231885, 0.900107528894048, 0.8998243588773226, 0.8997239448020299, 0.89970773211279, 0.8996430556858219, 0.8999792220200629, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.042936016973461465, 0.8828792859462253, 0.8828790829087825, 0.8828790829087825, 0.8828790829087825, 0.8828790829087825, 1.7238587499955151, 0.8831592745799368, 0.8830749125224263, 0.8830613090137542, 0.8830577558585041, 0.8830808006082693, 0.8830386703388747, 0.883039177932482, 0.04204200203876211, 0.04204200203876211, 0.04204200203876211, 0.04204200203876211, 0.04204200203876211, 0.04204200203876211, 0.04204200203876211, 0.04204200203876211, 0.04232799296687832, 0.04229938372219456, 0.042264949840868586, 0.04224079155762764, 0.04224001747737672, 0.042222223148985584, 0.04222015153257635, 0.04221350522878255, 0.042197966519483245, 0.04217704731794801, 0.04216673999526255, 0.04247986814660175, 0.04204200203876211, 0.04204200203876211, 0.04204200203876211, 0.04204200203876211, 0.04204200203876211, 0.0033341607241093017, 0.003324089833298646, 0.00327729918193496, 0.0032758826818243044, 0.0032688199961078404, 0.0032497779379296998, 0.0032461171468283353, 0.003243619955946712, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.003131754773270867, 0.0032780743070645346, 0.003273353422193627, 0.0032721754822893495, 0.0032648520750358607, 0.003244205275797333, 0.0032535563144442626, 0.0032408336248805287, 0.003237857227813854, 0.003131754773270867, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473, 0.00278359204313473], \"Total\": [5.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.9313323729607124, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 4.6129493952688, 4.6130148065332115, 4.613029021050687, 4.613123061452494, 4.613386830486739, 4.61342738311312, 4.613467366922092, 5.4541088862094025, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.1073403314412071, 1.1073460949061733, 1.1073478588523187, 1.1073435772280704, 1.1073452151023138, 1.1073387122083926, 1.9952333026845044, 1.1073478588523187, 1.1073472716475807, 1.1073460949061733, 1.1073452151023138, 1.1073448797019139, 1.107344803101648, 1.1073436470900846, 1.1073435772280704, 1.1073422637993078, 1.1073403314412071, 1.1073387122083926, 4.6130148065332115, 4.613029021050687, 4.613123061452494, 4.613386830486739, 4.613467366922092, 4.6129493952688, 4.61342738311312, 5.4541088862094025, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 2.9313323729607124, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.076752113388453, 1.076720325680942, 1.0766737794343968, 1.0764317913848116, 1.0763420428150665, 1.07632399246609, 1.0757292487475278, 1.0756012502848111, 4.613467366922092, 4.61342738311312, 4.613386830486739, 4.613123061452494, 4.613029021050687, 4.6130148065332115, 4.6129493952688, 5.4541088862094025, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 2.9313323729607124, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.060294049533215, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 5.4541088862094025, 4.6129493952688, 4.613029021050687, 4.6130148065332115, 4.613123061452494, 4.61342738311312, 4.613386830486739, 4.613467366922092, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.076720325680942, 1.076752113388453, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 1.1073472716475807, 1.1073478588523187, 1.9952333026845044, 2.9313323729607124, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.076720325680942, 1.076752113388453, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 1.1073472716475807, 1.1073478588523187, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 4.6129493952688, 4.6130148065332115, 4.613029021050687, 4.613123061452494, 4.613386830486739, 5.4541088862094025, 4.61342738311312, 4.613467366922092, 2.9313323729607124, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.076720325680942, 1.076752113388453, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 1.1073472716475807, 1.1073478588523187, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 4.613123061452494, 4.613386830486739, 4.61342738311312, 2.9313323729607124, 4.6130148065332115, 4.613029021050687, 4.6129493952688, 4.613467366922092, 5.4541088862094025, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.076720325680942, 1.076752113388453, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 1.1073472716475807, 1.1073478588523187, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 4.613123061452494, 4.613386830486739, 4.61342738311312, 2.9313323729607124, 4.6130148065332115, 4.613029021050687, 4.6129493952688, 4.613467366922092, 5.4541088862094025, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.076720325680942, 1.076752113388453, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 1.1073472716475807, 1.1073478588523187, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 4.613123061452494, 4.613386830486739, 4.61342738311312, 2.9313323729607124, 4.6130148065332115, 4.613029021050687, 4.6129493952688, 4.613467366922092, 5.4541088862094025, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.076720325680942, 1.076752113388453, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 1.1073472716475807, 1.1073478588523187, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 4.613123061452494, 4.613386830486739, 4.61342738311312, 2.9313323729607124, 4.6130148065332115, 4.613029021050687, 4.6129493952688, 4.613467366922092, 5.4541088862094025, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.076720325680942, 1.076752113388453, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 1.1073472716475807, 1.1073478588523187, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 4.613123061452494, 4.613386830486739, 4.61342738311312, 2.9313323729607124, 4.6130148065332115, 4.613029021050687, 4.6129493952688, 4.613467366922092, 5.4541088862094025, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.076720325680942, 1.076752113388453, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 1.1073472716475807, 1.1073478588523187, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 4.613123061452494, 4.613386830486739, 4.61342738311312, 2.9313323729607124, 4.6130148065332115, 4.613029021050687, 4.6129493952688, 4.613467366922092, 5.4541088862094025, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.076720325680942, 1.076752113388453, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 1.1073472716475807, 1.1073478588523187, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 4.613123061452494, 4.613386830486739, 4.61342738311312, 2.9313323729607124, 4.6130148065332115, 4.613029021050687, 4.6129493952688, 4.613467366922092, 5.4541088862094025, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.076720325680942, 1.076752113388453, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 1.1073472716475807, 1.1073478588523187, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 4.613123061452494, 4.613386830486739, 4.61342738311312, 2.9313323729607124, 4.6130148065332115, 4.613029021050687, 4.6129493952688, 4.613467366922092, 5.4541088862094025, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.076720325680942, 1.076752113388453, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 1.1073472716475807, 1.1073478588523187, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 4.613123061452494, 4.613386830486739, 4.61342738311312, 2.9313323729607124, 4.6130148065332115, 4.613029021050687, 4.6129493952688, 4.613467366922092, 5.4541088862094025, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.076720325680942, 1.076752113388453, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 1.1073472716475807, 1.1073478588523187, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 4.613123061452494, 4.613386830486739, 4.61342738311312, 2.9313323729607124, 4.6130148065332115, 4.613029021050687, 4.6129493952688, 4.613467366922092, 5.4541088862094025, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.076720325680942, 1.076752113388453, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 1.1073472716475807, 1.1073478588523187, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 4.613123061452494, 4.613386830486739, 4.61342738311312, 2.9313323729607124, 4.6130148065332115, 4.613029021050687, 4.6129493952688, 4.613467366922092, 5.4541088862094025, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.076720325680942, 1.076752113388453, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 1.1073472716475807, 1.1073478588523187, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 4.613123061452494, 4.613386830486739, 4.61342738311312, 2.9313323729607124, 4.6130148065332115, 4.613029021050687, 4.6129493952688, 4.613467366922092, 5.4541088862094025, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.076720325680942, 1.076752113388453, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 1.1073472716475807, 1.1073478588523187, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 4.613123061452494, 4.613386830486739, 4.61342738311312, 2.9313323729607124, 4.6130148065332115, 4.613029021050687, 4.6129493952688, 4.613467366922092, 5.4541088862094025, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.076720325680942, 1.076752113388453, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 1.1073472716475807, 1.1073478588523187, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 4.613123061452494, 4.613386830486739, 4.61342738311312, 2.9313323729607124, 4.6130148065332115, 4.613029021050687, 4.6129493952688, 4.613467366922092, 5.4541088862094025, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060293846495772, 1.060294049533215, 1.0756012502848111, 1.0757292487475278, 1.07632399246609, 1.0763420428150665, 1.0764317913848116, 1.0766737794343968, 1.076720325680942, 1.076752113388453, 1.1073387122083926, 1.1073403314412071, 1.1073422637993078, 1.1073435772280704, 1.1073436470900846, 1.107344803101648, 1.1073448797019139, 1.1073452151023138, 1.1073460949061733, 1.1073472716475807, 1.1073478588523187, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 1.12341102432581, 4.613123061452494, 4.613386830486739, 4.61342738311312, 2.9313323729607124, 4.6130148065332115, 4.613029021050687, 4.6129493952688, 4.613467366922092, 5.4541088862094025], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.3253, -3.3916, -3.3916, -3.3916, -3.3916, -3.3916, -3.3916, -3.3916, -3.3916, -3.3916, -3.3916, -2.7226, -2.7226, -2.7226, -2.7226, -2.7226, -2.7226, -2.7226, -2.7226, -6.4362, -6.4362, -6.4362, -6.4362, -6.4362, -6.4362, -6.4362, -6.4362, -6.4362, -6.4362, -6.4362, -6.4362, -6.4362, -6.4362, -6.4362, -6.4362, -6.4362, -2.4264, -3.0953, -3.0953, -3.0953, -3.0953, -3.0954, -3.0954, -3.0954, -3.0954, -3.0954, -3.0954, -3.0955, -3.0954, -3.0954, -3.0954, -3.0953, -3.0953, -3.0955, -3.0954, -3.0953, -6.1397, -6.1397, -6.1397, -6.1397, -6.1397, -6.1397, -6.1397, -6.1397, -6.1397, -6.1397, -6.1397, -6.1397, -6.1397, -6.1397, -6.1397, -6.1397, -6.1397, -2.8524, -2.8525, -2.8525, -2.8528, -2.8529, -2.8529, -2.8536, -2.8538, -2.8523, -2.8524, -2.8524, -2.8527, -2.8529, -2.8529, -2.853, -2.8526, -5.8952, -5.8952, -5.8952, -5.8952, -5.8952, -5.8952, -5.8952, -5.8952, -5.8952, -5.8952, -5.8952, -5.8952, -5.8952, -5.8952, -5.8952, -5.8952, -5.8952, -5.8952, -5.8952, -5.8952, -5.8952, -2.7365, -2.7365, -2.7365, -2.7365, -2.7365, -2.0674, -2.7362, -2.7363, -2.7363, -2.7363, -2.7363, -2.7363, -2.7363, -5.781, -5.781, -5.781, -5.781, -5.781, -5.781, -5.781, -5.781, -5.7743, -5.7749, -5.7757, -5.7763, -5.7763, -5.7768, -5.7768, -5.777, -5.7773, -5.7778, -5.7781, -5.7707, -5.781, -5.781, -5.781, -5.781, -5.781, -3.7372, -3.7403, -3.7544, -3.7549, -3.757, -3.7629, -3.764, -3.7648, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7999, -3.7542, -3.7556, -3.756, -3.7582, -3.7646, -3.7617, -3.7656, -3.7665, -3.7999, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842, -3.7842], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9813, 0.874, 0.874, 0.874, 0.874, 0.874, 0.874, 0.874, 0.874, 0.874, 0.874, 0.1306, 0.1306, 0.1306, 0.1305, 0.1305, 0.1305, 0.1305, -0.0369, -2.1127, -2.1127, -2.1127, -2.1127, -2.1127, -2.127, -2.1271, -2.1277, -2.1277, -2.1278, -2.128, -2.1561, -2.1561, -2.1561, -2.1561, -2.1561, -2.1561, 1.2649, 1.1848, 1.1848, 1.1847, 1.1847, 1.1847, 1.1847, 1.1847, 1.1847, 1.1847, 1.1846, 1.1846, -0.2422, -0.2422, -0.2422, -0.2423, -0.2423, -0.2423, -0.2423, -0.4097, -1.8162, -1.8162, -1.8162, -1.8162, -1.8162, -1.8305, -1.8306, -1.8312, -1.8312, -1.8313, -2.8331, -1.874, -1.874, -1.874, -1.874, -1.874, -1.874, 1.4557, 1.4556, 1.4556, 1.4556, 1.4555, 1.4555, 1.4554, 1.4553, 0.0007, 0.0007, 0.0006, 0.0004, 0.0003, 0.0003, 0.0002, -0.1669, -1.5718, -1.5718, -1.5718, -1.5718, -1.5718, -1.6152, -1.6152, -1.6152, -1.6152, -1.6152, -1.6152, -1.6152, -1.6152, -1.6152, -2.5887, -1.6296, -1.6296, -1.6296, -1.6296, -1.6296, -1.6296, 1.587, 1.587, 1.587, 1.587, 1.587, 0.6183, 0.117, 0.1168, 0.1168, 0.1168, 0.1168, 0.1167, 0.1167, -1.4719, -1.472, -1.4726, -1.4726, -1.4727, -1.4729, -1.4729, -1.473, -1.4942, -1.4949, -1.4957, -1.4963, -1.4963, -1.4967, -1.4967, -1.4969, -1.4973, -1.4978, -1.498, -2.0794, -2.4745, -1.5154, -1.5154, -1.5154, -1.5154, 0.5719, 0.5688, 0.554, 0.5536, 0.5513, 0.5453, 0.5441, 0.5433, 0.5236, 0.5236, 0.5236, 0.5236, 0.5236, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4658, 0.4658, 0.4658, 0.4658, 0.4658, 0.4658, -0.901, -0.9025, -0.9029, -0.9051, -0.9115, -1.076, -0.9126, -0.9135, -0.4933, 0.5393, 0.5393, 0.5393, 0.5393, 0.5393, 0.525, 0.5248, 0.5243, 0.5243, 0.5242, 0.524, 0.5239, 0.5239, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, -0.9311, -0.9311, -0.9311, -0.4776, -0.931, -0.931, -0.931, -0.9311, -1.0985, 0.5393, 0.5393, 0.5393, 0.5393, 0.5393, 0.525, 0.5248, 0.5243, 0.5243, 0.5242, 0.524, 0.5239, 0.5239, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, -0.9311, -0.9311, -0.9311, -0.4776, -0.931, -0.931, -0.931, -0.9311, -1.0985, 0.5393, 0.5393, 0.5393, 0.5393, 0.5393, 0.525, 0.5248, 0.5243, 0.5243, 0.5242, 0.524, 0.5239, 0.5239, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, -0.9311, -0.9311, -0.9311, -0.4776, -0.931, -0.931, -0.931, -0.9311, -1.0985, 0.5393, 0.5393, 0.5393, 0.5393, 0.5393, 0.525, 0.5248, 0.5243, 0.5243, 0.5242, 0.524, 0.5239, 0.5239, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, -0.9311, -0.9311, -0.9311, -0.4776, -0.931, -0.931, -0.931, -0.9311, -1.0985, 0.5393, 0.5393, 0.5393, 0.5393, 0.5393, 0.525, 0.5248, 0.5243, 0.5243, 0.5242, 0.524, 0.5239, 0.5239, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, -0.9311, -0.9311, -0.9311, -0.4776, -0.931, -0.931, -0.931, -0.9311, -1.0985, 0.5393, 0.5393, 0.5393, 0.5393, 0.5393, 0.525, 0.5248, 0.5243, 0.5243, 0.5242, 0.524, 0.5239, 0.5239, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, -0.9311, -0.9311, -0.9311, -0.4776, -0.931, -0.931, -0.931, -0.9311, -1.0985, 0.5393, 0.5393, 0.5393, 0.5393, 0.5393, 0.525, 0.5248, 0.5243, 0.5243, 0.5242, 0.524, 0.5239, 0.5239, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, -0.9311, -0.9311, -0.9311, -0.4776, -0.931, -0.931, -0.931, -0.9311, -1.0985, 0.5393, 0.5393, 0.5393, 0.5393, 0.5393, 0.525, 0.5248, 0.5243, 0.5243, 0.5242, 0.524, 0.5239, 0.5239, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, -0.9311, -0.9311, -0.9311, -0.4776, -0.931, -0.931, -0.931, -0.9311, -1.0985, 0.5393, 0.5393, 0.5393, 0.5393, 0.5393, 0.525, 0.5248, 0.5243, 0.5243, 0.5242, 0.524, 0.5239, 0.5239, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, -0.9311, -0.9311, -0.9311, -0.4776, -0.931, -0.931, -0.931, -0.9311, -1.0985, 0.5393, 0.5393, 0.5393, 0.5393, 0.5393, 0.525, 0.5248, 0.5243, 0.5243, 0.5242, 0.524, 0.5239, 0.5239, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, -0.9311, -0.9311, -0.9311, -0.4776, -0.931, -0.931, -0.931, -0.9311, -1.0985, 0.5393, 0.5393, 0.5393, 0.5393, 0.5393, 0.525, 0.5248, 0.5243, 0.5243, 0.5242, 0.524, 0.5239, 0.5239, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, -0.9311, -0.9311, -0.9311, -0.4776, -0.931, -0.931, -0.931, -0.9311, -1.0985, 0.5393, 0.5393, 0.5393, 0.5393, 0.5393, 0.525, 0.5248, 0.5243, 0.5243, 0.5242, 0.524, 0.5239, 0.5239, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, -0.9311, -0.9311, -0.9311, -0.4776, -0.931, -0.931, -0.931, -0.9311, -1.0985, 0.5393, 0.5393, 0.5393, 0.5393, 0.5393, 0.525, 0.5248, 0.5243, 0.5243, 0.5242, 0.524, 0.5239, 0.5239, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, -0.9311, -0.9311, -0.9311, -0.4776, -0.931, -0.931, -0.931, -0.9311, -1.0985, 0.5393, 0.5393, 0.5393, 0.5393, 0.5393, 0.525, 0.5248, 0.5243, 0.5243, 0.5242, 0.524, 0.5239, 0.5239, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, -0.9311, -0.9311, -0.9311, -0.4776, -0.931, -0.931, -0.931, -0.9311, -1.0985, 0.5393, 0.5393, 0.5393, 0.5393, 0.5393, 0.525, 0.5248, 0.5243, 0.5243, 0.5242, 0.524, 0.5239, 0.5239, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4959, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, -0.9311, -0.9311, -0.9311, -0.4776, -0.931, -0.931, -0.931, -0.9311, -1.0985]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 1, 2, 3, 4, 2, 1, 4, 1, 2, 1, 2, 3, 4, 1, 1, 2, 2, 1, 2, 3, 4, 2, 4, 1, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 1, 1, 3, 2, 1, 1, 1, 3, 4, 3, 3, 3, 2, 3, 2, 2, 3, 2, 1, 2, 4, 2, 1, 2, 3, 4, 1, 2, 3, 4], \"Freq\": [0.4335457722149466, 0.2167728861074733, 0.2167728861074733, 0.2167728861074733, 0.4335209843630191, 0.21676049218150956, 0.21676049218150956, 0.21676049218150956, 0.9030647323199152, 0.8901461516279205, 0.9431345959550006, 0.8901461516279205, 0.9030600320893633, 0.4335171736572147, 0.21675858682860735, 0.21675858682860735, 0.21675858682860735, 1.0234253978404815, 0.8901461516279205, 0.9030585935628426, 0.9030620853043863, 0.4335559463558381, 0.21677797317791905, 0.21677797317791905, 0.21677797317791905, 0.9030607495853084, 0.9431347765574225, 0.8901461516279205, 0.9431347765574225, 0.4335546104031381, 0.21677730520156904, 0.21677730520156904, 0.21677730520156904, 0.4335620941455089, 0.21678104707275445, 0.21678104707275445, 0.21678104707275445, 0.9296019432067135, 0.8901461516279205, 0.8901461516279205, 0.9297125674920957, 0.903066052848162, 0.8901461516279205, 0.8901461516279205, 0.8901461516279205, 0.9290726927144819, 0.9431347765574225, 0.9287188644126082, 0.9287864338307975, 0.928746282715131, 0.9030620283304411, 0.9290882736050368, 0.9030610855796879, 0.9030631564345654, 0.9289952303559491, 0.9030610231106951, 0.8901461516279205, 1.0023890425791722, 0.9431347765574225, 0.9030590724372647, 0.4335134164684283, 0.21675670823421414, 0.21675670823421414, 0.21675670823421414, 0.3666960161094248, 0.1833480080547124, 0.1833480080547124, 0.3666960161094248], \"Term\": [\"About\", \"About\", \"About\", \"About\", \"All\", \"All\", \"All\", \"All\", \"Are\", \"Beyond\", \"Body\", \"Bugs\", \"Can\", \"Cat\", \"Cat\", \"Cat\", \"Cat\", \"Cent\", \"Cents\", \"Do\", \"Good\", \"Hats\", \"Hats\", \"Hats\", \"Hats\", \"Healthy\", \"Human\", \"Insects\", \"Inside\", \"Learning\", \"Learning\", \"Learning\", \"Learning\", \"Library\", \"Library\", \"Library\", \"Library\", \"Like\", \"Money\", \"New\", \"No\", \"Oh\", \"Old\", \"On\", \"One\", \"Our\", \"Outside\", \"Place\", \"Solar\", \"Space\", \"Staying\", \"System\", \"That\", \"The\", \"Theres\", \"Things\", \"Two\", \"You\", \"Your\", \"for\", \"in\", \"in\", \"in\", \"in\", \"the\", \"the\", \"the\", \"the\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [17, 16, 19, 20, 4, 12, 18, 15, 14, 13, 1, 2, 10, 9, 8, 7, 6, 5, 3, 11]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1416417230352155683180311256\", ldavis_el1416417230352155683180311256_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1416417230352155683180311256\", ldavis_el1416417230352155683180311256_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1416417230352155683180311256\", ldavis_el1416417230352155683180311256_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "16     0.089845 -0.162587       1        1  35.254436\n",
       "15     0.143681  0.086227       2        1  25.743639\n",
       "18    -0.176307  0.030167       3        1  19.498479\n",
       "19    -0.117642 -0.156611       4        1  17.032015\n",
       "3      0.003418  0.012579       5        1   0.174967\n",
       "11     0.003800  0.012682       6        1   0.153098\n",
       "17     0.003800  0.012682       7        1   0.153098\n",
       "14     0.003800  0.012682       8        1   0.153098\n",
       "13     0.003800  0.012682       9        1   0.153098\n",
       "12     0.003800  0.012682      10        1   0.153098\n",
       "0      0.003800  0.012682      11        1   0.153098\n",
       "1      0.003800  0.012682      12        1   0.153098\n",
       "9      0.003800  0.012682      13        1   0.153098\n",
       "8      0.003800  0.012682      14        1   0.153098\n",
       "7      0.003800  0.012682      15        1   0.153098\n",
       "6      0.003800  0.012682      16        1   0.153098\n",
       "5      0.003800  0.012682      17        1   0.153098\n",
       "4      0.003800  0.012682      18        1   0.153098\n",
       "2      0.003800  0.012682      19        1   0.153098\n",
       "10     0.003800  0.012682      20        1   0.153098, topic_info=        Term      Freq     Total Category  logprob  loglift\n",
       "14       the  5.000000  5.000000  Default  30.0000  30.0000\n",
       "7    Library  4.000000  4.000000  Default  29.0000  29.0000\n",
       "5       Hats  4.000000  4.000000  Default  28.0000  28.0000\n",
       "6   Learning  4.000000  4.000000  Default  27.0000  27.0000\n",
       "0      About  4.000000  4.000000  Default  26.0000  26.0000\n",
       "..       ...       ...       ...      ...      ...      ...\n",
       "5       Hats  0.002784  4.613015  Topic20  -3.7842  -0.9310\n",
       "6   Learning  0.002784  4.613029  Topic20  -3.7842  -0.9310\n",
       "7    Library  0.002784  4.612949  Topic20  -3.7842  -0.9310\n",
       "13        in  0.002784  4.613467  Topic20  -3.7842  -0.9311\n",
       "14       the  0.002784  5.454109  Topic20  -3.7842  -1.0985\n",
       "\n",
       "[802 rows x 6 columns], token_table=      Topic      Freq   Term\n",
       "term                        \n",
       "0         1  0.433546  About\n",
       "0         2  0.216773  About\n",
       "0         3  0.216773  About\n",
       "0         4  0.216773  About\n",
       "1         1  0.433521    All\n",
       "...     ...       ...    ...\n",
       "13        4  0.216757     in\n",
       "14        1  0.366696    the\n",
       "14        2  0.183348    the\n",
       "14        3  0.183348    the\n",
       "14        4  0.366696    the\n",
       "\n",
       "[68 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[17, 16, 19, 20, 4, 12, 18, 15, 14, 13, 1, 2, 10, 9, 8, 7, 6, 5, 3, 11])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary=id2word,mds=\"mmds\")\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type complex is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py:339\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    341\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyLDAvis\\_display.py:308\u001b[0m, in \u001b[0;36menable_notebook.<locals>.<lambda>\u001b[1;34m(data, kwds)\u001b[0m\n\u001b[0;32m    305\u001b[0m ip \u001b[38;5;241m=\u001b[39m get_ipython()\n\u001b[0;32m    306\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ip\u001b[38;5;241m.\u001b[39mdisplay_formatter\u001b[38;5;241m.\u001b[39mformatters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext/html\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    307\u001b[0m formatter\u001b[38;5;241m.\u001b[39mfor_type(PreparedData,\n\u001b[1;32m--> 308\u001b[0m                    \u001b[38;5;28;01mlambda\u001b[39;00m data, kwds\u001b[38;5;241m=\u001b[39mkwargs: prepared_data_to_html(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyLDAvis\\_display.py:177\u001b[0m, in \u001b[0;36mprepared_data_to_html\u001b[1;34m(data, d3_url, ldavis_url, ldavis_css_url, template_type, visid, use_http)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m, visid):\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisid must not contain spaces\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m template\u001b[38;5;241m.\u001b[39mrender(visid\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(visid),\n\u001b[0;32m    174\u001b[0m                        visid_raw\u001b[38;5;241m=\u001b[39mvisid,\n\u001b[0;32m    175\u001b[0m                        d3_url\u001b[38;5;241m=\u001b[39md3_url,\n\u001b[0;32m    176\u001b[0m                        ldavis_url\u001b[38;5;241m=\u001b[39mldavis_url,\n\u001b[1;32m--> 177\u001b[0m                        vis_json\u001b[38;5;241m=\u001b[39m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    178\u001b[0m                        ldavis_css_url\u001b[38;5;241m=\u001b[39mldavis_css_url)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyLDAvis\\_prepare.py:471\u001b[0m, in \u001b[0;36mPreparedData.to_json\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_json\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNumPyEncoder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\json\\__init__.py:234\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\json\\encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\json\\encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[0;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyLDAvis\\utils.py:150\u001b[0m, in \u001b[0;36mNumPyEncoder.default\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np\u001b[38;5;241m.\u001b[39mfloat64) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np\u001b[38;5;241m.\u001b[39mfloat32):\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(obj)\n\u001b[1;32m--> 150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJSONEncoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\json\\encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type complex is not JSON serializable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=                        x                   y  topics  cluster       Freq\n",
       "topic                                                                    \n",
       "16     0.133799+0.000000j -0.001691+0.000000j       1        1  35.254387\n",
       "15    -0.070554+0.000000j  0.131136+0.000000j       2        1  25.743687\n",
       "18     0.102799+0.000000j -0.078289+0.000000j       3        1  19.498482\n",
       "19     0.157842+0.000000j  0.092625+0.000000j       4        1  17.032014\n",
       "3     -0.017464+0.000000j -0.010377+0.000000j       5        1   0.174967\n",
       "11    -0.020428+0.000000j -0.008894+0.000000j       6        1   0.153098\n",
       "17    -0.020428+0.000000j -0.008894+0.000000j       7        1   0.153098\n",
       "14    -0.020428+0.000000j -0.008894+0.000000j       8        1   0.153098\n",
       "13    -0.020428+0.000000j -0.008894+0.000000j       9        1   0.153098\n",
       "12    -0.020428+0.000000j -0.008894+0.000000j      10        1   0.153098\n",
       "0     -0.020428+0.000000j -0.008894+0.000000j      11        1   0.153098\n",
       "1     -0.020428+0.000000j -0.008894+0.000000j      12        1   0.153098\n",
       "9     -0.020428+0.000000j -0.008894+0.000000j      13        1   0.153098\n",
       "8     -0.020428+0.000000j -0.008894+0.000000j      14        1   0.153098\n",
       "7     -0.020428+0.000000j -0.008894+0.000000j      15        1   0.153098\n",
       "6     -0.020428+0.000000j -0.008894+0.000000j      16        1   0.153098\n",
       "5     -0.020428+0.000000j -0.008894+0.000000j      17        1   0.153098\n",
       "4     -0.020428+0.000000j -0.008894+0.000000j      18        1   0.153098\n",
       "2     -0.020428+0.000000j -0.008894+0.000000j      19        1   0.153098\n",
       "10    -0.020428+0.000000j -0.008894+0.000000j      20        1   0.153098, topic_info=        Term      Freq     Total Category  logprob  loglift\n",
       "14       the  5.000000  5.000000  Default  30.0000  30.0000\n",
       "7    Library  4.000000  4.000000  Default  29.0000  29.0000\n",
       "5       Hats  4.000000  4.000000  Default  28.0000  28.0000\n",
       "6   Learning  4.000000  4.000000  Default  27.0000  27.0000\n",
       "0      About  4.000000  4.000000  Default  26.0000  26.0000\n",
       "..       ...       ...       ...      ...      ...      ...\n",
       "5       Hats  0.002784  4.613014  Topic20  -3.7842  -0.9310\n",
       "6   Learning  0.002784  4.613028  Topic20  -3.7842  -0.9310\n",
       "7    Library  0.002784  4.612949  Topic20  -3.7842  -0.9310\n",
       "13        in  0.002784  4.613467  Topic20  -3.7842  -0.9311\n",
       "14       the  0.002784  5.454108  Topic20  -3.7842  -1.0985\n",
       "\n",
       "[802 rows x 6 columns], token_table=      Topic      Freq   Term\n",
       "term                        \n",
       "0         1  0.433546  About\n",
       "0         2  0.216773  About\n",
       "0         3  0.216773  About\n",
       "0         4  0.216773  About\n",
       "1         1  0.433521    All\n",
       "...     ...       ...    ...\n",
       "13        4  0.216757     in\n",
       "14        1  0.366696    the\n",
       "14        2  0.183348    the\n",
       "14        3  0.183348    the\n",
       "14        4  0.366696    the\n",
       "\n",
       "[68 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[17, 16, 19, 20, 4, 12, 18, 15, 14, 13, 1, 2, 10, 9, 8, 7, 6, 5, 3, 11])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "More:\n",
    "    https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module pyLDAvis.gensim_models in pyLDAvis:\n",
      "\n",
      "NAME\n",
      "    pyLDAvis.gensim_models\n",
      "\n",
      "DESCRIPTION\n",
      "    pyLDAvis Gensim\n",
      "    ===============\n",
      "    Helper functions to visualize LDA models trained by Gensim\n",
      "\n",
      "FUNCTIONS\n",
      "    prepare(topic_model, corpus, dictionary, doc_topic_dist=None, **kwargs)\n",
      "        Transforms the Gensim TopicModel and related corpus and dictionary into\n",
      "        the data structures needed for the visualization.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        topic_model : gensim.models.ldamodel.LdaModel\n",
      "            An already trained Gensim LdaModel. The other gensim model types are\n",
      "            not supported (PRs welcome).\n",
      "        \n",
      "        corpus : array-like list of bag of word docs in tuple form or scipy CSC matrix\n",
      "            The corpus in bag of word form, the same docs used to train the model.\n",
      "            The corpus is transformed into a csc matrix internally, if you intend to\n",
      "            call prepare multiple times it is a good idea to first call\n",
      "            `gensim.matutils.corpus2csc(corpus)` and pass in the csc matrix instead.\n",
      "        \n",
      "        For example: [(50, 3), (63, 5), ....]\n",
      "        \n",
      "        dictionary: gensim.corpora.Dictionary\n",
      "            The dictionary object used to create the corpus. Needed to extract the\n",
      "            actual terms (not ids).\n",
      "        \n",
      "        doc_topic_dist (optional): Document topic distribution from LDA (default=None)\n",
      "            The document topic distribution that is eventually visualised, if you will\n",
      "            be calling `prepare` multiple times it's a good idea to explicitly pass in\n",
      "            `doc_topic_dist` as inferring this for large corpora can be quite\n",
      "            expensive.\n",
      "        \n",
      "        **kwargs :\n",
      "            additional keyword arguments are passed through to :func:`pyldavis.prepare`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        prepared_data : PreparedData\n",
      "            the data structures used in the visualization\n",
      "        \n",
      "        Example\n",
      "        --------\n",
      "        For example usage please see this notebook:\n",
      "        http://nbviewer.ipython.org/github/bmabey/pyLDAvis/blob/master/notebooks/Gensim%20Newsgroup.ipynb\n",
      "        \n",
      "        See\n",
      "        ------\n",
      "        See `pyLDAvis.prepare` for **kwargs.\n",
      "\n",
      "FILE\n",
      "    c:\\anaconda\\envs\\textanalytics\\lib\\site-packages\\pyldavis\\gensim_models.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pyLDAvis.gensim_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try : LSA using Gensim package; LSA & LDA using Sklearn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try : BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try topic modelling for donald_trump_speeches datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
